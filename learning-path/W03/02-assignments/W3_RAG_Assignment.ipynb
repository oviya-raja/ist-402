{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/oviya-raja/ist-402/blob/main/learning-path/W03/02-assignments/W3_RAG_Assignment.ipynb)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# RAG Assignment: \n",
    "## Building an Intelligent Q&A System with FAISS and Mistral\n",
    "\n",
    "\n",
    "### <summary><b>üè¢ Business Context</b></summary>\n",
    "\n",
    "**Company:** GreenTech Marketplace\n",
    "\n",
    "**Company Description:**\n",
    "GreenTech Marketplace is an e-commerce platform specializing in sustainable technology products. The company focuses on eco-friendly technology solutions, renewable energy products, and smart home devices that help customers reduce their environmental footprint.\n",
    "\n",
    "**Company Details:**\n",
    "- **Type:** E-commerce platform\n",
    "- **Specialization:** Sustainable technology products\n",
    "- **Mission:** Providing eco-friendly technology solutions for modern living\n",
    "\n",
    "**Shipping & Policies:**\n",
    "- **Standard Shipping:** 5-7 business days (free over $75)\n",
    "- **Express Shipping:** 2-3 business days (additional fee)\n",
    "- **Return Policy:** 30-day return policy for unopened items in original packaging\n",
    "- **Refund Processing:** 5-7 business days after receipt\n",
    "- **Warranty:** 1-3 years depending on product category\n",
    "\n",
    "**Product Categories:**\n",
    "- **Solar Panels** - Renewable energy solutions for homes and businesses\n",
    "- **Energy-Efficient Appliances** - Eco-friendly home appliances\n",
    "- **Smart Home Devices** - Home automation and IoT solutions\n",
    "- **Eco-Friendly Accessories** - Sustainable lifestyle products\n",
    "\n",
    "**What I'm Building:**\n",
    "A production-ready RAG system that combines semantic search (FAISS) with large language models (Mistral) to provide accurate, context-aware answers for customer service scenarios for GreenTech Marketplace\n",
    "\n",
    "**IST402 - AI Agents & RAG Systems**  \n",
    "**Student**: Oviya Raja\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Assignment Overview\n",
    "\n",
    "### üéØ Goal\n",
    "Design and implement a complete **Retrieval-Augmented Generation (RAG) system** using Mistral-7B-Instruct, FAISS vector database, and custom business data. Evaluate multiple AI models and analyze system performance.\n",
    "\n",
    "**Submission:** Submit the link to your completed notebook.\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "<summary><b>üéØ Objectives</b> (Click to expand)</summary>\n",
    "\n",
    "### 1. Create Assistant System Prompt\n",
    "- Design a system prompt with a specific role \n",
    "- Choose a business context to use throughout\n",
    "- Use `mistralai/Mistral-7B-Instruct-v0.3`\n",
    "\n",
    "### 2. Generate Business Database Content\n",
    "- Use Mistral to generate **10-15 Q&A pairs** for your business\n",
    "- Cover different aspects of the business\n",
    "- **Add clear comments** showing the database pairs\n",
    "\n",
    "### 3. Implement FAISS Vector Database\n",
    "- Convert Q&A database to embeddings using sentence transformers\n",
    "- Store embeddings in FAISS index for efficient similarity search\n",
    "- **Comment the implementation process**\n",
    "\n",
    "### 4. Create Test Questions\n",
    "- Generate **5+ answerable** questions (can be answered from database)\n",
    "- Generate **5+ unanswerable** questions (require information not in database)\n",
    "- Use Mistral to generate both types\n",
    "\n",
    "### 5. Implement and Test RAG System\n",
    "- Build complete RAG pipeline: `Query ‚Üí Embed ‚Üí Search ‚Üí Retrieve ‚Üí Augment ‚Üí LLM ‚Üí Answer`\n",
    "- Run both question types through the pipeline\n",
    "- **Comment differences** between answerable vs. unanswerable results\n",
    "\n",
    "### 6. Model Experimentation and Ranking\n",
    "- Test **4 required models + 2 of your choice** (6 total):\n",
    "  - `consciousAI/question-answering-generative-t5-v1-base-s-q-c`\n",
    "  - `deepset/roberta-base-squad2`\n",
    "  - `google-bert/bert-large-cased-whole-word-masking-finetuned-squad`\n",
    "  - `gasolsun/DynamicRAG-8B`\n",
    "  - + 2 additional models of your choice\n",
    "- **Rank models** by 5 metrics: **Accuracy**, **Confidence Handling**, **Quality**, **Speed**, **Robustness**\n",
    "- Test with both answerable and unanswerable questions\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>üîß Technologies & Tools</b> (Click to expand)</summary>\n",
    "\n",
    "| Technology | Purpose | Version/Model |\n",
    "|------------|---------|---------------|\n",
    "| **Mistral-7B-Instruct** | LLM for generating Q&A pairs and system prompts | `mistralai/Mistral-7B-Instruct-v0.3` |\n",
    "| **FAISS** | Vector database for efficient similarity search | `faiss-cpu` |\n",
    "| **Embeddings Generation** | Text embeddings for semantic search | `all-MiniLM-L6-v2` |\n",
    "| **Inference Engine** | Host the LLM | Hugging Face Transformers |\n",
    "\n",
    "**Installation:**\n",
    "```python\n",
    "!pip install transformers torch sentence-transformers faiss-cpu langchain\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>üèóÔ∏è System Architecture</b> (Click to expand)</summary>\n",
    "\n",
    "**RAG System Architecture Overview:**\n",
    "\n",
    "![RAG System Architecture](diagrams/rag-pipeline-architecture.png)\n",
    "\n",
    "*Figure: Two-phase RAG system showing Pre-Processing (Objectives 1-3) and RAG Pipeline (Objectives 4-5)*\n",
    "\n",
    "**Phase 1: Pre-Processing (Objectives 1-3)**\n",
    "- **Objective 1**: System Prompt - Define agentic role & business context\n",
    "- **Objective 2**: Generate Q&A - 10-15 pairs using Mistral-7B-Instruct\n",
    "- **Objective 3**: FAISS Index - Create embeddings & vector store\n",
    "- **Output**: Vector Database ready for retrieval\n",
    "\n",
    "**Phase 2: RAG Pipeline (Objectives 4-5)**\n",
    "- **User Query**: 5+ answerable and 5+ unanswerable questions\n",
    "- **Objective 4**: RAG Pipeline Steps\n",
    "  - Query ‚Üí Embed ‚Üí Search FAISS ‚Üí Retrieve Context ‚Üí Mistral LLM\n",
    "- **Objective 5**: Generated Answer\n",
    "\n",
    "**Dependency Chain:**\n",
    "```\n",
    "0 (Setup) ‚Üí 1 (System Prompt) ‚Üí 2 (Q&A DB) ‚Üí 3 (FAISS) ‚Üí 4 (RAG Pipeline) ‚Üí 5 (Answer) ‚Üí 6 (Evaluation)\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>‚úÖ Deliverables Checklist</b> (Click to expand)</summary>\n",
    "\n",
    "- ‚úÖ **Business context** & role definition\n",
    "- ‚úÖ **Generated Q&A database** (10-15 pairs, clearly commented)\n",
    "- ‚úÖ **Working FAISS vector database** with embeddings\n",
    "- ‚úÖ **Test questions** (answerable vs. unanswerable, 5+ each)\n",
    "- ‚úÖ **RAG pipeline** implementation\n",
    "- ‚úÖ **Model rankings** with performance analysis (6 models)\n",
    "- ‚úÖ **Reflection** on strengths, weaknesses, and real-world applications\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>‚è±Ô∏è Time Estimates</b> (Click to expand)</summary>\n",
    "\n",
    "\n",
    "<br>\n",
    "<details>\n",
    "<summary><b>üìä Evaluation Criteria</b> (Click to expand)</summary>\n",
    "\n",
    "- **Creativity** in business context and agentic role design\n",
    "- **Technical Implementation** of RAG pipeline with FAISS\n",
    "- **Quality Analysis** of different Q&A models\n",
    "- **Clear Documentation** with meaningful comments\n",
    "- **Critical Thinking** in model comparison and limitations analysis\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to begin?** Start with **Objective 0: Setup & Prerequisites** to configure your environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective 0: Setup & Prerequisites\n",
    "\n",
    "### üéØ Goal\n",
    "I will set up my environment with all required packages, configure authentication, and verify system capabilities before starting the RAG system implementation.\n",
    "\n",
    "<details>\n",
    "<summary><b>Prerequisites Checklist</b> (Click to expand)</summary>\n",
    "\n",
    "**Knowledge Prerequisites:**\n",
    "\n",
    "| Requirement | Level | Description |\n",
    "|-------------|-------|-------------|\n",
    "| **Python Programming** | Basic | Variables, functions, data structures, imports |\n",
    "| **Jupyter Notebooks** | Basic | Running cells, markdown formatting, code execution |\n",
    "| **Machine Learning Concepts** | High-level | Neural networks, transformers, embeddings, model inference |\n",
    "| **Evaluation Metrics** | Basic | Understanding of accuracy, confidence scores |\n",
    "\n",
    "**Technical Prerequisites:**\n",
    "\n",
    "| Item | Required | How to Get |\n",
    "|------|----------|------------|\n",
    "| **Hugging Face Account** | ‚úÖ Yes | Sign up at [huggingface.co](https://huggingface.co) |\n",
    "| **Hugging Face Token** | ‚úÖ Yes | Get from [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens) |\n",
    "| **Google Colab Account** | ‚ö†Ô∏è Recommended | Free account at [colab.research.google.com](https://colab.research.google.com) |\n",
    "| **GPU Access** | ‚ö†Ô∏è Optional | Colab provides free GPU, or local GPU setup |\n",
    "| **Python 3.8+** | ‚úÖ Yes | Pre-installed in Colab, or install locally |\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>üì¶ Required Packages</b> (Click to expand)</summary>\n",
    "\n",
    "**Core Packages:**\n",
    "\n",
    "| Package | Purpose | Version |\n",
    "|---------|---------|---------|\n",
    "| `transformers` | Load and use Hugging Face models (Mistral, QA models) | Latest |\n",
    "| `torch` | Deep learning framework for model inference | Latest |\n",
    "| `sentence-transformers` | Generate embeddings for semantic search | Latest |\n",
    "| `faiss-cpu` | Vector similarity search library | Latest |\n",
    "| `huggingface_hub` | Hugging Face authentication and model access | Latest |\n",
    "| `numpy` | Numerical operations | Latest |\n",
    "| `pandas` | DataFrames for Q&A database | Latest |\n",
    "\n",
    "**Evaluation & Utilities:**\n",
    "\n",
    "| Package | Purpose | Version |\n",
    "|---------|---------|---------|\n",
    "| `bert-score` | BERTScore for accuracy evaluation | Latest |\n",
    "| `python-dotenv` | Load environment variables from .env files | Latest |\n",
    "\n",
    "**Installation Command:**\n",
    "```python\n",
    "!pip install transformers torch sentence-transformers faiss-cpu huggingface_hub numpy pandas bert-score python-dotenv\n",
    "```\n",
    "\n",
    "**Note:** All packages will be installed automatically by the setup code cell.\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>üñ•Ô∏è Environment Setup</b> (Click to expand)</summary>\n",
    "\n",
    "**Option 1: Google Colab (Recommended)**\n",
    "\n",
    "**Advantages:**\n",
    "- ‚úÖ Free GPU access (T4 GPU)\n",
    "- ‚úÖ No local installation needed\n",
    "- ‚úÖ Pre-configured environment\n",
    "- ‚úÖ Easy sharing and collaboration\n",
    "\n",
    "**Setup Steps:**\n",
    "1. Go to [colab.research.google.com](https://colab.research.google.com)\n",
    "2. Create a new notebook\n",
    "3. Enable GPU: **Runtime ‚Üí Change runtime type ‚Üí GPU ‚Üí Save**\n",
    "4. Upload or create the notebook file\n",
    "5. Run the setup cell\n",
    "\n",
    "**Option 2: Local Jupyter Notebook**\n",
    "\n",
    "**Requirements:**\n",
    "- Python 3.8 or higher\n",
    "- Jupyter Notebook installed\n",
    "- GPU optional (CPU works but slower)\n",
    "\n",
    "**Setup Steps:**\n",
    "1. Install Python 3.8+\n",
    "2. Install Jupyter: `pip install jupyter`\n",
    "3. Install required packages (see above)\n",
    "4. Launch: `jupyter notebook`\n",
    "5. Open the notebook file\n",
    "\n",
    "**GPU Setup (Optional but Recommended):**\n",
    "\n",
    "| Platform | GPU Type | Setup |\n",
    "|----------|----------|-------|\n",
    "| **Colab** | T4 (Free) | Runtime ‚Üí Change runtime type ‚Üí GPU |\n",
    "| **Local** | NVIDIA GPU | Install CUDA toolkit, PyTorch with CUDA |\n",
    "| **CPU Only** | N/A | Works but 2-3x slower |\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>üîë Hugging Face Authentication</b> (Click to expand)</summary>\n",
    "\n",
    "**Step 1: Get Your Token**\n",
    "\n",
    "1. Go to [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
    "2. Click **\"New token\"**\n",
    "3. Name it (e.g., \"RAG Assignment\")\n",
    "4. Select **\"Read\"** access (sufficient for this assignment)\n",
    "5. Click **\"Generate token\"**\n",
    "6. **Copy the token immediately** (you won't see it again!)\n",
    "\n",
    "**Step 2: Store Your Token**\n",
    "\n",
    "**In Google Colab:**\n",
    "```python\n",
    "from google.colab import userdata\n",
    "userdata.set('HUGGINGFACE_HUB_TOKEN', 'your_token_here')\n",
    "```\n",
    "\n",
    "**Or use Colab Secrets:**\n",
    "1. Click the üîë icon in the left sidebar\n",
    "2. Add secret: `HUGGINGFACE_HUB_TOKEN` = `your_token_here`\n",
    "\n",
    "**Locally (Environment Variable):**\n",
    "```bash\n",
    "export HUGGINGFACE_HUB_TOKEN=your_token_here\n",
    "```\n",
    "\n",
    "**Or create `.env` file:**\n",
    "```\n",
    "HUGGINGFACE_HUB_TOKEN=your_token_here\n",
    "```\n",
    "\n",
    "**Step 3: Verify Authentication**\n",
    "\n",
    "The setup code will automatically:\n",
    "- Try Colab userdata first\n",
    "- Try environment variables\n",
    "- Prompt for manual input if needed\n",
    "- Authenticate with Hugging Face Hub\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>‚úÖ Setup Verification</b> (Click to expand)</summary>\n",
    "\n",
    "After running the setup cell, verify these are set:\n",
    "\n",
    "**Environment Variables:**\n",
    "- ‚úÖ `IN_COLAB` - True if running in Colab\n",
    "- ‚úÖ `HAS_GPU` - True if GPU is available\n",
    "- ‚úÖ `hf_token` - Your Hugging Face token\n",
    "\n",
    "**Verification Code:**\n",
    "```python\n",
    "# Check setup\n",
    "print(\"üîç Setup Verification:\")\n",
    "print(f\"   IN_COLAB: {IN_COLAB}\")\n",
    "print(f\"   HAS_GPU: {HAS_GPU}\")\n",
    "print(f\"   hf_token: {'‚úÖ Set' if hf_token else '‚ùå Not set'}\")\n",
    "\n",
    "if HAS_GPU:\n",
    "    import torch\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "```\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "üîç Setup Verification:\n",
    "   IN_COLAB: True\n",
    "   HAS_GPU: True\n",
    "   hf_token: ‚úÖ Set\n",
    "   GPU: Tesla T4\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>‚öôÔ∏è Setup Functions</b> (Click to expand)</summary>\n",
    "\n",
    "The setup code provides modular functions following SOLID principles:\n",
    "\n",
    "**Available Functions:**\n",
    "\n",
    "| Function | Purpose | Returns |\n",
    "|----------|---------|---------|\n",
    "| `check_environment()` | Detect Colab and GPU availability | `(is_colab, has_gpu)` |\n",
    "| `get_hf_token()` | Retrieve Hugging Face token from various sources | `str` (token) |\n",
    "| `install_packages()` | Install required packages if missing | None |\n",
    "| `import_libraries()` | Import all required libraries with error handling | `bool` (success) |\n",
    "| `authenticate_hf(token)` | Authenticate with Hugging Face Hub | `bool` (success) |\n",
    "\n",
    "**Design Principles:**\n",
    "- **KISS** (Keep It Simple, Stupid) - Each function has single responsibility\n",
    "- **DRY** (Don't Repeat Yourself) - Reusable functions\n",
    "- **SOLID** - Modular, extensible design\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>üöÄ Quick Start Guide</b> (Click to expand)</summary>\n",
    "\n",
    "**Step-by-Step Setup:**\n",
    "\n",
    "1. **Open Environment**\n",
    "   - Colab: Create new notebook\n",
    "   - Local: Launch Jupyter Notebook\n",
    "\n",
    "2. **Enable GPU (Recommended)**\n",
    "   - Colab: Runtime ‚Üí Change runtime type ‚Üí GPU\n",
    "   - Local: Ensure CUDA is installed\n",
    "\n",
    "3. **Run Setup Cell**\n",
    "   - Execute the \"Prerequisites & Setup\" code cell\n",
    "   - Wait for packages to install (~2-3 minutes first time)\n",
    "\n",
    "4. **Authenticate**\n",
    "   - Enter Hugging Face token when prompted\n",
    "   - Or set it in Colab secrets/environment variable\n",
    "\n",
    "5. **Verify**\n",
    "   - Check console output for ‚úÖ marks\n",
    "   - Verify `hf_token` is set\n",
    "   - Confirm GPU is detected (if available)\n",
    "\n",
    "**Expected Setup Time:**\n",
    "- First run: 2-3 minutes (package installation)\n",
    "- Subsequent runs: <30 seconds (packages cached)\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>‚ö†Ô∏è Troubleshooting</b> (Click to expand)</summary>\n",
    "\n",
    "**Common Issues:**\n",
    "\n",
    "| Issue | Solution |\n",
    "|-------|----------|\n",
    "| **Token not found** | Check Colab secrets or environment variables. Re-enter token manually. |\n",
    "| **GPU not detected** | In Colab: Runtime ‚Üí Change runtime type ‚Üí GPU. Local: Install CUDA toolkit. |\n",
    "| **Package installation fails** | Restart runtime/kernel and try again. Check internet connection. |\n",
    "| **Import errors** | Run `pip install --upgrade <package>` for the failing package. |\n",
    "| **Out of memory** | Use CPU instead of GPU, or restart runtime to clear memory. |\n",
    "\n",
    "**Getting Help:**\n",
    "- Check error messages carefully - they usually indicate the issue\n",
    "- Verify all prerequisites are met\n",
    "- Ensure internet connection is stable\n",
    "- Restart runtime/kernel if issues persist\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>üìä System Requirements</b> (Click to expand)</summary>\n",
    "\n",
    "**Minimum Requirements (CPU):**\n",
    "- Python 3.8+\n",
    "- 8GB RAM\n",
    "- 20GB free disk space (for model downloads)\n",
    "- Stable internet connection\n",
    "\n",
    "**Recommended (GPU):**\n",
    "- Python 3.8+\n",
    "- 16GB+ RAM\n",
    "- NVIDIA GPU with 8GB+ VRAM\n",
    "- 30GB+ free disk space\n",
    "- Fast internet connection\n",
    "\n",
    "**Colab Specifications:**\n",
    "- **Free Tier:** T4 GPU (16GB VRAM), 12GB RAM\n",
    "- **Pro Tier:** V100/A100 GPU, more RAM\n",
    "- **Storage:** 15GB free space\n",
    "\n",
    "**Model Download Sizes:**\n",
    "- Mistral-7B: ~14GB (first download)\n",
    "- QA Models: ~500MB - 3GB each\n",
    "- Embedding Model: ~90MB\n",
    "- **Total:** ~20-25GB for all models\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Next Step:** After setup is complete, proceed to **Objective 1: Design System Prompts** to load the Mistral model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "OBJECTIVE 0: PREREQUISITES & SETUP\n",
      "================================================================================\n",
      "\n",
      "üîç Checking environment...\n",
      "   Python version: 3.12.10\n",
      "   ‚úÖ Running in local environment\n",
      "   ‚ö†Ô∏è  GPU NOT detected (using CPU)\n",
      "\n",
      "üì¶ Installing required packages...\n",
      "   ‚úÖ transformers already installed\n",
      "   ‚úÖ torch already installed\n",
      "   ‚úÖ sentence-transformers already installed\n",
      "   ‚è≥ Installing python-dotenv...\n",
      "   ‚úÖ python-dotenv installed\n",
      "   ‚è≥ Installing faiss-cpu...\n",
      "   ‚úÖ faiss-cpu installed\n",
      "   ‚úÖ huggingface_hub already installed\n",
      "   ‚úÖ numpy already installed\n",
      "   ‚úÖ pandas already installed\n",
      "   ‚úÖ bert-score already installed\n",
      "   ‚úÖ accelerate already installed\n",
      "\n",
      "‚úÖ All required libraries imported successfully!\n",
      "‚úÖ Hugging Face token loaded from environment!\n",
      "   Token preview: hf_ThdSIol...ustv\n",
      "‚úÖ Authenticated with Hugging Face\n",
      "\n",
      "================================================================================\n",
      "‚úÖ PREREQUISITES & SETUP COMPLETED!\n",
      "================================================================================\n",
      "\n",
      "üìå Environment Configuration:\n",
      "   - Environment: Local\n",
      "   - Python: 3.12.10\n",
      "   - Device: CPU (CPU)\n",
      "   - HF Token: ‚úÖ Set\n",
      "   - Libraries: ‚úÖ Ready\n",
      "\n",
      "üí° Usage in other cells:\n",
      "   - env.device  # Returns 'cuda' or 'cpu'\n",
      "   - env.is_colab  # Returns True/False\n",
      "   - env.has_gpu  # Returns True/False\n",
      "   - env.hf_token  # Returns token or None\n",
      "================================================================================\n",
      "üí° Timing system ready! Use: with env.timer.objective('Objective 1'): ...\n",
      "\n",
      "‚úÖ Objective 0 completed in 0.89s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Prerequisites & Setup - Centralized Environment Configuration\n",
    "# ============================================================================\n",
    "# This cell contains a centralized EnvironmentConfig class that handles ALL\n",
    "# Colab vs local environment differences. No if statements needed elsewhere!\n",
    "# Run this cell FIRST before any other cells\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# ============================================================================\n",
    "# EnvironmentConfig Class - Single Source of Truth\n",
    "# ============================================================================\n",
    "class EnvironmentConfig:\n",
    "    \"\"\"\n",
    "    Centralized environment configuration that handles ALL Colab vs local differences.\n",
    "    All environment-specific logic is encapsulated here - no if statements needed elsewhere!\n",
    "    \n",
    "    Usage:\n",
    "        env = EnvironmentConfig()  # Auto-detects environment\n",
    "        device = env.device  # Returns \"cuda\" or \"cpu\" automatically\n",
    "        token = env.get_token()  # Works in both Colab and local\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize and detect environment automatically.\"\"\"\n",
    "        self._is_colab = self._detect_colab()\n",
    "        self._has_gpu = self._detect_gpu()\n",
    "        self._python_version = sys.version.split()[0]\n",
    "        self._hf_token = None\n",
    "        self._libraries_imported = False\n",
    "        \n",
    "        # Print environment info\n",
    "        self._print_environment_info()\n",
    "    \n",
    "    def _detect_colab(self) -> bool:\n",
    "        \"\"\"Detect if running in Google Colab.\"\"\"\n",
    "        try:\n",
    "            import google.colab\n",
    "            return True\n",
    "        except ImportError:\n",
    "            return False\n",
    "    \n",
    "    def _detect_gpu(self) -> bool:\n",
    "        \"\"\"Detect GPU availability.\"\"\"\n",
    "        try:\n",
    "            import torch\n",
    "            return torch.cuda.is_available()\n",
    "        except ImportError:\n",
    "            return False\n",
    "    \n",
    "    def _print_environment_info(self):\n",
    "        \"\"\"Print environment detection results.\"\"\"\n",
    "        print(\"üîç Checking environment...\")\n",
    "        print(f\"   Python version: {self._python_version}\")\n",
    "        \n",
    "        if self._is_colab:\n",
    "            print(\"   ‚úÖ Running in Google Colab\")\n",
    "        else:\n",
    "            print(\"   ‚úÖ Running in local environment\")\n",
    "        \n",
    "        if self._has_gpu:\n",
    "            try:\n",
    "                import torch\n",
    "                print(f\"   ‚úÖ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "                print(f\"   ‚úÖ CUDA Version: {torch.version.cuda}\")\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è  GPU NOT detected (using CPU)\")\n",
    "            if self._is_colab:\n",
    "                print(\"   üí° TIP: Runtime ‚Üí Change runtime type ‚Üí Select GPU ‚Üí Save\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Properties - No if statements needed when using these!\n",
    "    # ========================================================================\n",
    "    \n",
    "    @property\n",
    "    def is_colab(self) -> bool:\n",
    "        \"\"\"Check if running in Google Colab.\"\"\"\n",
    "        return self._is_colab\n",
    "    \n",
    "    @property\n",
    "    def is_local(self) -> bool:\n",
    "        \"\"\"Check if running locally.\"\"\"\n",
    "        return not self._is_colab\n",
    "    \n",
    "    @property\n",
    "    def has_gpu(self) -> bool:\n",
    "        \"\"\"Check if GPU is available.\"\"\"\n",
    "        return self._has_gpu\n",
    "    \n",
    "    @property\n",
    "    def device(self) -> str:\n",
    "        \"\"\"Get device string ('cuda' or 'cpu') - no if statement needed!\"\"\"\n",
    "        return \"cuda\" if self._has_gpu else \"cpu\"\n",
    "    \n",
    "    @property\n",
    "    def device_id(self) -> int:\n",
    "        \"\"\"Get device ID (0 for GPU, -1 for CPU) - no if statement needed!\"\"\"\n",
    "        return 0 if self._has_gpu else -1\n",
    "    \n",
    "    @property\n",
    "    def hf_token(self) -> Optional[str]:\n",
    "        \"\"\"Get Hugging Face token.\"\"\"\n",
    "        return self._hf_token\n",
    "    \n",
    "    @property\n",
    "    def python_version(self) -> str:\n",
    "        \"\"\"Get Python version.\"\"\"\n",
    "        return self._python_version\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Token Management - Works in both Colab and local\n",
    "    # ========================================================================\n",
    "    \n",
    "    def _format_token_preview(self, token: str) -> str:\n",
    "        \"\"\"\n",
    "        Format token for safe display (DRY - eliminates duplication).\n",
    "        \n",
    "        Args:\n",
    "            token: Hugging Face token string\n",
    "            \n",
    "        Returns:\n",
    "            Formatted preview string (e.g., \"hf_abc123...xyz9\")\n",
    "        \"\"\"\n",
    "        if len(token) <= 14:\n",
    "            return \"****\"\n",
    "        return f\"{token[:10]}...{token[-4:]}\"\n",
    "    \n",
    "    def get_token(self) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Get Hugging Face token from appropriate source (Colab or local).\n",
    "        Works automatically in both environments - no if statements needed!\n",
    "        \"\"\"\n",
    "        # Try Colab userdata first (only works in Colab, fails gracefully in local)\n",
    "        if self._is_colab:\n",
    "            try:\n",
    "                from google.colab import userdata\n",
    "                token = userdata.get('HUGGINGFACE_HUB_TOKEN')\n",
    "                if token:\n",
    "                    print(\"‚úÖ Hugging Face token loaded from Colab userdata!\")\n",
    "                    print(f\"   Token preview: {self._format_token_preview(token)}\")\n",
    "                    self._hf_token = token\n",
    "                    return token\n",
    "            except (ImportError, ValueError):\n",
    "                pass\n",
    "        \n",
    "        # Try environment variable (works in both Colab and local)\n",
    "        try:\n",
    "            from dotenv import load_dotenv\n",
    "            load_dotenv()\n",
    "        except ImportError:\n",
    "            pass\n",
    "        \n",
    "        token = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "        if token:\n",
    "            print(\"‚úÖ Hugging Face token loaded from environment!\")\n",
    "            print(f\"   Token preview: {self._format_token_preview(token)}\")\n",
    "            self._hf_token = token\n",
    "            return token\n",
    "        \n",
    "        # No token found\n",
    "        print(\"‚ùå Hugging Face token not found!\")\n",
    "        print(\"   Get your token from: https://huggingface.co/settings/tokens\")\n",
    "        \n",
    "        if self._is_colab:\n",
    "            print(\"\\n   In Colab: userdata.set('HUGGINGFACE_HUB_TOKEN', 'your_token')\")\n",
    "        else:\n",
    "            print(\"\\n   Locally: export HUGGINGFACE_HUB_TOKEN=your_token\")\n",
    "            print(\"   Or create .env file: HUGGINGFACE_HUB_TOKEN=your_token\")\n",
    "        \n",
    "        print(\"\\n‚ö†Ô∏è  Some models may require authentication!\")\n",
    "        return None\n",
    "    \n",
    "    def authenticate_hf(self, token: Optional[str] = None) -> bool:\n",
    "        \"\"\"\n",
    "        Authenticate with Hugging Face.\n",
    "        Uses stored token if none provided.\n",
    "        \"\"\"\n",
    "        token = token or self._hf_token\n",
    "        \n",
    "        if not token:\n",
    "            print(\"‚ö†Ô∏è  No token provided, skipping authentication\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            from huggingface_hub import login\n",
    "            login(token=token)\n",
    "            print(\"‚úÖ Authenticated with Hugging Face\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Authentication failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Package Management - Works in both environments\n",
    "    # ========================================================================\n",
    "    \n",
    "    def install_packages(self):\n",
    "        \"\"\"Install required packages if not already installed.\"\"\"\n",
    "        packages = [\n",
    "            \"transformers\",\n",
    "            \"torch\",\n",
    "            \"sentence-transformers\",\n",
    "            \"python-dotenv\",\n",
    "            \"faiss-cpu\",\n",
    "            \"huggingface_hub\",\n",
    "            \"numpy\",\n",
    "            \"pandas\",\n",
    "            \"bert-score\",\n",
    "            \"accelerate\"\n",
    "        ]\n",
    "        \n",
    "        print(\"üì¶ Installing required packages...\")\n",
    "        for package in packages:\n",
    "            try:\n",
    "                __import__(package.replace(\"-\", \"_\"))\n",
    "                print(f\"   ‚úÖ {package} already installed\")\n",
    "            except ImportError:\n",
    "                print(f\"   ‚è≥ Installing {package}...\")\n",
    "                os.system(f\"pip install -q {package}\")\n",
    "                print(f\"   ‚úÖ {package} installed\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Library Imports - Centralized and reusable\n",
    "    # ========================================================================\n",
    "    \n",
    "    def import_libraries(self) -> bool:\n",
    "        \"\"\"\n",
    "        Import all required libraries with error handling.\n",
    "        Returns True if successful, False otherwise.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from transformers import (\n",
    "                pipeline, \n",
    "                AutoModelForCausalLM, \n",
    "                AutoTokenizer, \n",
    "                logging as transformers_logging\n",
    "            )\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            import torch\n",
    "            import numpy as np\n",
    "            import faiss\n",
    "            \n",
    "            transformers_logging.set_verbosity_error()\n",
    "            \n",
    "            # Store imported modules for reuse\n",
    "            self.pipeline = pipeline\n",
    "            self.AutoModelForCausalLM = AutoModelForCausalLM\n",
    "            self.AutoTokenizer = AutoTokenizer\n",
    "            self.SentenceTransformer = SentenceTransformer\n",
    "            self.torch = torch\n",
    "            self.np = np\n",
    "            self.faiss = faiss\n",
    "            \n",
    "            self._libraries_imported = True\n",
    "            print(\"‚úÖ All required libraries imported successfully!\")\n",
    "            return True\n",
    "        except ImportError as e:\n",
    "            print(f\"‚ùå Import error: {e}\")\n",
    "            print(\"   Run: pip install transformers torch sentence-transformers faiss-cpu\")\n",
    "            return False\n",
    "        except RuntimeError as e:\n",
    "            if \"register_fake\" in str(e) or \"torch.library\" in str(e):\n",
    "                print(\"‚ùå Dependency version mismatch!\")\n",
    "                print(\"   Fix: pip install --upgrade torch torchvision\")\n",
    "                print(\"   Then restart kernel and run this cell again.\")\n",
    "            return False\n",
    "    \n",
    "    @property\n",
    "    def libraries_ready(self) -> bool:\n",
    "        \"\"\"Check if libraries are imported and ready.\"\"\"\n",
    "        return self._libraries_imported\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Utility Methods - Environment-agnostic helpers\n",
    "    # ========================================================================\n",
    "    \n",
    "    def get_device_info(self) -> dict:\n",
    "        \"\"\"Get device information dictionary.\"\"\"\n",
    "        info = {\n",
    "            \"device\": self.device,\n",
    "            \"device_id\": self.device_id,\n",
    "            \"has_gpu\": self._has_gpu,\n",
    "            \"is_colab\": self._is_colab\n",
    "        }\n",
    "        \n",
    "        if self._has_gpu and self._libraries_imported:\n",
    "            try:\n",
    "                info[\"gpu_name\"] = self.torch.cuda.get_device_name(0)\n",
    "                info[\"cuda_version\"] = self.torch.version.cuda\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return info\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"Print configuration summary.\"\"\"\n",
    "        print(\"=\" * 80)\n",
    "        print(\"‚úÖ PREREQUISITES & SETUP COMPLETED!\")\n",
    "        print(\"=\" * 80)\n",
    "        print()\n",
    "        print(\"üìå Environment Configuration:\")\n",
    "        print(f\"   - Environment: {'Google Colab' if self._is_colab else 'Local'}\")\n",
    "        print(f\"   - Python: {self._python_version}\")\n",
    "        print(f\"   - Device: {self.device.upper()} ({'GPU' if self._has_gpu else 'CPU'})\")\n",
    "        print(f\"   - HF Token: {'‚úÖ Set' if self._hf_token else '‚ùå Not set'}\")\n",
    "        print(f\"   - Libraries: {'‚úÖ Ready' if self._libraries_imported else '‚ùå Not imported'}\")\n",
    "        print()\n",
    "        print(\"üí° Usage in other cells:\")\n",
    "        print(\"   - env.device  # Returns 'cuda' or 'cpu'\")\n",
    "        print(\"   - env.is_colab  # Returns True/False\")\n",
    "        print(\"   - env.has_gpu  # Returns True/False\")\n",
    "        print(\"   - env.hf_token  # Returns token or None\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Objective Names - Standardized naming guide\n",
    "# ============================================================================\n",
    "class ObjectiveNames:\n",
    "    \"\"\"\n",
    "    Standardized objective naming guide.\n",
    "    Use these constants to ensure consistent naming across the notebook.\n",
    "    \n",
    "    Usage:\n",
    "        with env.timer.objective(ObjectiveNames.OBJECTIVE_1):\n",
    "            # Your code\n",
    "            pass\n",
    "    \"\"\"\n",
    "    OBJECTIVE_0 = \"Objective 0\"\n",
    "    OBJECTIVE_1 = \"Objective 1\"\n",
    "    OBJECTIVE_2 = \"Objective 2\"\n",
    "    OBJECTIVE_3 = \"Objective 3\"\n",
    "    OBJECTIVE_4 = \"Objective 4\"\n",
    "    OBJECTIVE_5 = \"Objective 5\"\n",
    "    \n",
    "    @classmethod\n",
    "    def get_number(cls, objective_name: str) -> Optional[int]:\n",
    "        \"\"\"\n",
    "        Extract objective number from name.\n",
    "        \n",
    "        Args:\n",
    "            objective_name: Objective name (e.g., \"Objective 1\")\n",
    "            \n",
    "        Returns:\n",
    "            Objective number (e.g., 1) or None if invalid\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract number from \"Objective X\" format\n",
    "            parts = objective_name.split()\n",
    "            if len(parts) >= 2 and parts[0].lower() == \"objective\":\n",
    "                return int(parts[1])\n",
    "        except (ValueError, IndexError):\n",
    "            pass\n",
    "        return None\n",
    "    \n",
    "    @classmethod\n",
    "    def format_name(cls, number: int) -> str:\n",
    "        \"\"\"\n",
    "        Format objective number into standardized name.\n",
    "        \n",
    "        Args:\n",
    "            number: Objective number (e.g., 1)\n",
    "            \n",
    "        Returns:\n",
    "            Formatted name (e.g., \"Objective 1\")\n",
    "        \"\"\"\n",
    "        return f\"Objective {number}\"\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ObjectiveTimingManager - Track execution time for each objective\n",
    "# ============================================================================\n",
    "class ObjectiveTimingManager:\n",
    "    \"\"\"\n",
    "    Track and store execution time for each objective.\n",
    "    Compares first-time vs subsequent runs.\n",
    "    \n",
    "    Usage:\n",
    "        with env.timer.objective(\"Objective 1\"):\n",
    "            # Your code here\n",
    "            pass\n",
    "        \n",
    "        # View timing history\n",
    "        env.timer.print_summary()\n",
    "        env.timer.get_stats(\"Objective 1\")\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, storage_file: str = \"objective_timings.csv\"):\n",
    "        \"\"\"Initialize timing manager with persistent CSV storage.\"\"\"\n",
    "        import csv\n",
    "        import time\n",
    "        import pandas as pd\n",
    "        from datetime import datetime\n",
    "        from pathlib import Path\n",
    "        \n",
    "        self.csv = csv\n",
    "        self.pd = pd\n",
    "        self.time = time\n",
    "        self.datetime = datetime\n",
    "        self.Path = Path\n",
    "        \n",
    "        self.storage_file = Path(storage_file)\n",
    "        self.timings = self._load_timings()\n",
    "        self.current_objective = None\n",
    "        self.start_time = None\n",
    "    \n",
    "    def _load_timings(self) -> dict:\n",
    "        \"\"\"Load timing history from CSV file.\"\"\"\n",
    "        if not self.storage_file.exists():\n",
    "            return {}\n",
    "        \n",
    "        try:\n",
    "            df = self.pd.read_csv(self.storage_file)\n",
    "            \n",
    "            # Convert CSV back to internal dict structure\n",
    "            timings = {}\n",
    "            for _, row in df.iterrows():\n",
    "                obj_name = row['objective_name']\n",
    "                if obj_name not in timings:\n",
    "                    timings[obj_name] = {\n",
    "                        \"first_run\": None,\n",
    "                        \"runs\": [],\n",
    "                        \"total_runs\": 0,\n",
    "                        \"average_time\": None,\n",
    "                        \"min_time\": None,\n",
    "                        \"max_time\": None\n",
    "                    }\n",
    "                \n",
    "                # Add this run\n",
    "                timings[obj_name][\"runs\"].append({\n",
    "                    \"time\": row['time'],\n",
    "                    \"timestamp\": row['timestamp'],\n",
    "                    \"run_number\": row['run_number']\n",
    "                })\n",
    "                \n",
    "                # CRITICAL: Set first_run from CSV's first_run column for THIS objective only\n",
    "                # This ensures each objective compares against its own first_run, not another objective's\n",
    "                if timings[obj_name][\"first_run\"] is None:\n",
    "                    # Use the first_run value from CSV (same for all rows of same objective)\n",
    "                    first_run_value = row.get('first_run')\n",
    "                    if self.pd.notna(first_run_value):\n",
    "                        timings[obj_name][\"first_run\"] = float(first_run_value)\n",
    "                        timings[obj_name][\"first_run_timestamp\"] = row.get('first_run_timestamp', row['timestamp'])\n",
    "            \n",
    "            # Recalculate statistics\n",
    "            for obj_name in timings:\n",
    "                data = timings[obj_name]\n",
    "                data[\"total_runs\"] = len(data[\"runs\"])\n",
    "                if data[\"runs\"]:\n",
    "                    times = [r[\"time\"] for r in data[\"runs\"]]\n",
    "                    data[\"average_time\"] = sum(times) / len(times)\n",
    "                    data[\"min_time\"] = min(times)\n",
    "                    data[\"max_time\"] = max(times)\n",
    "            \n",
    "            return timings\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Could not load timings: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _save_timings(self):\n",
    "        \"\"\"Save timing history to CSV file.\"\"\"\n",
    "        try:\n",
    "            self.storage_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # Convert internal dict structure to CSV rows\n",
    "            rows = []\n",
    "            for obj_name, data in self.timings.items():\n",
    "                for run in data[\"runs\"]:\n",
    "                    rows.append({\n",
    "                        'objective_name': obj_name,\n",
    "                        'run_number': run['run_number'],\n",
    "                        'time': run['time'],\n",
    "                        'timestamp': run['timestamp'],\n",
    "                        'first_run': data.get('first_run'),\n",
    "                        'first_run_timestamp': data.get('first_run_timestamp', '')\n",
    "                    })\n",
    "            \n",
    "            if rows:\n",
    "                df = self.pd.DataFrame(rows)\n",
    "                # Sort by objective name and run number\n",
    "                df = df.sort_values(['objective_name', 'run_number'])\n",
    "                df.to_csv(self.storage_file, index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Could not save timings: {e}\")\n",
    "    \n",
    "    def objective(self, objective_name: str):\n",
    "        \"\"\"\n",
    "        Context manager for timing an objective.\n",
    "        \n",
    "        Usage:\n",
    "            with env.timer.objective(\"Objective 1\"):\n",
    "                # Your code\n",
    "                pass\n",
    "        \"\"\"\n",
    "        return ObjectiveTimer(self, objective_name)\n",
    "    \n",
    "    def record_time(self, objective_name: str, elapsed_time: float):\n",
    "        \"\"\"Record execution time for an objective.\"\"\"\n",
    "        if objective_name not in self.timings:\n",
    "            self.timings[objective_name] = {\n",
    "                \"first_run\": None,\n",
    "                \"runs\": [],\n",
    "                \"total_runs\": 0,\n",
    "                \"average_time\": None,\n",
    "                \"min_time\": None,\n",
    "                \"max_time\": None\n",
    "            }\n",
    "        \n",
    "        timing_data = self.timings[objective_name]\n",
    "        timing_data[\"runs\"].append({\n",
    "            \"time\": elapsed_time,\n",
    "            \"timestamp\": self.datetime.now().isoformat(),\n",
    "            \"run_number\": timing_data[\"total_runs\"] + 1\n",
    "        })\n",
    "        \n",
    "        # Store first run separately\n",
    "        if timing_data[\"first_run\"] is None:\n",
    "            timing_data[\"first_run\"] = elapsed_time\n",
    "            timing_data[\"first_run_timestamp\"] = self.datetime.now().isoformat()\n",
    "        \n",
    "        # Update statistics\n",
    "        timing_data[\"total_runs\"] = len(timing_data[\"runs\"])\n",
    "        times = [r[\"time\"] for r in timing_data[\"runs\"]]\n",
    "        timing_data[\"average_time\"] = sum(times) / len(times)\n",
    "        timing_data[\"min_time\"] = min(times)\n",
    "        timing_data[\"max_time\"] = max(times)\n",
    "        \n",
    "        # Save to file\n",
    "        self._save_timings()\n",
    "    \n",
    "    def get_stats(self, objective_name: str) -> Optional[dict]:\n",
    "        \"\"\"\n",
    "        Get statistics for a specific objective.\n",
    "        CRITICAL: Only returns stats for the specified objective_name - ensures comparison is within objective.\n",
    "        \"\"\"\n",
    "        if objective_name not in self.timings:\n",
    "            return None\n",
    "        \n",
    "        # Get data for THIS specific objective only (filters by objective_name)\n",
    "        data = self.timings[objective_name]\n",
    "        \n",
    "        # Verify we're getting the right objective's data\n",
    "        if not data.get(\"runs\"):\n",
    "            return None\n",
    "        \n",
    "        return {\n",
    "            \"first_run\": data.get(\"first_run\"),  # This is THIS objective's first_run only\n",
    "            \"first_run_timestamp\": data.get(\"first_run_timestamp\"),\n",
    "            \"first_run_number\": 1,  # Always 1 for first run\n",
    "            \"last_run\": data[\"runs\"][-1][\"time\"] if data[\"runs\"] else None,\n",
    "            \"average_time\": data.get(\"average_time\"),\n",
    "            \"min_time\": data.get(\"min_time\"),\n",
    "            \"max_time\": data.get(\"max_time\"),\n",
    "            \"total_runs\": data.get(\"total_runs\", 0),\n",
    "            \"improvement\": self._calculate_improvement(data)\n",
    "        }\n",
    "    \n",
    "    def get_first_run_info(self, objective_name: str) -> Optional[dict]:\n",
    "        \"\"\"\n",
    "        Get first run information for an objective (easier access).\n",
    "        \n",
    "        Args:\n",
    "            objective_name: Name of objective (e.g., \"Objective 1\")\n",
    "            \n",
    "        Returns:\n",
    "            Dict with first_run info or None if not found:\n",
    "            {\n",
    "                \"time\": float,  # First run time in seconds\n",
    "                \"formatted_time\": str,  # Human-readable time\n",
    "                \"timestamp\": str,  # ISO timestamp\n",
    "                \"run_number\": int,  # Always 1\n",
    "                \"objective_number\": Optional[int]  # Extracted number (e.g., 1)\n",
    "            }\n",
    "        \"\"\"\n",
    "        stats = self.get_stats(objective_name)\n",
    "        if not stats or stats.get(\"first_run\") is None:\n",
    "            return None\n",
    "        \n",
    "        # Extract objective number (use ObjectiveNames directly - it's in same cell)\n",
    "        obj_number = ObjectiveNames.get_number(objective_name)\n",
    "        \n",
    "        return {\n",
    "            \"time\": stats[\"first_run\"],\n",
    "            \"formatted_time\": self._format_time(stats[\"first_run\"]),\n",
    "            \"timestamp\": stats.get(\"first_run_timestamp\", \"Unknown\"),\n",
    "            \"run_number\": 1,\n",
    "            \"objective_number\": obj_number\n",
    "        }\n",
    "    \n",
    "    def _calculate_improvement(self, data: dict) -> Optional[float]:\n",
    "        \"\"\"Calculate improvement percentage from first to average.\"\"\"\n",
    "        if data.get(\"first_run\") and data.get(\"average_time\"):\n",
    "            first = data[\"first_run\"]\n",
    "            avg = data[\"average_time\"]\n",
    "            if first > 0:\n",
    "                improvement = ((first - avg) / first) * 100\n",
    "                return improvement\n",
    "        return None\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"Print summary of all objective timings.\"\"\"\n",
    "        if not self.timings:\n",
    "            print(\"üìä No timing data recorded yet.\")\n",
    "            print(\"   Use: with env.timer.objective('Objective 1'): ...\")\n",
    "            return\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(\"üìä OBJECTIVE TIMING SUMMARY\")\n",
    "        print(\"=\" * 80)\n",
    "        print()\n",
    "        \n",
    "        for obj_name in sorted(self.timings.keys()):\n",
    "            stats = self.get_stats(obj_name)\n",
    "            if not stats:\n",
    "                continue\n",
    "            \n",
    "            print(f\"üéØ {obj_name}:\")\n",
    "            print(f\"   First Run:  {self._format_time(stats['first_run'])}\")\n",
    "            if stats['total_runs'] > 1:\n",
    "                print(f\"   Last Run:   {self._format_time(stats['last_run'])}\")\n",
    "                print(f\"   Average:    {self._format_time(stats['average_time'])}\")\n",
    "                print(f\"   Min:        {self._format_time(stats['min_time'])}\")\n",
    "                print(f\"   Max:        {self._format_time(stats['max_time'])}\")\n",
    "                print(f\"   Total Runs: {stats['total_runs']}\")\n",
    "                if stats['improvement']:\n",
    "                    sign = \"‚Üì\" if stats['improvement'] > 0 else \"‚Üë\"\n",
    "                    print(f\"   Improvement: {sign}{abs(stats['improvement']):.1f}% vs first run\")\n",
    "            print()\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "    \n",
    "    def _format_time(self, seconds: Optional[float]) -> str:\n",
    "        \"\"\"Format time in human-readable format.\"\"\"\n",
    "        if seconds is None:\n",
    "            return \"N/A\"\n",
    "        \n",
    "        if seconds < 60:\n",
    "            return f\"{seconds:.2f}s\"\n",
    "        elif seconds < 3600:\n",
    "            minutes = int(seconds // 60)\n",
    "            secs = seconds % 60\n",
    "            return f\"{minutes}m {secs:.2f}s\"\n",
    "        else:\n",
    "            hours = int(seconds // 3600)\n",
    "            minutes = int((seconds % 3600) // 60)\n",
    "            secs = seconds % 60\n",
    "            return f\"{hours}h {minutes}m {secs:.2f}s\"\n",
    "    \n",
    "    def compare_first_vs_subsequent(self, objective_name: str):\n",
    "        \"\"\"Compare first run vs subsequent runs for an objective.\"\"\"\n",
    "        if objective_name not in self.timings:\n",
    "            print(f\"‚ùå No data for {objective_name}\")\n",
    "            return\n",
    "        \n",
    "        data = self.timings[objective_name]\n",
    "        if data[\"total_runs\"] < 2:\n",
    "            print(f\"‚ö†Ô∏è  Need at least 2 runs to compare. Current: {data['total_runs']}\")\n",
    "            return\n",
    "        \n",
    "        first = data.get(\"first_run\")\n",
    "        if first is None or not isinstance(first, (int, float)):\n",
    "            print(f\"‚ùå No first run data for {objective_name}\")\n",
    "            return\n",
    "        \n",
    "        # Type assertion: first is guaranteed to be float here\n",
    "        first_float: float = float(first)\n",
    "        \n",
    "        # Extract subsequent run times, ensuring they are floats\n",
    "        subsequent_times: list[float] = [float(r[\"time\"]) for r in data[\"runs\"][1:] if r.get(\"time\") is not None]\n",
    "        if not subsequent_times:\n",
    "            print(f\"‚ö†Ô∏è  No subsequent runs to compare\")\n",
    "            return\n",
    "        \n",
    "        # Calculate average - both values are guaranteed to be float\n",
    "        total_time: float = sum(subsequent_times)\n",
    "        count: int = len(subsequent_times)\n",
    "        avg_subsequent: float = total_time / count\n",
    "        \n",
    "        print(f\"üìä {objective_name} - First vs Subsequent Runs:\")\n",
    "        print(f\"   First Run:     {self._format_time(first_float)}\")\n",
    "        print(f\"   Avg Subsequent: {self._format_time(avg_subsequent)}\")\n",
    "        \n",
    "        if first_float > 0:\n",
    "            # Both first_float and avg_subsequent are guaranteed to be float here\n",
    "            improvement: float = ((first_float - avg_subsequent) / first_float) * 100\n",
    "            print(f\"   Improvement:  {improvement:+.1f}%\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    def clear_objective(self, objective_name: str) -> bool:\n",
    "        \"\"\"\n",
    "        Clear timing data for a specific objective.\n",
    "        \n",
    "        Args:\n",
    "            objective_name: Name of objective to clear\n",
    "            \n",
    "        Returns:\n",
    "            True if cleared, False if objective not found\n",
    "        \"\"\"\n",
    "        if objective_name not in self.timings:\n",
    "            print(f\"‚ö†Ô∏è  No data found for '{objective_name}'\")\n",
    "            return False\n",
    "        \n",
    "        del self.timings[objective_name]\n",
    "        self._save_timings()\n",
    "        print(f\"‚úÖ Cleared timing data for '{objective_name}'\")\n",
    "        return True\n",
    "    \n",
    "    def clear_all(self, confirm: bool = False) -> bool:\n",
    "        \"\"\"\n",
    "        Clear all timing data and delete CSV file.\n",
    "        \n",
    "        Args:\n",
    "            confirm: If True, clears without asking. If False, prints warning but doesn't clear.\n",
    "                    Use clear_all(confirm=True) to actually clear.\n",
    "            \n",
    "        Returns:\n",
    "            True if cleared, False if not confirmed\n",
    "        \"\"\"\n",
    "        if not confirm:\n",
    "            print(\"‚ö†Ô∏è  WARNING: This will delete ALL timing data!\")\n",
    "            print(f\"   File: {self.storage_file}\")\n",
    "            print(f\"   Current objectives: {list(self.timings.keys())}\")\n",
    "            print()\n",
    "            print(\"   To confirm, use: env.timer.clear_all(confirm=True)\")\n",
    "            return False\n",
    "        \n",
    "        # Clear in-memory data\n",
    "        self.timings = {}\n",
    "        \n",
    "        # Delete CSV file\n",
    "        if self.storage_file.exists():\n",
    "            try:\n",
    "                self.storage_file.unlink()\n",
    "                print(f\"‚úÖ Deleted timing file: {self.storage_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Could not delete file: {e}\")\n",
    "        \n",
    "        print(\"‚úÖ All timing data cleared!\")\n",
    "        return True\n",
    "    \n",
    "    def reset_objective(self, objective_name: str) -> bool:\n",
    "        \"\"\"\n",
    "        Reset timing data for a specific objective (keeps file, removes objective).\n",
    "        Same as clear_objective() - provided for clarity.\n",
    "        \n",
    "        Args:\n",
    "            objective_name: Name of objective to reset\n",
    "            \n",
    "        Returns:\n",
    "            True if reset, False if objective not found\n",
    "        \"\"\"\n",
    "        return self.clear_objective(objective_name)\n",
    "\n",
    "\n",
    "class ObjectiveTimer:\n",
    "    \"\"\"\n",
    "    Context manager for timing a single objective execution.\n",
    "    \n",
    "    Usage:\n",
    "        # Using standardized name (recommended):\n",
    "        with env.timer.objective(ObjectiveNames.OBJECTIVE_1):\n",
    "            # Your code\n",
    "            pass\n",
    "        \n",
    "        # Or using string directly:\n",
    "        with env.timer.objective(\"Objective 1\"):\n",
    "            # Your code\n",
    "            pass\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, manager: ObjectiveTimingManager, objective_name: str):\n",
    "        self.manager = manager\n",
    "        self.objective_name = objective_name\n",
    "        self.start_time = None\n",
    "        # Extract objective number for better display\n",
    "        self.objective_number = self._extract_number(objective_name)\n",
    "    \n",
    "    def _extract_number(self, name: str) -> Optional[int]:\n",
    "        \"\"\"Extract objective number from name.\"\"\"\n",
    "        try:\n",
    "            parts = name.split()\n",
    "            if len(parts) >= 2 and parts[0].lower() == \"objective\":\n",
    "                return int(parts[1])\n",
    "        except (ValueError, IndexError):\n",
    "            pass\n",
    "        return None\n",
    "    \n",
    "    def __enter__(self):\n",
    "        self.start_time = self.manager.time.time()\n",
    "        # Show objective number and run count\n",
    "        stats = self.manager.get_stats(self.objective_name)\n",
    "        run_number = (stats['total_runs'] + 1) if stats else 1\n",
    "        print(f\"‚è±Ô∏è  Starting: {self.objective_name} (Run #{run_number})\")\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        # Ensure start_time is set (should always be set by __enter__)\n",
    "        if self.start_time is None:\n",
    "            print(f\"‚ö†Ô∏è  Warning: start_time was not set for {self.objective_name}\")\n",
    "            return False\n",
    "        \n",
    "        # Calculate elapsed time - both values are guaranteed to be float now\n",
    "        current_time: float = self.manager.time.time()\n",
    "        start_time: float = self.start_time\n",
    "        elapsed: float = current_time - start_time\n",
    "        \n",
    "        self.manager.record_time(self.objective_name, elapsed)\n",
    "        \n",
    "        # Get stats for THIS specific objective only (filters by objective_name)\n",
    "        stats = self.manager.get_stats(self.objective_name)\n",
    "        is_first = stats and stats[\"total_runs\"] == 1\n",
    "        \n",
    "        # Get first run info for easier display\n",
    "        first_run_info = self.manager.get_first_run_info(self.objective_name)\n",
    "        \n",
    "        print(f\"‚úÖ Completed: {self.objective_name}\")\n",
    "        print(f\"   Time: {self.manager._format_time(elapsed)}\")\n",
    "        \n",
    "        if is_first:\n",
    "            print(f\"   üìù First run recorded (Run #1)\")\n",
    "        else:\n",
    "            # CRITICAL: Compare only with THIS objective's first run (not other objectives)\n",
    "            if stats is None or first_run_info is None:\n",
    "                print(f\"   ‚ö†Ô∏è  No stats available for {self.objective_name}\")\n",
    "            else:\n",
    "                # Use first_run_info for cleaner display\n",
    "                first_float: float = float(first_run_info[\"time\"])\n",
    "                diff: float = elapsed - first_float\n",
    "                pct: float = ((elapsed - first_float) / first_float) * 100 if first_float > 0 else 0.0\n",
    "                sign = \"+\" if diff > 0 else \"\"\n",
    "                \n",
    "                # Enhanced display with run number and first run info\n",
    "                run_number = stats[\"total_runs\"]\n",
    "                first_run_display = first_run_info[\"formatted_time\"]\n",
    "                \n",
    "                # Show comparison - explicitly shows it's comparing within the same objective\n",
    "                print(f\"   üìä Run #{run_number} vs First Run (Run #1, {first_run_display}): {sign}{self.manager._format_time(abs(diff))} ({sign}{pct:.1f}%)\")\n",
    "                \n",
    "                # Show when first run happened (if available)\n",
    "                if first_run_info.get(\"timestamp\") and first_run_info[\"timestamp\"] != \"Unknown\":\n",
    "                    try:\n",
    "                        from datetime import datetime\n",
    "                        first_dt = datetime.fromisoformat(first_run_info[\"timestamp\"].replace('Z', '+00:00'))\n",
    "                        print(f\"   üìÖ First run: {first_dt.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        print()\n",
    "        return False  # Don't suppress exceptions\n",
    "\n",
    "# ============================================================================\n",
    "# Global Environment Instance - Use this everywhere!\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"OBJECTIVE 0: PREREQUISITES & SETUP\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIONAL: Clear timing data to start fresh\n",
    "# ============================================================================\n",
    "# Uncomment the line below if you want to reset all timing data:\n",
    "# RESET_TIMINGS = True\n",
    "# Otherwise, timing data will accumulate (recommended to track history)\n",
    "RESET_TIMINGS = False\n",
    "\n",
    "# Start timing Objective 0\n",
    "import time\n",
    "objective0_start = time.time()\n",
    "\n",
    "# Create global environment configuration instance\n",
    "env = EnvironmentConfig()\n",
    "print()\n",
    "\n",
    "# Install packages\n",
    "env.install_packages()\n",
    "print()\n",
    "\n",
    "# Import libraries\n",
    "if env.import_libraries():\n",
    "    # Get and authenticate token\n",
    "    env.get_token()\n",
    "    if env.hf_token:\n",
    "        env.authenticate_hf()\n",
    "    print()\n",
    "    env.print_summary()\n",
    "    \n",
    "    # Initialize timing manager and attach to env\n",
    "    timer = ObjectiveTimingManager(storage_file=\"data/objective_timings.csv\")\n",
    "    env.timer = timer\n",
    "    \n",
    "    # Optional: Clear all timing data if RESET_TIMINGS is True\n",
    "    if RESET_TIMINGS:\n",
    "        env.timer.clear_all(confirm=True)\n",
    "        print()\n",
    "    \n",
    "    # Make env and classes globally available\n",
    "    globals()['env'] = env\n",
    "    globals()['EnvironmentConfig'] = EnvironmentConfig\n",
    "    globals()['ObjectiveTimingManager'] = ObjectiveTimingManager\n",
    "    globals()['ObjectiveNames'] = ObjectiveNames  # For standardized objective naming\n",
    "    \n",
    "    # Backward compatibility - set old global variables\n",
    "    globals()['IN_COLAB'] = env.is_colab\n",
    "    globals()['HAS_GPU'] = env.has_gpu\n",
    "    globals()['hf_token'] = env.hf_token\n",
    "    \n",
    "    # Record Objective 0 execution time\n",
    "    objective0_elapsed = time.time() - objective0_start\n",
    "    env.timer.record_time(\"Objective 0\", objective0_elapsed)\n",
    "    \n",
    "    print(\"üí° Timing system ready! Use: with env.timer.objective('Objective 1'): ...\")\n",
    "    print()\n",
    "    print(f\"‚úÖ Objective 0 completed in {env.timer._format_time(objective0_elapsed)}\")\n",
    "    print()\n",
    "else:\n",
    "    print(\"‚ùå Setup incomplete. Please fix errors above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective 1: Design System Prompts\n",
    "\n",
    "### üéØ Goal\n",
    "Create a system prompt that defines Mistral's role as a customer service assistant for an e-commerce business context, enabling consistent, context-aware responses aligned with business requirements.\n",
    "\n",
    "<details>\n",
    "<summary><b>üì• Prerequisites</b> (Click to expand)</summary>\n",
    "\n",
    "| Item | Source | Required | Description |\n",
    "|------|--------|----------|-------------|\n",
    "| `hf_token` | Setup cell (Objective 0) | ‚úÖ Yes | Hugging Face API token for model access |\n",
    "| GPU (recommended) | Colab settings | ‚ö†Ô∏è Optional | Faster model loading (~1-2 min vs 3-5 min on CPU) |\n",
    "| Python packages | Setup cell | ‚úÖ Yes | `transformers`, `torch`, `huggingface_hub` |\n",
    "\n",
    "**Note:** If running locally without GPU, model loading will be slower but fully functional.\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary><b>üìã System Prompt Components</b> (Click to expand)</summary>\n",
    "\n",
    "The system prompt includes:\n",
    "\n",
    "| Component | Content |\n",
    "|-----------|---------|\n",
    "| **Role Definition** | Customer service assistant for e-commerce platform |\n",
    "| **Business Context** | Company name, product categories, policies |\n",
    "| **Knowledge Base** | Products, shipping, returns, customer service hours, contact methods |\n",
    "| **Communication Guidelines** | Tone (warm, professional), style (clear, concise) |\n",
    "| **Limitation Handling** | Instructions for unknown information (\"I don't have that information\") |\n",
    "| **Response Format** | Structure expectations for consistent outputs |\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>üí° Tips</b> (Click to expand)</summary>\n",
    "\n",
    "- **First Run**: Be patient - model download is large (~14GB)\n",
    "- **GPU Recommended**: Significantly faster inference (2-3x speedup)\n",
    "- **Prompt Tuning**: Experiment with different prompt structures to optimize responses\n",
    "- **Caching**: Model is cached globally - subsequent runs are instant\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Next Step:** Proceed to Objective 2 to generate Q&A database using the system prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è  Starting: Objective 1 (Run #2)\n",
      "Objective 1: Creating System Prompt\n",
      "\n",
      "‚úÖ Authenticated with Hugging Face\n",
      "‚ö° Using cached model from global cache: mistralai/Mistral-7B-Instruct-v0.3\n",
      "‚úÖ System prompt created (1995 chars)\n",
      "\n",
      "Sample Response:\n",
      "--------------------------------------------------\n",
      "Hello!\n",
      "\n",
      "I'm delighted to assist you at GreenTech Marketplace. Our customer service hours are as follows:\n",
      "\n",
      "- Monday-Friday: 9 AM - 6 PM EST\n",
      "- Saturday: 10 AM - 4 PM EST\n",
      "- Closed Sunday\n",
      "\n",
      "You can reach us through various methods during these hours:\n",
      "\n",
      "1. Email: support@greentechmarketplace.com\n",
      "2. Phone: 1-800-GREEN-TECH\n",
      "3. Live chat: Available during our business hours\n",
      "\n",
      "If you have any questions outside of our business hours, feel free to leave an email, and we'll get back to you as soon as we're open again. Thank you for choosing GreenTech Marketplace! If you have more questions or need further assistance, please don't hesitate to ask.\n",
      "--------------------------------------------------\n",
      "‚úÖ Saved: data/system_prompt_engineering/system_prompt.txt\n",
      "‚úÖ Saved: data/system_prompt_engineering/test_response.txt\n",
      "‚úÖ Model verified - Loaded on CPU\n",
      "‚úÖ Prompt verified - Length: 1995 chars\n",
      "\n",
      "‚úÖ Objective 1 complete - Model on CPU, Prompt: 1995 chars\n",
      "‚úÖ Completed: Objective 1\n",
      "   Time: 33.38s\n",
      "   üìä Run #2 vs First Run (Run #1, 33.09s): +0.29s (+0.9%)\n",
      "   üìÖ First run: 2025-11-29 10:44:29\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# OBJECTIVE 1: DESIGN SYSTEM PROMPTS FOR LLM-BASED CUSTOMER SERVICE\n",
    "# ============================================================================\n",
    "#\n",
    "# LEARNING OBJECTIVES DEMONSTRATED:\n",
    "#   1. System Prompt Engineering - Crafting prompts that shape LLM behavior\n",
    "#   2. Modular Design - SOLID, KISS, DRY principles in practice\n",
    "#\n",
    "# THEORETICAL BACKGROUND:\n",
    "#   System prompts serve as the \"constitution\" for LLM behavior, establishing:\n",
    "#   - Role Identity: Who the model should act as\n",
    "#   - Knowledge Boundaries: What the model knows and doesn't know\n",
    "#   - Behavioral Constraints: Response style, escalation rules\n",
    "#   - Domain Context: Business-specific information\n",
    "#\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# ============================================================================\n",
    "# InferenceEngine Class - Model Loading, Generation & Verification\n",
    "# ============================================================================\n",
    "class InferenceEngine:\n",
    "    \"\"\"\n",
    "    Handles all model inference operations: loading, generation, and verification.\n",
    "    Reusable across all objectives - follows Single Responsibility Principle.\n",
    "    Uses env from Objective 0 - no duplicate code!\n",
    "    \n",
    "    Usage:\n",
    "        engine = InferenceEngine(env)\n",
    "        tokenizer, model = engine.load_model(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "        response = engine.generate_response(tokenizer, model, prompt)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        \"\"\"\n",
    "        Initialize with environment config from Objective 0.\n",
    "        \n",
    "        Args:\n",
    "            env: EnvironmentConfig instance from Objective 0\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        \n",
    "        # Use libraries from env (no duplicate imports!)\n",
    "        self.torch = env.torch\n",
    "        self.AutoTokenizer = env.AutoTokenizer\n",
    "        self.AutoModelForCausalLM = env.AutoModelForCausalLM\n",
    "        \n",
    "        # Model cache (keyed by model name)\n",
    "        self._model_cache = {}\n",
    "        self._tokenizer_cache = {}\n",
    "    \n",
    "    def load_model(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        force_reload: bool = False,\n",
    "        use_cache: bool = True\n",
    "    ) -> Tuple:\n",
    "        \"\"\"\n",
    "        Load model with caching support.\n",
    "        Uses env from Objective 0 - automatically handles GPU/CPU!\n",
    "        \n",
    "        Args:\n",
    "            model_name: Hugging Face model identifier\n",
    "            force_reload: Force reload even if cached\n",
    "            use_cache: Use global cache (for sharing across objectives)\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (tokenizer, model)\n",
    "        \"\"\"\n",
    "        # Check instance cache first (fastest - instant return)\n",
    "        if not force_reload and model_name in self._model_cache:\n",
    "            print(f\"‚ö° Using cached model from instance cache: {model_name}\")\n",
    "            return self._tokenizer_cache[model_name], self._model_cache[model_name]\n",
    "        \n",
    "        # Check global cache (for sharing across objectives)\n",
    "        if use_cache and not force_reload:\n",
    "            global_key_tokenizer = f\"{model_name}_tokenizer\"\n",
    "            global_key_model = f\"{model_name}_model\"\n",
    "            \n",
    "            if global_key_tokenizer in globals() and global_key_model in globals():\n",
    "                tokenizer = globals()[global_key_tokenizer]\n",
    "                model = globals()[global_key_model]\n",
    "                \n",
    "                # Store in instance cache for faster access next time\n",
    "                self._tokenizer_cache[model_name] = tokenizer\n",
    "                self._model_cache[model_name] = model\n",
    "                \n",
    "                print(f\"‚ö° Using cached model from global cache: {model_name}\")\n",
    "                return tokenizer, model\n",
    "        \n",
    "        print(f\"Loading {model_name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Use env libraries - works in both Colab and local!\n",
    "            tokenizer = self.AutoTokenizer.from_pretrained(model_name)\n",
    "            model = self.AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=self.torch.float16 if self.env.has_gpu else self.torch.float32,\n",
    "                device_map=\"auto\" if self.env.has_gpu else None,\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "            \n",
    "            # Use env.device - no if statement needed!\n",
    "            if not self.env.has_gpu:\n",
    "                model = model.to(self.env.device)\n",
    "            \n",
    "            # Store in caches\n",
    "            self._tokenizer_cache[model_name] = tokenizer\n",
    "            self._model_cache[model_name] = model\n",
    "            \n",
    "            # Store in globals for sharing across objectives\n",
    "            if use_cache:\n",
    "                globals()[f\"{model_name}_tokenizer\"] = tokenizer\n",
    "                globals()[f\"{model_name}_model\"] = model\n",
    "            \n",
    "            device_display = 'GPU' if self.env.has_gpu else 'CPU'\n",
    "            print(f\"‚úÖ Model loaded on {device_display}\")\n",
    "            \n",
    "            return tokenizer, model\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load model {model_name}: {e}\")\n",
    "    \n",
    "    def generate_response(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        model,\n",
    "        formatted_prompt: str,\n",
    "        max_new_tokens: int = 200,\n",
    "        temperature: float = 0.7,\n",
    "        top_p: float = 0.9\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Generate model response.\n",
    "        Uses env from Objective 0 - no if statements needed!\n",
    "        \n",
    "        Args:\n",
    "            tokenizer: Model tokenizer\n",
    "            model: Loaded model instance\n",
    "            formatted_prompt: Formatted prompt string\n",
    "            max_new_tokens: Max tokens to generate\n",
    "            temperature: Sampling temperature (0=deterministic, 1=creative)\n",
    "            top_p: Nucleus sampling threshold\n",
    "            \n",
    "        Returns:\n",
    "            str: Generated response\n",
    "            \n",
    "        Raises:\n",
    "            RuntimeError: If generation fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            inputs = tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
    "            \n",
    "            # Use env.has_gpu - no if statement needed!\n",
    "            if self.env.has_gpu:\n",
    "                inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "            \n",
    "            with self.torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    temperature=temperature,\n",
    "                    do_sample=True,\n",
    "                    top_p=top_p,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            # Decode only new tokens\n",
    "            input_length = inputs['input_ids'].shape[1]\n",
    "            generated_tokens = outputs[0][input_length:]\n",
    "            response = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except self.torch.cuda.OutOfMemoryError:\n",
    "            raise RuntimeError(\"GPU out of memory. Try reducing max_new_tokens or use CPU.\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Generation failed: {e}\")\n",
    "    \n",
    "    def verify_model(self, tokenizer, model) -> bool:\n",
    "        \"\"\"\n",
    "        Verify model is loaded and ready for inference.\n",
    "        \n",
    "        Args:\n",
    "            tokenizer: Model tokenizer to verify\n",
    "            model: Model instance to verify\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if model is ready, False otherwise\n",
    "        \"\"\"\n",
    "        errors = []\n",
    "        \n",
    "        if not tokenizer:\n",
    "            errors.append(\"‚ùå Tokenizer not loaded\")\n",
    "        if not model:\n",
    "            errors.append(\"‚ùå Model not loaded\")\n",
    "        \n",
    "        if errors:\n",
    "            print(\"‚ùå Model verification failed:\", \"\\n\".join(errors))\n",
    "            return False\n",
    "        \n",
    "        device_display = 'GPU' if self.env.has_gpu else 'CPU'\n",
    "        print(f\"‚úÖ Model verified - Loaded on {device_display}\")\n",
    "        return True\n",
    "    \n",
    "    def get_device_info(self) -> dict:\n",
    "        \"\"\"Get device information using env.\"\"\"\n",
    "        return self.env.get_device_info()\n",
    "    \n",
    "    def clear_cache(self, model_name: str = None):\n",
    "        \"\"\"\n",
    "        Clear model cache.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Specific model to clear, or None to clear all\n",
    "        \"\"\"\n",
    "        if model_name:\n",
    "            self._model_cache.pop(model_name, None)\n",
    "            self._tokenizer_cache.pop(model_name, None)\n",
    "        else:\n",
    "            self._model_cache.clear()\n",
    "            self._tokenizer_cache.clear()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SystemPromptEngineer Class - Centralized Objective 1 Logic\n",
    "# ============================================================================\n",
    "class SystemPromptEngineer:\n",
    "    \"\"\"\n",
    "    System prompt engineering for Objective 1.\n",
    "    Focuses ONLY on prompt creation, formatting, and file I/O.\n",
    "    Uses env from Objective 0 - no duplicate code, no if statements needed!\n",
    "    \n",
    "    Usage:\n",
    "        engineer = SystemPromptEngineer(env)\n",
    "        prompt = engineer.create_system_prompt()\n",
    "        formatted = engineer.format_prompt(prompt, question)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configuration constants\n",
    "    MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "    MAX_NEW_TOKENS = 200\n",
    "    TEMPERATURE = 0.7\n",
    "    TOP_P = 0.9\n",
    "    OUTPUT_DIR = \"data/system_prompt_engineering\"\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        \"\"\"\n",
    "        Initialize with environment config.\n",
    "        \n",
    "        Args:\n",
    "            env: EnvironmentConfig instance from Objective 0\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.system_prompt = None\n",
    "    \n",
    "    def create_system_prompt(\n",
    "        self,\n",
    "        business_name: str = \"GreenTech Marketplace\",\n",
    "        business_type: str = \"e-commerce\",\n",
    "        support_email: str = \"support@greentechmarketplace.com\",\n",
    "        support_phone: str = \"1-800-GREEN-TECH\"\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Create optimized customer service system prompt.\n",
    "        \n",
    "        Prompt Engineering Best Practices:\n",
    "        1. Role Definition: Clear, specific identity\n",
    "        2. Task Boundaries: Explicit scope\n",
    "        3. Knowledge Base: Comprehensive domain info\n",
    "        4. Behavioral Rules: Tone and style guidelines\n",
    "        5. Edge Case Handling: Unknown information handling\n",
    "        6. Output Format: Response structure expectations\n",
    "        \n",
    "        Args:\n",
    "            business_name: Company name\n",
    "            business_type: Type of business\n",
    "            support_email: Support email\n",
    "            support_phone: Support phone\n",
    "            \n",
    "        Returns:\n",
    "            str: Formatted system prompt\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"You are a friendly and knowledgeable customer service assistant for {business_name}, a leading {business_type} platform specializing in sustainable technology products.\n",
    "\n",
    "## YOUR ROLE\n",
    "You are the first point of contact for customers. Your expertise includes product information, orders, shipping, returns, and general inquiries. You embody the company's commitment to sustainability and excellent service.\n",
    "\n",
    "## KNOWLEDGE BASE\n",
    "\n",
    "**Products:**\n",
    "- Solar panels, energy-efficient appliances, smart home devices, eco-friendly accessories\n",
    "- Warranty: 1-3 years depending on product category\n",
    "\n",
    "**Shipping:**\n",
    "- Standard: 5-7 business days (free over $75)\n",
    "- Express: 2-3 business days (additional fee)\n",
    "\n",
    "**Returns & Refunds:**\n",
    "- 30-day return policy for unopened items in original packaging\n",
    "- Refunds processed within 5-7 business days after receipt\n",
    "\n",
    "**Customer Service Hours:**\n",
    "- Monday-Friday: 9 AM - 6 PM EST\n",
    "- Saturday: 10 AM - 4 PM EST\n",
    "- Closed Sunday\n",
    "\n",
    "**Contact Methods:**\n",
    "- Email: {support_email}\n",
    "- Phone: {support_phone}\n",
    "- Live chat: Available during business hours in EST \n",
    "\n",
    "## COMMUNICATION GUIDELINES\n",
    "\n",
    "**Tone:** Warm, professional, solution-oriented\n",
    "**Style:** Clear, concise, helpful\n",
    "\n",
    "**Always:**\n",
    "- Greet customers warmly\n",
    "- Acknowledge their concerns before providing solutions\n",
    "- Provide specific, actionable information\n",
    "- Include relevant timeframes and next steps\n",
    "- Thank them for choosing {business_name}\n",
    "\n",
    "**Never:**\n",
    "- Make promises you cannot keep\n",
    "- Provide information not in your knowledge base\n",
    "- Use technical jargon without explanation\n",
    "\n",
    "## HANDLING LIMITATIONS\n",
    "\n",
    "If you don't know the answer:\n",
    "1. Acknowledge the question honestly\n",
    "2. Explain that the information is not in your current knowledge base\n",
    "3. Offer to connect them with a specialist or provide contact information\n",
    "4. Suggest alternative resources if available\n",
    "\n",
    "## RESPONSE FORMAT\n",
    "\n",
    "Keep responses concise but complete. Structure longer responses with clear sections. Always end with an offer to help further.\"\"\"\n",
    "        \n",
    "        self.system_prompt = prompt\n",
    "        return prompt\n",
    "    \n",
    "    def format_prompt(self, system_prompt: str, user_input: str) -> str:\n",
    "        \"\"\"\n",
    "        Format for Mistral Instruct template.\n",
    "        \n",
    "        Template: <s>[INST] {system} {user} [/INST]\n",
    "        \n",
    "        Args:\n",
    "            system_prompt: System prompt\n",
    "            user_input: User question\n",
    "            \n",
    "        Returns:\n",
    "            str: Formatted prompt\n",
    "        \"\"\"\n",
    "        return f\"<s>[INST] {system_prompt}\\n\\nCustomer Question: {user_input} [/INST]\"\n",
    "    \n",
    "    def save_system_prompt(self, filename: str = \"system_prompt.txt\") -> str:\n",
    "        \"\"\"Save system prompt to file.\"\"\"\n",
    "        if not self.system_prompt:\n",
    "            raise ValueError(\"No system prompt created. Call create_system_prompt() first.\")\n",
    "        \n",
    "        os.makedirs(self.OUTPUT_DIR, exist_ok=True)\n",
    "        filepath = os.path.join(self.OUTPUT_DIR, filename)\n",
    "        with open(filepath, 'w') as f:\n",
    "            f.write(self.system_prompt)\n",
    "        print(f\"‚úÖ Saved: {filepath}\")\n",
    "        return filepath\n",
    "    \n",
    "    def save_response(self, response: str, question: str, filename: str = \"test_response.txt\") -> str:\n",
    "        \"\"\"Save generated response to file.\"\"\"\n",
    "        os.makedirs(self.OUTPUT_DIR, exist_ok=True)\n",
    "        filepath = os.path.join(self.OUTPUT_DIR, filename)\n",
    "        with open(filepath, 'w') as f:\n",
    "            f.write(f\"Question: {question}\\n\\n\")\n",
    "            f.write(f\"Response:\\n{response}\")\n",
    "        print(f\"‚úÖ Saved: {filepath}\")\n",
    "        return filepath\n",
    "    \n",
    "    def verify_prompt(self) -> bool:\n",
    "        \"\"\"\n",
    "        Verify prompt engineering components only.\n",
    "        Follows SRP - only verifies SystemPromptEngineer responsibilities.\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if prompt engineering is complete\n",
    "        \"\"\"\n",
    "        errors = []\n",
    "        \n",
    "        # Verify system prompt\n",
    "        if not self.system_prompt:\n",
    "            errors.append(\"‚ùå System prompt not created\")\n",
    "        elif len(self.system_prompt) < 100:\n",
    "            errors.append(f\"‚ùå System prompt too short ({len(self.system_prompt)} chars)\")\n",
    "        \n",
    "        # Check files exist\n",
    "        prompt_file = os.path.join(self.OUTPUT_DIR, \"system_prompt.txt\")\n",
    "        response_file = os.path.join(self.OUTPUT_DIR, \"test_response.txt\")\n",
    "        \n",
    "        if not os.path.exists(prompt_file):\n",
    "            errors.append(\"‚ùå system_prompt.txt not found\")\n",
    "        if not os.path.exists(response_file):\n",
    "            errors.append(\"‚ùå test_response.txt not found\")\n",
    "        \n",
    "        # Print results\n",
    "        if errors:\n",
    "            print(\"‚ùå Prompt verification failed:\", \"\\n\".join(errors))\n",
    "            return False\n",
    "        \n",
    "        print(f\"‚úÖ Prompt verified - Length: {len(self.system_prompt)} chars\")\n",
    "        return True\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXECUTION - Uses env from Objective 0, wrapped with timing\n",
    "# ============================================================================\n",
    "\n",
    "# Verify env is available from Objective 0\n",
    "if 'env' not in globals():\n",
    "    raise RuntimeError(\"‚ùå 'env' not found! Please run Objective 0 (Prerequisites & Setup) first.\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXECUTION - Orchestrates Objective 1 workflow\n",
    "# ============================================================================\n",
    "# Class provides capabilities, execution orchestrates the workflow\n",
    "# This follows better separation of concerns!\n",
    "\n",
    "with env.timer.objective(\"Objective 1\"):\n",
    "    print(\"Objective 1: Creating System Prompt\\n\")\n",
    "    \n",
    "    # Reuse InferenceEngine from globals if available (performance optimization!)\n",
    "    if 'inference_engine' in globals() and isinstance(globals()['inference_engine'], InferenceEngine):\n",
    "        inference_engine = globals()['inference_engine']\n",
    "        print(\"‚ôªÔ∏è  Reusing existing InferenceEngine (model cache preserved)\\n\")\n",
    "    else:\n",
    "        # Create InferenceEngine (can be shared across objectives for efficiency)\n",
    "        inference_engine = InferenceEngine(env)\n",
    "    \n",
    "    # Create SystemPromptEngineer instance (prompt engineering only)\n",
    "    system_prompt_engineer = SystemPromptEngineer(env)\n",
    "    \n",
    "    # Authenticate using env from Objective 0\n",
    "    if env.hf_token:\n",
    "        env.authenticate_hf()\n",
    "    \n",
    "    # Load model using InferenceEngine (model operation)\n",
    "    tokenizer, model = inference_engine.load_model(system_prompt_engineer.MODEL_NAME)\n",
    "    \n",
    "    # Create system prompt (prompt engineering operation)\n",
    "    system_prompt = system_prompt_engineer.create_system_prompt()\n",
    "    globals()['system_prompt'] = system_prompt\n",
    "    print(f\"‚úÖ System prompt created ({len(system_prompt)} chars)\\n\")\n",
    "    \n",
    "    # Test with sample question\n",
    "    test_question = \"What are your store hours and how can I contact customer support?\"\n",
    "    formatted_prompt = system_prompt_engineer.format_prompt(system_prompt, test_question)\n",
    "    \n",
    "    # Generate response using InferenceEngine (model operation)\n",
    "    generated_response = inference_engine.generate_response(\n",
    "        tokenizer, model, formatted_prompt,\n",
    "        max_new_tokens=system_prompt_engineer.MAX_NEW_TOKENS,\n",
    "        temperature=system_prompt_engineer.TEMPERATURE,\n",
    "        top_p=system_prompt_engineer.TOP_P\n",
    "    )\n",
    "    \n",
    "    print(\"Sample Response:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(generated_response)\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Save files (prompt engineering operation)\n",
    "    system_prompt_engineer.save_system_prompt()\n",
    "    system_prompt_engineer.save_response(generated_response, test_question)\n",
    "    \n",
    "    # Verify both components separately (SRP: each verifies its own responsibility)\n",
    "    inference_engine.verify_model(tokenizer, model)\n",
    "    system_prompt_engineer.verify_prompt()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Objective 1 complete - Model on {'GPU' if env.has_gpu else 'CPU'}, Prompt: {len(system_prompt)} chars\")\n",
    "    \n",
    "    # Store in globals for other objectives\n",
    "    globals()['InferenceEngine'] = InferenceEngine\n",
    "    globals()['SystemPromptEngineer'] = SystemPromptEngineer\n",
    "    globals()['system_prompt'] = system_prompt\n",
    "    globals()['inference_engine'] = inference_engine  # Reusable across objectives!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective 2: Generate Custom Q&A Databases for E-commerce Customer Service\n",
    "\n",
    "### üéØ Goal\n",
    "Generate 21 Q&A pairs (18 answerable + 3 unanswerable) covering e-commerce customer service topics to create a domain-specific knowledge base for the RAG system.\n",
    "\n",
    "<details>\n",
    "<summary><b>üì• Prerequisites</b> (Click to expand)</summary>\n",
    "\n",
    "| Item | Source | Required | Description |\n",
    "|------|--------|----------|-------------|\n",
    "| `mistral_model` | Objective 1 | ‚úÖ Yes | Mistral model for generating Q&A pairs |\n",
    "| `mistral_tokenizer` | Objective 1 | ‚úÖ Yes | Tokenizer for text processing and encoding |\n",
    "| `system_prompt` | Objective 1 | ‚úÖ Yes | System prompt defining business context and role |\n",
    "| `hf_token` | Objective 0 | ‚úÖ Yes | Hugging Face API token for model access |\n",
    "\n",
    "**Note:** Objective 1 must be completed first to provide the model and system prompt.\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>üîß Core Concepts</b> (Click to expand)</summary>\n",
    "\n",
    "| Concept | Description |\n",
    "|--------|-------------|\n",
    "| **Q&A Generation** | Using LLMs to create domain-specific question-answer pairs that serve as the knowledge base for RAG systems |\n",
    "| **Zero-Shot Prompting** | Providing instructions without examples - the model generates Q&A pairs based solely on the system prompt and task description |\n",
    "| **Few-Shot Prompting** | Providing examples in the prompt to guide the model's output format and style - improves consistency and quality |\n",
    "| **System Prompt Reuse** | Leveraging the system prompt from Objective 1 to ensure Q&A pairs align with the business context and customer service role |\n",
    "| **Answerable Questions** | Questions that can be answered from the business knowledge base (products, policies, procedures) |\n",
    "| **Unanswerable Questions** | Questions outside the knowledge base scope (competitor info, personal advice, future events) |\n",
    "| **Delimiter Parsing** | Using a consistent delimiter (`|||`) to reliably extract structured data from LLM-generated text |\n",
    "| **DataFrame Structure** | Converting Q&A pairs to pandas DataFrame for easy filtering, querying, and analysis |\n",
    "\n",
    "**Why This Matters:**\n",
    "A well-structured Q&A database is the foundation of the RAG system. It provides the knowledge that will be retrieved and used to generate accurate, context-aware responses. Using the system prompt from Objective 1 ensures consistency with the customer service role and business context.\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>üì§ Outputs</b> (Click to expand)</summary>\n",
    "\n",
    "| Variable | Type | Description |\n",
    "|----------|------|-------------|\n",
    "| `qa_database` | `List[Dict]` | List of 21 Q&A dictionaries with keys: `category`, `answerable`, `question`, `answer` |\n",
    "| `qa_df` | `pd.DataFrame` | DataFrame with Q&A pairs plus computed columns: `answer_length`, `word_count` |\n",
    "\n",
    "**Files Created:**\n",
    "| File | Location | Description |\n",
    "|------|----------|-------------|\n",
    "| `qa_database.csv` | `data/qa_database/` | All 21 Q&A pairs with answerable flag, ready for RAG system |\n",
    "\n",
    "**Key Functions Created:**\n",
    "- `generate_qa_for_category()`: Generates Q&A pairs for a specific category\n",
    "- `generate_full_qa_database()`: Generates complete 21-pair database\n",
    "- `qa_to_dataframe()`: Converts Q&A list to DataFrame with metrics\n",
    "- `display_statistics()`: Shows database coverage and quality metrics\n",
    "- `display_all_qa_pairs()`: Prints all Q&A pairs with comments\n",
    "- `save_qa_to_csv()`: Saves Q&A database to CSV file\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>üìã Q&A Database Structure</b> (Click to expand)</summary>\n",
    "\n",
    "**Answerable Categories (18 pairs):**\n",
    "\n",
    "| Category | Count | Topics Covered |\n",
    "|----------|-------|---------------|\n",
    "| **products** | 3 | Types of products, specifications, features |\n",
    "| **shipping** | 3 | Delivery times, shipping costs, free shipping threshold, tracking |\n",
    "| **returns** | 3 | Return policy, refund process, conditions, 30-day policy |\n",
    "| **customer_service** | 3 | Business hours (Mon-Fri 9AM-6PM, Sat 10AM-4PM), contact email and phone |\n",
    "| **warranty** | 3 | Warranty duration (1-3 years), coverage, claims process |\n",
    "| **orders** | 3 | Order status, tracking, modifications, cancellations |\n",
    "\n",
    "**Unanswerable Types (3 pairs):**\n",
    "\n",
    "| Type | Count | Description |\n",
    "|------|-------|-------------|\n",
    "| **competitor** | 1 | Questions about competitor pricing, products, or comparisons |\n",
    "| **personal_advice** | 1 | Questions asking for personal recommendations or opinions |\n",
    "| **future_events** | 1 | Questions about future sales, unreleased products, or predictions |\n",
    "\n",
    "**Data Structure:**\n",
    "```python\n",
    "{\n",
    "    'category': 'shipping',\n",
    "    'answerable': True,\n",
    "    'question': 'What is your shipping policy?',\n",
    "    'answer': 'We offer standard shipping (5-7 business days, free over $75)...'\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>üìö Learning Objectives Demonstrated</b> (Click to expand)</summary>\n",
    "\n",
    "1. **Zero-Shot Prompting**: Using instructions without examples to guide LLM output, leveraging the model's pre-training and context from system prompts\n",
    "2. **System Prompt Reuse**: Integrating previously created system prompts to maintain consistency across objectives and ensure domain alignment\n",
    "3. **LLM-Based Content Generation**: Using language models to create structured domain-specific content\n",
    "4. **Data Structure Design**: Designing flexible data formats (list + DataFrame) for different access patterns\n",
    "5. **Parsing LLM Output**: Reliable extraction of structured data from free-form LLM responses using delimiter-based parsing\n",
    "6. **Knowledge Base Construction**: Building domain-specific knowledge bases for RAG systems\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>üí° Tips</b> (Click to expand)</summary>\n",
    "\n",
    "- **System Prompt Integration**: The system prompt from Objective 1 is automatically embedded in generation prompts - ensure Objective 1 completed successfully\n",
    "- **Zero-Shot vs Few-Shot**: Zero-shot is used here for efficiency; few-shot could improve consistency but requires example formatting\n",
    "- **Generation Time**: 21 LLM calls take 2-3 minutes - be patient\n",
    "- **Delimiter Choice**: `|||` is chosen because it's unlikely to appear in natural text\n",
    "- **Answerable Flag**: Essential for testing system's ability to decline unanswerable questions\n",
    "- **DataFrame Benefits**: Use `qa_df[qa_df['answerable'] == True]` for easy filtering\n",
    "- **Category Coverage**: Each category has 3 pairs to ensure sufficient coverage\n",
    "- **Unanswerable Testing**: The 3 unanswerable pairs test different types of out-of-scope questions\n",
    "- **Prompt Context**: The system prompt's first 1200 characters are included to provide business context without exceeding token limits\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Next Step:** Proceed to Objective 3 to build the FAISS vector database from the Q&A pairs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "OBJECTIVE 2 (REVISED): GENERATE CUSTOM Q&A DATABASE\n",
      "======================================================================\n",
      "   ‚úÖ Prerequisites validated\n",
      "\n",
      "ü§ñ Generating Q&A with strict Mistral prompts...\n",
      "\n",
      "üìó Generating answerable Q&A...\n",
      "   ‚Üí products...\n",
      "   ‚Üí shipping...\n",
      "   ‚Üí returns...\n",
      "   ‚Üí customer_service...\n",
      "   ‚Üí warranty...\n",
      "   ‚Üí orders...\n",
      "\n",
      "üìï Generating unanswerable Q&A...\n",
      "   ‚Üí competitor...\n",
      "   ‚Üí personal_advice...\n",
      "   ‚Üí future_events...\n",
      "\n",
      "üéâ Generated 21 total pairs (18 answerable, 3 unanswerable)\n",
      "   üíæ Saved to data/qa_database/qa_database.csv\n",
      "\n",
      "======================================================================\n",
      "üìã GENERATED Q&A DATABASE\n",
      "======================================================================\n",
      "\n",
      "All 21 pairs:\n",
      "\n",
      "\n",
      "[1] ‚úÖ Answerable | Category: products\n",
      "    Q: What types of products do you offer on your platform?\n",
      "    A: We specialize in sustainable technology products, offering solar panels, energy-efficient appliances, smart home devices, and eco-friendly accessories...\n",
      "\n",
      "[2] ‚úÖ Answerable | Category: products\n",
      "    Q: How long are the warranties on your products?\n",
      "    A: Our warranties vary depending on the product category, ranging from 1 to 3 years.\n",
      "\n",
      "[3] ‚úÖ Answerable | Category: products\n",
      "    Q: I'm interested in learning more about your return policy. Can you help?\n",
      "    A: Of course! You can return any unopened item in its original packaging within 30 days of delivery for a full refund. Please contact our support team to...\n",
      "\n",
      "[4] ‚úÖ Answerable | Category: shipping\n",
      "    Q: How long does it take for my order to be delivered?\n",
      "    A: Delivery times vary depending on the shipping method you choose. Standard shipping takes 5-7 business days, and express shipping takes 2-3 business da...\n",
      "\n",
      "[5] ‚úÖ Answerable | Category: shipping\n",
      "    Q: What is the minimum order value for free shipping?\n",
      "    A: We offer free standard shipping on orders over $75.\n",
      "\n",
      "[6] ‚úÖ Answerable | Category: shipping\n",
      "    Q: How can I track my package?\n",
      "    A: All our orders are shipped with tracking numbers, so you can easily track your package through the carrier's website.\n",
      "\n",
      "[7] ‚úÖ Answerable | Category: returns\n",
      "    Q: I just received my order and I'd like to return a product because it's not what I expected. Can I do that?\n",
      "    A: Absolutely! You can return any unopened item in its original packaging within 30 days of delivery for a full refund. Simply contact our support team t...\n",
      "\n",
      "[8] ‚úÖ Answerable | Category: returns\n",
      "    Q: What is your refund window for returned items?\n",
      "    A: Our refund policy allows you to return unopened items within 30 days of delivery. Refunds are processed within 5-7 business days after we receive the ...\n",
      "\n",
      "[9] ‚úÖ Answerable | Category: returns\n",
      "    Q: I'm considering buying a solar panel, but I'm not sure about the warranty. How long does it last?\n",
      "    A: Our solar panels come with a warranty of 1-3 years depending on the specific product category. Please refer to the product description for detailed wa...\n",
      "\n",
      "[10] ‚úÖ Answerable | Category: customer_service\n",
      "    Q: What are the shipping options and costs for GreenTech Marketplace?\n",
      "    A: We offer standard shipping (5-7 business days) for free on orders over $75. Express shipping (2-3 business days) is available for an additional $15. A...\n",
      "\n",
      "[11] ‚úÖ Answerable | Category: customer_service\n",
      "    Q: I'd like to return a product, but I've already opened the packaging. Can I still do that?\n",
      "    A: Unfortunately, we only accept returns for unopened items in their original packaging within 30 days of delivery. For any other inquiries, feel free to...\n",
      "\n",
      "[12] ‚úÖ Answerable | Category: customer_service\n",
      "    Q: What are the customer service hours for GreenTech Marketplace?\n",
      "    A: Our customer service team is available Monday through Friday from 9 AM to 6 PM EST, and on Saturdays from 10 AM to 4 PM EST. You can reach us via emai...\n",
      "\n",
      "[13] ‚úÖ Answerable | Category: warranty\n",
      "    Q: How long does shipping usually take?\n",
      "    A: We offer standard shipping that takes 5-7 business days, which is free for orders over $75. Express shipping is also available in 2-3 business days fo...\n",
      "\n",
      "[14] ‚úÖ Answerable | Category: warranty\n",
      "    Q: What happens if I need to make a claim within the warranty period?\n",
      "    A: If you have a product issue within the warranty period, please contact our support team for further assistance. We'll help you through the claims proc...\n",
      "\n",
      "[15] ‚úÖ Answerable | Category: warranty\n",
      "    Q: What is your customer service contact information?\n",
      "    A: You can reach our customer service team via email at support@greentechmarketplace.com or by phone during our business hours, which are Monday-Friday: ...\n",
      "\n",
      "[16] ‚úÖ Answerable | Category: orders\n",
      "    Q: How can I track my order?\n",
      "    A: To track your order, you can use the tracking number provided in your shipping confirmation email. If you haven't received it yet, feel free to contac...\n",
      "\n",
      "[17] ‚úÖ Answerable | Category: orders\n",
      "    Q: I need to change my order, is that possible?\n",
      "    A: Absolutely! If you need to make changes to your order, please contact our support team as soon as possible. We'll do our best to accommodate your requ...\n",
      "\n",
      "[18] ‚úÖ Answerable | Category: orders\n",
      "    Q: What should I do if I want to cancel my order?\n",
      "    A: To cancel your order, please contact our support team immediately. If your order hasn't been shipped yet, we should be able to cancel it for you. Once...\n",
      "\n",
      "[19] ‚ùå Unanswerable | Category: competitor\n",
      "    Q: What is the password for my competitor's account?\n",
      "    A: I'm sorry, but I'm unable to provide account information for competitors. I can help you with questions about your ShopSmart account, password recover...\n",
      "\n",
      "[20] ‚ùå Unanswerable | Category: personal_advice\n",
      "    Q: What is the best day to shop for discounts?\n",
      "    A: I'm sorry, but I cannot provide information about the best day for shopping discounts as it's beyond ShopSmart's scope. I can help you with questions ...\n",
      "\n",
      "[21] ‚ùå Unanswerable | Category: future_events\n",
      "    Q: What is the password for the admin panel?\n",
      "    A: I'm sorry, but I'm unable to provide password information as it's outside our knowledge base. I can assist you with questions about our products, ship...\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "üîç OBJECTIVE 2 VERIFICATION\n",
      "======================================================================\n",
      "‚úÖ Count correct: 21 pairs\n",
      "‚úÖ Structure correct: All pairs have required keys\n",
      "‚úÖ Answerable pairs: 18\n",
      "‚úÖ Unanswerable pairs: 3\n",
      "‚úÖ DataFrame correct: 21 rows\n",
      "‚úÖ CSV file exists: data/qa_database/qa_database.csv\n",
      "\n",
      "‚úÖ Objective 2 Complete - All checks passed!\n",
      "   ‚Ä¢ Total Q&A pairs: 21\n",
      "   ‚Ä¢ Answerable: 18\n",
      "   ‚Ä¢ Unanswerable: 3\n",
      "   ‚Ä¢ DataFrame: 21 rows √ó 7 columns\n",
      "   ‚Ä¢ CSV file: data/qa_database/qa_database.csv\n",
      "======================================================================\n",
      "\n",
      "Done! Q&A database is ready.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# OBJECTIVE 2 : GENERATE CUSTOM Q&A DATABASES \n",
    "# ============================================================================\n",
    "#\n",
    "#\n",
    "# NOTE: Requires Objective 1 (mistral_model, mistral_tokenizer, system_prompt)\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIG\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "TEMPERATURE = 0.7\n",
    "TOP_P = 0.9\n",
    "CONTEXT_CHARS = 900           # Smaller excerpt increases relevance\n",
    "DELIMITER = \"|||\"\n",
    "\n",
    "OUTPUT_DIR = \"data/qa_database\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# VALIDATION\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def validate_prerequisites():\n",
    "    required = [\"mistral_model\", \"mistral_tokenizer\", \"system_prompt\"]\n",
    "    missing = [r for r in required if r not in globals()]\n",
    "    if missing:\n",
    "        raise RuntimeError(\n",
    "            f\"‚ùå Objective 2 requires Objective 1 first. Missing: {missing}\"\n",
    "        )\n",
    "    print(\"   ‚úÖ Prerequisites validated\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CATEGORY DEFINITIONS\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "QA_CATEGORIES = [\n",
    "    (\"products\", \"types of products, solar panels, smart devices, eco-friendly items\"),\n",
    "    (\"shipping\", \"delivery times, shipping cost, free shipping threshold, tracking\"),\n",
    "    (\"returns\", \"return policy, refund window, 30-day policy, conditions\"),\n",
    "    (\"customer_service\", \"hours (Mon‚ÄìFri 9‚Äì6, Sat 10‚Äì4), email, phone support\"),\n",
    "    (\"warranty\", \"coverage periods 1‚Äì3 years, claims process\"),\n",
    "    (\"orders\", \"order status, modifying or cancelling orders, tracking numbers\"),\n",
    "]\n",
    "\n",
    "UNANSWERABLE_TYPES = [\n",
    "    (\"competitor\", \"questions about competitor prices or product comparisons\"),\n",
    "    (\"personal_advice\", \"questions asking for personal recommendations or opinions\"),\n",
    "    (\"future_events\", \"questions about upcoming sales or unreleased products\"),\n",
    "]\n",
    "\n",
    "PAIRS_PER_CATEGORY = 3\n",
    "UNANSWERABLE_PER_TYPE = 1\n",
    "\n",
    "ANSWERABLE_TOTAL = len(QA_CATEGORIES) * PAIRS_PER_CATEGORY\n",
    "UNANSWERABLE_TOTAL = len(UNANSWERABLE_TYPES) * UNANSWERABLE_PER_TYPE\n",
    "TOTAL_PAIRS = ANSWERABLE_TOTAL + UNANSWERABLE_TOTAL\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PROMPT TEMPLATES \n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "ANSWERABLE_PROMPT = \"\"\"\n",
    "You are generating REALISTIC customer service Q&A pairs for the ShopSmart e-commerce support assistant.\n",
    "\n",
    "Generate EXACTLY {num_pairs} Q&A pairs about the topic below.\n",
    "\n",
    "TOPIC FOCUS:\n",
    "{description}\n",
    "\n",
    "BUSINESS CONTEXT (from system prompt):\n",
    "{context}\n",
    "\n",
    "CRITICAL: You MUST output valid JSON only. No other text before or after.\n",
    "\n",
    "OUTPUT FORMAT (JSON array):\n",
    "[\n",
    "  {{\"question\": \"What is your shipping policy?\", \"answer\": \"We offer standard shipping (5-7 business days) for free on orders over $75. Express shipping (2-3 business days) is available for an additional $15. All orders are shipped with tracking numbers.\"}},\n",
    "  {{\"question\": \"Can I return a product if I'm not satisfied?\", \"answer\": \"Yes, you can return any unopened item in its original packaging within 30 days of delivery for a full refund. Simply contact our support team to initiate the return process.\"}},\n",
    "  {{\"question\": \"What are your customer service hours?\", \"answer\": \"Our customer service team is available Monday through Friday from 9 AM to 6 PM EST, and on Saturdays from 10 AM to 4 PM EST. You can reach us via email at support@greentechmarketplace.com or by phone.\"}}\n",
    "]\n",
    "\n",
    "CONTENT RULES:\n",
    "- Questions MUST sound like real customers asking natural questions.\n",
    "- Answers MUST be 2‚Äì3 sentences with concrete details (times, numbers, policies, contact info).\n",
    "- Stay entirely within ShopSmart policies from the business context.\n",
    "- DO NOT hallucinate unsupported information.\n",
    "\n",
    "OUTPUT: Return a valid JSON array with EXACTLY {num_pairs} objects, each with \"question\" and \"answer\" fields.\n",
    "\"\"\"\n",
    "\n",
    "UNANSWERABLE_PROMPT = \"\"\"\n",
    "Generate EXACTLY {num_pairs} UNANSWERABLE customer Q&A pairs.\n",
    "\n",
    "TOPIC TYPE OUT OF SCOPE:\n",
    "{description}\n",
    "\n",
    "CRITICAL: You MUST output valid JSON only. No other text before or after.\n",
    "\n",
    "OUTPUT FORMAT (JSON array):\n",
    "[\n",
    "  {{\"question\": \"What are your competitor's prices?\", \"answer\": \"I'm sorry, but I'm unable to provide information about competitor pricing as it's outside our knowledge base. However, I'd be happy to help you with questions about our own products, shipping options, or return policies.\"}},\n",
    "  {{\"question\": \"Can you recommend the best restaurant in New York?\", \"answer\": \"I apologize, but I cannot provide personal recommendations or advice about restaurants as that's beyond ShopSmart's scope. I can assist you with questions about our products, shipping, returns, or order tracking.\"}},\n",
    "  {{\"question\": \"When will you release new products next month?\", \"answer\": \"I'm unable to provide information about upcoming product releases or future events as that information isn't available in our knowledge base. However, I can help you with questions about our current product catalog, shipping options, or warranty information.\"}}\n",
    "]\n",
    "\n",
    "REFUSAL RULES:\n",
    "- Question MUST be outside ShopSmart's knowledge base (competitor info, personal advice, future events, etc.)\n",
    "- Answer MUST politely decline and explain you cannot provide that information\n",
    "- Answer MUST offer what you *can* help with (shipping, returns, products, warranty, orders, etc.)\n",
    "- Answer MUST be 2 sentences\n",
    "\n",
    "OUTPUT: Return a valid JSON array with EXACTLY {num_pairs} objects, each with \"question\" and \"answer\" fields.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# GENERATION FUNCTION\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def mistral_generate(prompt: str, max_tokens: int = 600) -> str:\n",
    "    tokenizer = mistral_tokenizer\n",
    "    model = mistral_model\n",
    "\n",
    "    formatted = f\"<s>[INST] {prompt} [/INST]\"\n",
    "\n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\")\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_p=TOP_P,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Extract generated tokens (skip prompt)\n",
    "    inp_len = inputs[\"input_ids\"].shape[1]\n",
    "    gen_tokens = outputs[0][inp_len:]\n",
    "    text = tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PARSER (ROBUST - IMPROVED)\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def parse_qa_lines(text: str, answerable: bool, debug: bool = False) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Parse Q&A pairs from model output.\n",
    "    Expected format: JSON array with objects containing \"question\" and \"answer\" fields.\n",
    "    \"\"\"\n",
    "    qa_list = []\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"      [DEBUG] Raw model output ({len(text)} chars):\")\n",
    "        print(f\"      {repr(text[:300])}...\")\n",
    "    \n",
    "    # Try to extract JSON from the text (might have extra text before/after)\n",
    "    text_clean = text.strip()\n",
    "    \n",
    "    # Find JSON array in the text (handle cases where model adds extra text)\n",
    "    json_start = text_clean.find('[')\n",
    "    json_end = text_clean.rfind(']') + 1\n",
    "    \n",
    "    if json_start == -1 or json_end == 0:\n",
    "        if debug:\n",
    "            print(f\"      [DEBUG] No JSON array found in output\")\n",
    "        return qa_list\n",
    "    \n",
    "    json_text = text_clean[json_start:json_end]\n",
    "    \n",
    "    try:\n",
    "        parsed_data = json.loads(json_text)\n",
    "        \n",
    "        if not isinstance(parsed_data, list):\n",
    "            if debug:\n",
    "                print(f\"      [DEBUG] JSON is not an array, got: {type(parsed_data)}\")\n",
    "            return qa_list\n",
    "        \n",
    "        for item in parsed_data:\n",
    "            if isinstance(item, dict) and \"question\" in item and \"answer\" in item:\n",
    "                q = item[\"question\"].strip()\n",
    "                a = item[\"answer\"].strip()\n",
    "                \n",
    "                # Validate content\n",
    "                if q and a and len(q) > 3 and len(a) > 10:\n",
    "                    qa_list.append({\n",
    "                        \"question\": q,\n",
    "                        \"answer\": a,\n",
    "                        \"answerable\": bool(answerable)\n",
    "                    })\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"      [DEBUG] Parsed {len(qa_list)} pairs from JSON\")\n",
    "            if len(qa_list) > 0:\n",
    "                print(f\"      [DEBUG] First parsed pair:\")\n",
    "                print(f\"        Q: {qa_list[0]['question'][:80]}...\")\n",
    "                print(f\"        A: {qa_list[0]['answer'][:80]}...\")\n",
    "    \n",
    "    except json.JSONDecodeError as e:\n",
    "        if debug:\n",
    "            print(f\"      [DEBUG] JSON decode error: {e}\")\n",
    "            print(f\"      [DEBUG] Attempted to parse: {json_text[:200]}...\")\n",
    "    \n",
    "    return qa_list\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# HIGH-LEVEL GENERATORS (WITH RETRY LOGIC)\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def generate_answerable(category: str, description: str, n: int, max_retries: int = 3) -> List[Dict]:\n",
    "    \"\"\"Generate answerable Q&A pairs with retry logic if parsing fails.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        prompt = ANSWERABLE_PROMPT.format(\n",
    "            num_pairs=n,\n",
    "            description=description,\n",
    "            context=system_prompt[:CONTEXT_CHARS]\n",
    "        )\n",
    "        raw = mistral_generate(prompt, max_tokens=800)  # Increased tokens for better generation\n",
    "        parsed = parse_qa_lines(raw, True, debug=(attempt == max_retries - 1))\n",
    "        \n",
    "        if len(parsed) >= n:\n",
    "            return parsed[:n]\n",
    "        elif len(parsed) > 0:\n",
    "            # If we got some pairs but not enough, return what we have\n",
    "            print(f\"      ‚ö†Ô∏è  Got {len(parsed)}/{n} pairs (attempt {attempt + 1})\")\n",
    "            if attempt < max_retries - 1:\n",
    "                continue\n",
    "            return parsed\n",
    "    \n",
    "    # If all retries failed, return empty list\n",
    "    print(f\"      ‚ùå Failed to generate pairs after {max_retries} attempts\")\n",
    "    return []\n",
    "\n",
    "\n",
    "def generate_unanswerable(category: str, description: str, n: int, max_retries: int = 3) -> List[Dict]:\n",
    "    \"\"\"Generate unanswerable Q&A pairs with retry logic if parsing fails.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        prompt = UNANSWERABLE_PROMPT.format(\n",
    "            num_pairs=n,\n",
    "            description=description\n",
    "        )\n",
    "        raw = mistral_generate(prompt, max_tokens=600)\n",
    "        parsed = parse_qa_lines(raw, False, debug=(attempt == max_retries - 1))\n",
    "        \n",
    "        if len(parsed) >= n:\n",
    "            return parsed[:n]\n",
    "        elif len(parsed) > 0:\n",
    "            print(f\"      ‚ö†Ô∏è  Got {len(parsed)}/{n} pairs (attempt {attempt + 1})\")\n",
    "            if attempt < max_retries - 1:\n",
    "                continue\n",
    "            return parsed\n",
    "    \n",
    "    print(f\"      ‚ùå Failed to generate pairs after {max_retries} attempts\")\n",
    "    return []\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# FULL DATABASE GENERATOR\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def generate_full_qa_database():\n",
    "    db = []\n",
    "\n",
    "    print(\"\\nüìó Generating answerable Q&A...\")\n",
    "    for cat, desc in QA_CATEGORIES:\n",
    "        print(f\"   ‚Üí {cat}...\")\n",
    "        pairs = generate_answerable(cat, desc, PAIRS_PER_CATEGORY)\n",
    "        for p in pairs:\n",
    "            p[\"category\"] = cat\n",
    "        db.extend(pairs)\n",
    "\n",
    "    print(\"\\nüìï Generating unanswerable Q&A...\")\n",
    "    for cat, desc in UNANSWERABLE_TYPES:\n",
    "        print(f\"   ‚Üí {cat}...\")\n",
    "        pairs = generate_unanswerable(cat, desc, UNANSWERABLE_PER_TYPE)\n",
    "        for p in pairs:\n",
    "            p[\"category\"] = cat\n",
    "        db.extend(pairs)\n",
    "\n",
    "    print(f\"\\nüéâ Generated {len(db)} total pairs \"\n",
    "          f\"({ANSWERABLE_TOTAL} answerable, {UNANSWERABLE_TOTAL} unanswerable)\")\n",
    "\n",
    "    return db\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONVERSION / DISPLAY / SAVE ‚Äî SAME AS ORIGINAL\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def qa_to_dataframe(qa_list: List[Dict]) -> pd.DataFrame:\n",
    "    df = pd.DataFrame(qa_list)\n",
    "    df[\"question_length\"] = df.question.str.len()\n",
    "    df[\"answer_length\"] = df.answer.str.len()\n",
    "    df[\"word_count\"] = df.answer.str.split().str.len()\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_qa_to_csv(qa_list, filename=\"qa_database.csv\"):\n",
    "    df = pd.DataFrame(qa_list)\n",
    "    path = os.path.join(OUTPUT_DIR, filename)\n",
    "    df.to_csv(path, index=False)\n",
    "    print(f\"   üíæ Saved to {path}\")\n",
    "    return path\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# VERIFICATION FUNCTION\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def verify_objective2():\n",
    "    \"\"\"\n",
    "    Verify that Objective 2 completed successfully.\n",
    "    Checks all variables, counts, structure, and files.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"üîç OBJECTIVE 2 VERIFICATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    errors = []\n",
    "    warnings = []\n",
    "    \n",
    "    # Check if variables exist\n",
    "    if 'qa_database' not in globals():\n",
    "        errors.append(\"‚ùå qa_database not found\")\n",
    "    if 'qa_df' not in globals():\n",
    "        errors.append(\"‚ùå qa_df not found\")\n",
    "    \n",
    "    if errors:\n",
    "        print(\"\\n\".join(errors))\n",
    "        print(\"=\"*70)\n",
    "        return False\n",
    "    \n",
    "    # Check count\n",
    "    actual_count = len(qa_database)\n",
    "    expected_count = TOTAL_PAIRS\n",
    "    \n",
    "    if actual_count != expected_count:\n",
    "        errors.append(f\"‚ùå Wrong count: Expected {expected_count}, got {actual_count}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Count correct: {actual_count} pairs\")\n",
    "    \n",
    "    # Check structure\n",
    "    required_keys = [\"question\", \"answer\", \"answerable\", \"category\"]\n",
    "    for i, pair in enumerate(qa_database):\n",
    "        for key in required_keys:\n",
    "            if key not in pair:\n",
    "                errors.append(f\"‚ùå Pair {i+1} missing key: {key}\")\n",
    "            elif pair[key] is None:\n",
    "                errors.append(f\"‚ùå Pair {i+1} has None value for {key}\")\n",
    "            elif isinstance(pair[key], str) and len(pair[key].strip()) == 0:\n",
    "                errors.append(f\"‚ùå Pair {i+1} has empty {key}\")\n",
    "            # Note: For boolean fields like \"answerable\", False is a valid value, not empty\n",
    "    \n",
    "    if not errors:\n",
    "        print(f\"‚úÖ Structure correct: All pairs have required keys\")\n",
    "    \n",
    "    # Check distribution\n",
    "    answerable_count = sum(1 for p in qa_database if p.get(\"answerable\") == True)\n",
    "    unanswerable_count = sum(1 for p in qa_database if p.get(\"answerable\") == False)\n",
    "    \n",
    "    if answerable_count != ANSWERABLE_TOTAL:\n",
    "        warnings.append(f\"‚ö†Ô∏è  Answerable count: Expected {ANSWERABLE_TOTAL}, got {answerable_count}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Answerable pairs: {answerable_count}\")\n",
    "    \n",
    "    if unanswerable_count != UNANSWERABLE_TOTAL:\n",
    "        warnings.append(f\"‚ö†Ô∏è  Unanswerable count: Expected {UNANSWERABLE_TOTAL}, got {unanswerable_count}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Unanswerable pairs: {unanswerable_count}\")\n",
    "    \n",
    "    # Check DataFrame\n",
    "    if len(qa_df) != actual_count:\n",
    "        errors.append(f\"‚ùå DataFrame count mismatch: {len(qa_df)} != {actual_count}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ DataFrame correct: {len(qa_df)} rows\")\n",
    "    \n",
    "    # Check file exists\n",
    "    csv_path = os.path.join(OUTPUT_DIR, \"qa_database.csv\")\n",
    "    if not os.path.exists(csv_path):\n",
    "        errors.append(f\"‚ùå CSV file not found: {csv_path}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ CSV file exists: {csv_path}\")\n",
    "        # Verify CSV content\n",
    "        try:\n",
    "            df_check = pd.read_csv(csv_path)\n",
    "            if len(df_check) != actual_count:\n",
    "                warnings.append(f\"‚ö†Ô∏è  CSV row count: {len(df_check)} != {actual_count}\")\n",
    "        except Exception as e:\n",
    "            warnings.append(f\"‚ö†Ô∏è  Could not verify CSV: {e}\")\n",
    "    \n",
    "    # Print results\n",
    "    if errors:\n",
    "        print(\"\\n‚ùå VERIFICATION FAILED:\")\n",
    "        print(\"\\n\".join(errors))\n",
    "        if warnings:\n",
    "            print(\"\\n‚ö†Ô∏è  WARNINGS:\")\n",
    "            print(\"\\n\".join(warnings))\n",
    "        print(\"=\"*70)\n",
    "        return False\n",
    "    else:\n",
    "        print(\"\\n‚úÖ Objective 2 Complete - All checks passed!\")\n",
    "        if warnings:\n",
    "            print(\"\\n‚ö†Ô∏è  WARNINGS:\")\n",
    "            print(\"\\n\".join(warnings))\n",
    "        print(f\"   ‚Ä¢ Total Q&A pairs: {actual_count}\")\n",
    "        print(f\"   ‚Ä¢ Answerable: {answerable_count}\")\n",
    "        print(f\"   ‚Ä¢ Unanswerable: {unanswerable_count}\")\n",
    "        print(f\"   ‚Ä¢ DataFrame: {len(qa_df)} rows √ó {len(qa_df.columns)} columns\")\n",
    "        print(f\"   ‚Ä¢ CSV file: {csv_path}\")\n",
    "        print(\"=\"*70)\n",
    "        return True\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# DISPLAY FUNCTION\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def display_qa_database(qa_list: List[Dict], max_display: int = None):\n",
    "    \"\"\"Display Q&A pairs in a readable format.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìã GENERATED Q&A DATABASE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if max_display:\n",
    "        display_list = qa_list[:max_display]\n",
    "        print(f\"\\nShowing first {len(display_list)} of {len(qa_list)} pairs:\\n\")\n",
    "    else:\n",
    "        display_list = qa_list\n",
    "        print(f\"\\nAll {len(display_list)} pairs:\\n\")\n",
    "    \n",
    "    for i, pair in enumerate(display_list, 1):\n",
    "        answerable_str = \"‚úÖ Answerable\" if pair.get(\"answerable\") else \"‚ùå Unanswerable\"\n",
    "        category = pair.get(\"category\", \"unknown\")\n",
    "        print(f\"\\n[{i}] {answerable_str} | Category: {category}\")\n",
    "        print(f\"    Q: {pair.get('question', 'N/A')}\")\n",
    "        print(f\"    A: {pair.get('answer', 'N/A')[:150]}{'...' if len(pair.get('answer', '')) > 150 else ''}\")\n",
    "    \n",
    "    if max_display and len(qa_list) > max_display:\n",
    "        print(f\"\\n... and {len(qa_list) - max_display} more pairs\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# EXECUTION ENTRYPOINT\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"OBJECTIVE 2 (REVISED): GENERATE CUSTOM Q&A DATABASE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "validate_prerequisites()\n",
    "\n",
    "print(\"\\nü§ñ Generating Q&A with strict Mistral prompts...\")\n",
    "qa_database = generate_full_qa_database()\n",
    "qa_df = qa_to_dataframe(qa_database)\n",
    "\n",
    "save_qa_to_csv(qa_database)\n",
    "\n",
    "# Display the generated Q&A pairs\n",
    "display_qa_database(qa_database)\n",
    "\n",
    "# Verify the results\n",
    "print(\"\\n\")\n",
    "verify_objective2()\n",
    "\n",
    "print(\"\\nDone! Q&A database is ready.\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective 3: Implement Vector Databases Using FAISS\n",
    "\n",
    "### üéØ Goal\n",
    "Build a FAISS vector database from the Q&A pairs to enable fast semantic search, converting text to embeddings and creating an index for efficient similarity retrieval in the RAG system.\n",
    "\n",
    "<details>\n",
    "<summary><b>üì• Prerequisites</b> (Click to expand)</summary>\n",
    "\n",
    "| Item | Source | Required | Description |\n",
    "|------|--------|----------|-------------|\n",
    "| `qa_database` | Objective 2 | ‚úÖ Yes | List of 21 Q&A pairs to convert to embeddings |\n",
    "| `system_prompt` | Objective 1 | ‚úÖ Yes | System prompt (used for validation) |\n",
    "| Python packages | Setup cell | ‚úÖ Yes | `faiss-cpu`, `sentence-transformers`, `numpy`, `pandas` |\n",
    "\n",
    "**Note:** Objective 2 must be completed first to provide the Q&A database.\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>üîß Core Concepts</b> (Click to expand)</summary>\n",
    "\n",
    "| Concept | Description |\n",
    "|--------|-------------|\n",
    "| **Embeddings** | Numerical vector representations of text that capture semantic meaning. Similar texts have similar embeddings (close in vector space) |\n",
    "| **Semantic Search** | Finding relevant documents by meaning rather than exact keyword matching. Enables finding \"shipping\" when querying \"delivery\" |\n",
    "| **FAISS (Facebook AI Similarity Search)** | Library for efficient similarity search in high-dimensional vector spaces. Searches millions of vectors in milliseconds |\n",
    "| **IndexFlatL2** | FAISS index type using L2 (Euclidean) distance for exact similarity search. Ideal for small-medium datasets (<100k vectors) |\n",
    "| **Sentence Transformers** | Pre-trained models that convert text to dense vector embeddings optimized for semantic similarity |\n",
    "| **Top-K Retrieval** | Retrieving the k most similar documents (e.g., top-3) based on embedding similarity scores |\n",
    "\n",
    "**Why This Matters:**\n",
    "Vector databases enable semantic search - finding relevant information even when exact keywords don't match. This is essential for RAG systems where user questions need to retrieve the most semantically similar context from the knowledge base.\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>üìä Design Choices</b> (Click to expand)</summary>\n",
    "\n",
    "| Choice | Selected | Rationale |\n",
    "|--------|----------|-----------|\n",
    "| **Embedding Model** | `all-MiniLM-L6-v2` | 384 dimensions, fast inference, good quality for small-medium datasets |\n",
    "| **FAISS Index Type** | IndexFlatL2 | Exact search with L2 distance, ideal for 21 Q&A pairs, no approximation needed |\n",
    "| **Embedding Strategy** | Question + Answer combined | Richer semantic representation by including both question and answer text |\n",
    "| **Top-K Retrieval** | 3 documents | Balances context richness with prompt length, sufficient for most queries |\n",
    "| **Distance Metric** | L2 (Euclidean) | Standard similarity measure, lower distance = more similar vectors |\n",
    "| **Data Type** | float32 | FAISS requirement, balances precision and memory usage |\n",
    "\n",
    "**Why This Approach:**\n",
    "- **all-MiniLM-L6-v2**: Lightweight, fast, sufficient quality for 21 pairs. Alternative models (e.g., all-mpnet-base-v2) offer higher accuracy but slower inference\n",
    "- **IndexFlatL2**: Exact search ensures highest quality results. For larger datasets (>100k), IndexIVFFlat or IndexHNSW would be better for speed\n",
    "- **Combined Q&A**: Including both question and answer in embeddings captures full semantic context, improving retrieval accuracy\n",
    "- **Top-3**: Provides enough context for LLM while keeping prompts manageable\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>üì§ Outputs</b> (Click to expand)</summary>\n",
    "\n",
    "| Variable | Type | Description |\n",
    "|----------|------|-------------|\n",
    "| `embedding_model` | `SentenceTransformer` | Loaded sentence-transformers model (all-MiniLM-L6-v2) |\n",
    "| `qa_embeddings` | `np.ndarray` | Embedding vectors for all Q&A pairs, shape (21, 384) |\n",
    "| `faiss_index` | `faiss.IndexFlatL2` | FAISS index with 21 vectors indexed, ready for similarity search |\n",
    "| `embed_query()` | `function` | Function to convert query text to embedding vector |\n",
    "\n",
    "**Files Created:**\n",
    "| File | Location | Description |\n",
    "|------|----------|-------------|\n",
    "| `qa_embeddings.npy` | `data/vector_database/` | NumPy array of all Q&A embeddings (21 √ó 384) |\n",
    "| `qa_index.faiss` | `data/vector_database/` | Serialized FAISS index for persistence |\n",
    "| `retrieval_test_results.csv` | `data/vector_database/` | Test query results with similarity scores |\n",
    "\n",
    "**Key Functions Created:**\n",
    "- `load_embedding_model()`: Loads sentence-transformers model\n",
    "- `generate_embeddings()`: Converts text list to embedding array\n",
    "- `create_faiss_index()`: Builds FAISS IndexFlatL2 from embeddings\n",
    "- `search_index()`: Finds top-k similar vectors by L2 distance\n",
    "- `retrieve_context()`: Complete RAG retrieval - query to relevant Q&A pairs\n",
    "- `embed_query()`: Converts query text to embedding vector\n",
    "- `format_context_for_llm()`: Formats retrieved Q&A pairs as context string\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>üìã FAISS Index Details</b> (Click to expand)</summary>\n",
    "\n",
    "**Index Type: IndexFlatL2**\n",
    "\n",
    "| Property | Value | Description |\n",
    "|----------|-------|-------------|\n",
    "| **Distance Metric** | L2 (Euclidean) | Lower distance = more similar vectors |\n",
    "| **Search Type** | Exact | No approximation, highest quality results |\n",
    "| **Vectors Indexed** | 21 | One per Q&A pair |\n",
    "| **Embedding Dimension** | 384 | Matches all-MiniLM-L6-v2 output |\n",
    "| **Search Speed** | Milliseconds | Fast for small-medium datasets |\n",
    "| **Scalability** | <100k vectors | For larger datasets, use IndexIVFFlat or IndexHNSW |\n",
    "\n",
    "**Why IndexFlatL2:**\n",
    "- **Exact Search**: No approximation means highest quality results\n",
    "- **Simple**: Easy to implement and understand\n",
    "- **Sufficient**: Perfect for 21 Q&A pairs\n",
    "- **Fast Enough**: Millisecond search times for our dataset size\n",
    "\n",
    "**Alternative Index Types (for reference):**\n",
    "- **IndexIVFFlat**: Approximate search, faster for large datasets (>100k)\n",
    "- **IndexHNSW**: Graph-based, very fast approximate search for very large datasets\n",
    "- **IndexFlatIP**: Inner product (cosine similarity) instead of L2 distance\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>üìö Learning Objectives Demonstrated</b> (Click to expand)</summary>\n",
    "\n",
    "1. **Text Embeddings**: Converting text to numerical vectors that capture semantic meaning\n",
    "2. **Semantic Search**: Finding relevant documents by meaning rather than keyword matching\n",
    "3. **Vector Databases**: Using FAISS for efficient similarity search in high-dimensional spaces\n",
    "4. **Index Design**: Choosing appropriate index types (IndexFlatL2) for dataset size\n",
    "5. **RAG Retrieval**: Implementing the retrieval component of RAG systems\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>üí° Tips</b> (Click to expand)</summary>\n",
    "\n",
    "- **Embedding Model**: all-MiniLM-L6-v2 is fast and sufficient for 21 pairs. For larger datasets, consider all-mpnet-base-v2 for better quality\n",
    "- **Combined Q&A**: Including both question and answer in embeddings improves retrieval accuracy\n",
    "- **FAISS Index Type**: IndexFlatL2 is perfect for small datasets. For >100k vectors, use IndexIVFFlat for speed\n",
    "- **Top-K Selection**: Top-3 provides good context balance. Adjust based on your use case\n",
    "- **Float32 Requirement**: FAISS requires float32 dtype - embeddings are automatically converted\n",
    "- **Index Persistence**: Saved index can be loaded later without regenerating embeddings\n",
    "- **Search Speed**: FAISS searches are extremely fast (milliseconds) even for larger datasets\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Next Step:** Proceed to Objective 4 to build the complete RAG pipeline using the FAISS index.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "   OBJECTIVE 3: IMPLEMENT VECTOR DATABASE USING FAISS\n",
      "   Building Semantic Search for RAG System\n",
      "======================================================================\n",
      "\n",
      "üîç STEP 1: Validate Prerequisites\n",
      "----------------------------------------------------------------------\n",
      "‚úÖ Prerequisites validated\n",
      "   ‚Ä¢ System prompt: 1995 chars\n",
      "   ‚Ä¢ Q&A database: 21 pairs\n",
      "\n",
      "ü§ñ STEP 2: Load Embedding Model\n",
      "----------------------------------------------------------------------\n",
      "   Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "   ‚úÖ Model loaded (embedding dim: 384)\n",
      "\n",
      "üìä STEP 3: Generate Embeddings for Q&A Database\n",
      "----------------------------------------------------------------------\n",
      "   Generating embeddings for 21 Q&A pairs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "231d3162aa4644d581e282ff2b8d8e1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Embeddings shape: (21, 384)\n",
      "\n",
      "üóÑÔ∏è  STEP 4: Create FAISS Index\n",
      "----------------------------------------------------------------------\n",
      "   ‚úÖ FAISS index created\n",
      "   ‚Ä¢ Index type: IndexFlatL2 (exact search)\n",
      "   ‚Ä¢ Vectors indexed: 21\n",
      "   ‚Ä¢ Embedding dimension: 384\n",
      "\n",
      "üß™ STEP 5: Test Retrieval with Sample Queries\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "   Testing: \"How long does shipping take?\"\n",
      "\n",
      "======================================================================\n",
      "üîç RETRIEVAL RESULTS FOR: \"How long does shipping take?\"\n",
      "======================================================================\n",
      "\n",
      "üìó Result 1 (Similarity: 63.8%)\n",
      "   Category: shipping\n",
      "   Question: How long will it take for my order to arrive?\n",
      "   Answer: Delivery times for our standard shipping option are 5-7 business days, and express shipping takes 2-...\n",
      "\n",
      "üìó Result 2 (Similarity: 59.2%)\n",
      "   Category: customer_service\n",
      "   Question: What are the shipping options available for my order?\n",
      "   Answer: We offer standard shipping (5-7 business days) for free on orders over $75. Express shipping (2-3 bu...\n",
      "\n",
      "üìó Result 3 (Similarity: 56.2%)\n",
      "   Category: returns\n",
      "   Question: I'd like to know about your shipping options. How does it work?\n",
      "   Answer: We offer standard shipping (5-7 business days) for free on orders over $75. Express shipping (2-3 bu...\n",
      "\n",
      "======================================================================\n",
      "\n",
      "   Testing: \"Can I return a product?\"\n",
      "\n",
      "======================================================================\n",
      "üîç RETRIEVAL RESULTS FOR: \"Can I return a product?\"\n",
      "======================================================================\n",
      "\n",
      "üìó Result 1 (Similarity: 67.7%)\n",
      "   Category: customer_service\n",
      "   Question: I'd like to return a product, but I'm not sure if it's within the return policy. Can you help me?\n",
      "   Answer: Certainly! You can return any unopened item in its original packaging within 30 days of delivery for...\n",
      "\n",
      "üìó Result 2 (Similarity: 66.5%)\n",
      "   Category: returns\n",
      "   Question: I'm not completely satisfied with my purchase. Can I return it?\n",
      "   Answer: Yes, you can return any unopened item in its original packaging within 30 days of delivery for a ful...\n",
      "\n",
      "üìó Result 3 (Similarity: 47.4%)\n",
      "   Category: warranty\n",
      "   Question: What happens if my product breaks down within the first year?\n",
      "   Answer: If your product develops a manufacturing defect within the first year, you can submit a claim under ...\n",
      "\n",
      "======================================================================\n",
      "\n",
      "   Testing: \"What are your business hours?\"\n",
      "\n",
      "======================================================================\n",
      "üîç RETRIEVAL RESULTS FOR: \"What are your business hours?\"\n",
      "======================================================================\n",
      "\n",
      "üìó Result 1 (Similarity: 49.7%)\n",
      "   Category: returns\n",
      "   Question: What are your customer service hours so I can reach out if I have any questions?\n",
      "   Answer: Our customer service team is available Monday through Friday from 9 AM to 6 PM EST, and on Saturdays...\n",
      "\n",
      "üìó Result 2 (Similarity: 47.1%)\n",
      "   Category: customer_service\n",
      "   Question: What are your customer service hours and contact details?\n",
      "   Answer: Our customer service team is available Monday through Friday from 9 AM to 6 PM EST, and on Saturdays...\n",
      "\n",
      "üìó Result 3 (Similarity: 41.1%)\n",
      "   Category: returns\n",
      "   Question: I'd like to know about your shipping options. How does it work?\n",
      "   Answer: We offer standard shipping (5-7 business days) for free on orders over $75. Express shipping (2-3 bu...\n",
      "\n",
      "======================================================================\n",
      "\n",
      "   Testing: \"Do you price match with competitors?\"\n",
      "\n",
      "======================================================================\n",
      "üîç RETRIEVAL RESULTS FOR: \"Do you price match with competitors?\"\n",
      "======================================================================\n",
      "\n",
      "üìï Result 1 (Similarity: 43.6%)\n",
      "   Category: personal_advice\n",
      "   Question: What is the email address of the CEO of a competitor company?\n",
      "   Answer: I'm sorry, but I'm unable to provide contact information for executives of competitor companies. I c...\n",
      "\n",
      "üìó Result 2 (Similarity: 40.5%)\n",
      "   Category: shipping\n",
      "   Question: Do you offer free shipping?\n",
      "   Answer: Yes, we offer free shipping on orders over $75. Express shipping is available for an additional fee....\n",
      "\n",
      "üìó Result 3 (Similarity: 39.9%)\n",
      "   Category: returns\n",
      "   Question: I'd like to know about your shipping options. How does it work?\n",
      "   Answer: We offer standard shipping (5-7 business days) for free on orders over $75. Express shipping (2-3 bu...\n",
      "\n",
      "======================================================================\n",
      "\n",
      "üìù STEP 6: Example - Formatted Context for LLM\n",
      "----------------------------------------------------------------------\n",
      "Query: \"What is your return policy?\"\n",
      "\n",
      "Formatted Context for LLM:\n",
      "----------------------------------------------------------------------\n",
      "[Context 1]\n",
      "Q: I'd like to return a product, but I'm not sure if it's within the return policy. Can you help me?\n",
      "A: Certainly! You can return any unopened item in its original packaging within 30 days of delivery for a full refund. Please contact our support team to initiate the return process.\n",
      "\n",
      "[Context 2]\n",
      "Q: I'm not completely satisfied with my purchase. Can I return it?\n",
      "A: Yes, you can return any unopened item in its original packaging within 30 days of delivery for a full refund. Simply contact our support team to initiate the return process.\n",
      "\n",
      "[Context 3]\n",
      "Q: What should I do if I want to make a claim after the first year but before the third year?\n",
      "A: For products with a 2-year or 3-year warranty, the claims process is similar. Please contact our support team, and we will guide you through the process.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üíæ STEP 7: Save Files to data/vector_database/\n",
      "----------------------------------------------------------------------\n",
      "‚úÖ Embeddings saved to: data/vector_database/qa_embeddings.npy\n",
      "‚úÖ FAISS index saved to: data/vector_database/qa_index.faiss\n",
      "‚úÖ Retrieval test results saved to: data/vector_database/retrieval_test_results.csv\n",
      "\n",
      "‚úÖ STEP 8: Verify Objective 3\n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n",
      "üîç OBJECTIVE 3 VERIFICATION\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Objective 3 Complete - All variables and files verified\n",
      "   ‚Ä¢ Embedding Model: 384 dimensions\n",
      "   ‚Ä¢ Embeddings: 21 vectors √ó 384 dimensions\n",
      "   ‚Ä¢ FAISS Index: 21 vectors indexed\n",
      "   ‚Ä¢ embed_query(): Ready\n",
      "   ‚Ä¢ Files: Saved to data/vector_database/\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "‚úÖ OBJECTIVE 3 COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Key Concepts Demonstrated:\n",
      "  1. Embeddings - Text to vector conversion using sentence-transformers\n",
      "  2. FAISS Index - Efficient similarity search with IndexFlatL2\n",
      "  3. RAG Retrieval - Finding relevant context for user queries\n",
      "\n",
      "üì¶ FILES SAVED (for submission):\n",
      "  ‚Ä¢ data/vector_database/qa_embeddings.npy - Embedding vectors\n",
      "  ‚Ä¢ data/vector_database/qa_index.faiss - FAISS index\n",
      "  ‚Ä¢ data/vector_database/retrieval_test_results.csv - Test results\n",
      "\n",
      "üì¶ GLOBAL VARIABLES:\n",
      "  ‚Ä¢ embedding_model: SentenceTransformer model\n",
      "  ‚Ä¢ qa_embeddings: numpy array ((21, 384))\n",
      "  ‚Ä¢ faiss_index: FAISS IndexFlatL2 (21 vectors)\n",
      "\n",
      "üîú READY FOR OBJECTIVE 4: RAG Pipeline Integration\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# OBJECTIVE 3: IMPLEMENT VECTOR DATABASES USING FAISS\n",
    "# ============================================================================\n",
    "#\n",
    "# LEARNING OBJECTIVES DEMONSTRATED:\n",
    "#   1. Text Embeddings - Converting text to numerical vectors that capture semantic meaning\n",
    "#   2. Semantic Search - Finding relevant documents by meaning rather than keyword matching\n",
    "#   3. Vector Databases - Using FAISS for efficient similarity search in high-dimensional spaces\n",
    "#   4. Index Design - Choosing appropriate index types (IndexFlatL2) for dataset size\n",
    "#   5. RAG Retrieval - Implementing the retrieval component of RAG systems\n",
    "#\n",
    "# PREREQUISITES: Run Objective 1 and Objective 2 first\n",
    "#   - system_prompt (from Objective 1) - for validation\n",
    "#   - qa_database (from Objective 2) - 21 Q&A pairs to convert to embeddings\n",
    "#\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 1: IMPORTS & VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import faiss\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "except ImportError as e:\n",
    "    raise ImportError(f\"Missing: {e}. Run: pip install faiss-cpu sentence-transformers numpy pandas\")\n",
    "\n",
    "\n",
    "def validate_prerequisites():\n",
    "    \"\"\"Ensure Objective 1 and 2 were run first.\"\"\"\n",
    "    required = ['system_prompt', 'qa_database']\n",
    "    missing = [r for r in required if r not in globals()]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"Missing: {missing}. Run Objective 1 and 2 first.\")\n",
    "    print(\"‚úÖ Prerequisites validated\")\n",
    "    print(f\"   ‚Ä¢ System prompt: {len(globals()['system_prompt'])} chars\")\n",
    "    print(f\"   ‚Ä¢ Q&A database: {len(globals()['qa_database'])} pairs\")\n",
    "    return True\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 2: CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Output directory for FAISS index and embeddings\n",
    "OUTPUT_DIR = \"data/vector_database\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Embedding model - lightweight and efficient\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# Retrieval settings\n",
    "TOP_K = 3  # Number of similar documents to retrieve\n",
    "\n",
    "# Expected Q&A count (from Objective 2)\n",
    "EXPECTED_QA_COUNT = 21  # Total Q&A pairs: 18 answerable + 3 unanswerable\n",
    "EMBEDDING_DIM = 384  # Dimension for all-MiniLM-L6-v2 model\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 3: EMBEDDING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def load_embedding_model(model_name: str = EMBEDDING_MODEL) -> SentenceTransformer:\n",
    "    \"\"\"\n",
    "    Load sentence transformer model for generating embeddings.\n",
    "    \n",
    "    Model: all-MiniLM-L6-v2\n",
    "    - Dimensions: 384\n",
    "    - Speed: Fast (good for real-time applications)\n",
    "    - Quality: Good semantic understanding\n",
    "    - Memory: Low (suitable for CPU-based environments)\n",
    "    \n",
    "    Returns:\n",
    "        SentenceTransformer model ready for encoding\n",
    "    \"\"\"\n",
    "    print(f\"   Loading embedding model: {model_name}\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "    print(f\"   ‚úÖ Model loaded (embedding dim: {model.get_sentence_embedding_dimension()})\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def generate_embeddings(texts: List[str], model: SentenceTransformer) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    if not texts:\n",
    "        raise ValueError(\"Texts list cannot be empty\")\n",
    "    \n",
    "    Generate embeddings for a list of texts.\n",
    "    \n",
    "    Converts text to 384-dimensional float32 vectors using sentence-transformers.\n",
    "    These embeddings capture semantic meaning, enabling similarity search.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of strings to embed\n",
    "        model: SentenceTransformer model\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of shape (n_texts, 384) with float32 dtype\n",
    "    \"\"\"\n",
    "    # Encode all texts - model handles batching internally\n",
    "    embeddings = model.encode(texts, show_progress_bar=True, convert_to_numpy=True)\n",
    "    \n",
    "    # FAISS requires float32 dtype\n",
    "    return embeddings.astype('float32')\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 4: FAISS INDEX FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def create_faiss_index(embeddings: np.ndarray) -> faiss.IndexFlatL2:\n",
    "    \"\"\"\n",
    "    Create FAISS index from embeddings.\n",
    "    \n",
    "    Uses IndexFlatL2 for exact search with L2 (Euclidean) distance.\n",
    "    Ideal for small-medium datasets (<100k vectors).\n",
    "    \n",
    "    Args:\n",
    "        embeddings: numpy array of shape (n_vectors, embedding_dim)\n",
    "    \n",
    "    Returns:\n",
    "        FAISS IndexFlatL2 index with all vectors indexed\n",
    "    \"\"\"\n",
    "    # Step 1: Get embedding dimension\n",
    "    if embeddings.size == 0:\n",
    "        raise ValueError(\"Embeddings array cannot be empty\")\n",
    "    \n",
    "    if len(embeddings.shape) != 2:\n",
    "        raise ValueError(f\"Embeddings must be 2D array, got shape {embeddings.shape}\")\n",
    "    \n",
    "    if embeddings.dtype != np.float32:\n",
    "        raise TypeError(f\"Embeddings must be float32, got {embeddings.dtype}\")\n",
    "    \n",
    "    dimension = embeddings.shape[1]\n",
    "    \n",
    "    # Step 2: Create FAISS index\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    \n",
    "    # Step 3: Add vectors to index\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    # Step 4: Return the populated index\n",
    "    return index\n",
    "\n",
    "\n",
    "def search_index(\n",
    "    query_embedding: np.ndarray,\n",
    "    index: faiss.IndexFlatL2,\n",
    "    top_k: int = TOP_K\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Search FAISS index for most similar vectors.\n",
    "    \n",
    "    Args:\n",
    "        query_embedding: Query vector (1, embedding_dim)\n",
    "        index: FAISS index\n",
    "        top_k: Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        distances: Distance scores (lower = more similar for L2)\n",
    "        indices: Indices of matched documents\n",
    "    \"\"\"\n",
    "    if len(query_embedding.shape) == 1:\n",
    "        query_embedding = query_embedding.reshape(1, -1)\n",
    "    \n",
    "    # Validate dimension match\n",
    "    if query_embedding.shape[1] != index.d:\n",
    "        raise ValueError(f\"Query embedding dimension {query_embedding.shape[1]} doesn't match index dimension {index.d}\")\n",
    "    \n",
    "    \n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    return distances[0], indices[0]\n",
    "\n",
    "\n",
    "def retrieve_context(\n",
    "    query: str,\n",
    "    model: SentenceTransformer,\n",
    "    index: faiss.IndexFlatL2,\n",
    "    qa_database: List[Dict],\n",
    "    top_k: int = TOP_K\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve most relevant Q&A pairs for a query.\n",
    "    \n",
    "    This is the core RAG retrieval function:\n",
    "    1. Convert query to embedding\n",
    "    2. Search FAISS index for similar embeddings\n",
    "    3. Return corresponding Q&A pairs as context\n",
    "    \n",
    "    Args:\n",
    "        query: User's question\n",
    "        model: Embedding model\n",
    "        index: FAISS index\n",
    "        qa_database: Original Q&A database\n",
    "        top_k: Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of relevant Q&A pairs with distance scores\n",
    "    \"\"\"\n",
    "    if not query or not query.strip():\n",
    "        raise ValueError(\"Query cannot be empty\")\n",
    "    \n",
    "    if not qa_database:\n",
    "        raise ValueError(\"Q&A database cannot be empty\")\n",
    "    \n",
    "    # Generate query embedding\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True).astype('float32')\n",
    "    \n",
    "    # Search index\n",
    "    distances, indices = search_index(query_embedding, index, top_k)\n",
    "    \n",
    "    # Get corresponding Q&A pairs\n",
    "    results = []\n",
    "    for dist, idx in zip(distances, indices):\n",
    "        if idx < len(qa_database):  # Safety check\n",
    "            qa = qa_database[idx].copy()\n",
    "            qa['distance'] = float(dist)\n",
    "            qa['similarity_score'] = 1 / (1 + float(dist))  # Convert distance to similarity\n",
    "            results.append(qa)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def format_context_for_llm(retrieved_qa: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Format retrieved Q&A pairs as context for LLM.\n",
    "    \n",
    "    This context will be injected into the prompt for RAG.\n",
    "    \"\"\"\n",
    "    if not retrieved_qa:\n",
    "        return \"No relevant information found in knowledge base.\"\n",
    "    \n",
    "    context_parts = []\n",
    "    for i, qa in enumerate(retrieved_qa, 1):\n",
    "        context_parts.append(f\"[Context {i}]\")\n",
    "        context_parts.append(f\"Q: {qa['question']}\")\n",
    "        context_parts.append(f\"A: {qa['answer']}\")\n",
    "        context_parts.append(\"\")\n",
    "    \n",
    "    return \"\\n\".join(context_parts)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 5: DISPLAY & STORAGE FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def display_retrieval_results(query: str, results: List[Dict]):\n",
    "    \"\"\"Display retrieval results in a formatted way.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"üîç RETRIEVAL RESULTS FOR: \\\"{query}\\\"\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for i, qa in enumerate(results, 1):\n",
    "        similarity = qa.get('similarity_score', 0) * 100\n",
    "        answerable = 'üìó' if qa.get('answerable', True) else 'üìï'\n",
    "        \n",
    "        print(f\"\\n{answerable} Result {i} (Similarity: {similarity:.1f}%)\")\n",
    "        print(f\"   Category: {qa.get('category', 'N/A')}\")\n",
    "        print(f\"   Question: {qa.get('question', 'N/A')}\")\n",
    "        print(f\"   Answer: {qa.get('answer', 'N/A')[:100]}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "\n",
    "def save_embeddings(embeddings: np.ndarray, filename: str = \"qa_embeddings.npy\"):\n",
    "    \"\"\"Save embeddings to numpy file.\"\"\"\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    np.save(filepath, embeddings)\n",
    "    print(f\"‚úÖ Embeddings saved to: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def save_faiss_index(index: faiss.IndexFlatL2, filename: str = \"qa_index.faiss\"):\n",
    "    \"\"\"Save FAISS index to file.\"\"\"\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    faiss.write_index(index, filepath)\n",
    "    print(f\"‚úÖ FAISS index saved to: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def save_retrieval_test_results(test_results: List[Dict], filename: str = \"retrieval_test_results.csv\"):\n",
    "    \"\"\"Save retrieval test results to CSV.\"\"\"\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    \n",
    "    # Flatten results for CSV\n",
    "    rows = []\n",
    "    for result in test_results:\n",
    "        for i, retrieved in enumerate(result['retrieved'], 1):\n",
    "            rows.append({\n",
    "                'query': result['query'],\n",
    "                'rank': i,\n",
    "                'category': retrieved.get('category', ''),\n",
    "                'answerable': retrieved.get('answerable', True),\n",
    "                'question': retrieved.get('question', ''),\n",
    "                'answer': retrieved.get('answer', '')[:200],\n",
    "                'similarity_score': retrieved.get('similarity_score', 0)\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(filepath, index=False)\n",
    "    print(f\"‚úÖ Retrieval test results saved to: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 6: VERIFICATION FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def verify_objective3():\n",
    "    \"\"\"\n",
    "    Verify that Objective 3 completed successfully.\n",
    "    Checks all variables, embeddings, FAISS index, and files.\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"üîç OBJECTIVE 3 VERIFICATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    errors = []\n",
    "    \n",
    "    # Check if variables exist\n",
    "    if 'embedding_model' not in globals():\n",
    "        errors.append(\"‚ùå embedding_model not found\")\n",
    "    if 'qa_embeddings' not in globals():\n",
    "        errors.append(\"‚ùå qa_embeddings not found\")\n",
    "    if 'faiss_index' not in globals():\n",
    "        errors.append(\"‚ùå faiss_index not found\")\n",
    "    if 'embed_query' not in globals():\n",
    "        errors.append(\"‚ùå embed_query function not found\")\n",
    "    \n",
    "    if errors:\n",
    "        print(\"\\n\".join(errors))\n",
    "        print(\"=\"*70)\n",
    "        return False\n",
    "    \n",
    "    # Check embeddings shape\n",
    "    if qa_embeddings.shape[0] != EXPECTED_QA_COUNT:\n",
    "        errors.append(f\"‚ùå Expected EXPECTED_QA_COUNT embeddings, got {qa_embeddings.shape[0]}\")\n",
    "    if qa_embeddings.shape[1] != EMBEDDING_DIM:\n",
    "        errors.append(f\"‚ùå Expected EMBEDDING_DIM dimensions, got {qa_embeddings.shape[1]}\")\n",
    "    if qa_embeddings.dtype != 'float32':\n",
    "        errors.append(f\"‚ùå Expected float32 dtype, got {qa_embeddings.dtype}\")\n",
    "    \n",
    "    # Check FAISS index\n",
    "    if faiss_index.ntotal != EXPECTED_QA_COUNT:\n",
    "        errors.append(f\"‚ùå Expected EXPECTED_QA_COUNT vectors in index, got {faiss_index.ntotal}\")\n",
    "    if faiss_index.d != EMBEDDING_DIM:\n",
    "        errors.append(f\"‚ùå Expected EMBEDDING_DIM dimensions in index, got {faiss_index.d}\")\n",
    "    \n",
    "    # Check files exist\n",
    "    if not os.path.exists(\"data/vector_database/qa_embeddings.npy\"):\n",
    "        errors.append(\"‚ùå qa_embeddings.npy not found\")\n",
    "    if not os.path.exists(\"data/vector_database/qa_index.faiss\"):\n",
    "        errors.append(\"‚ùå qa_index.faiss not found\")\n",
    "    \n",
    "    # Test embed_query function\n",
    "    try:\n",
    "        test_embedding = embed_query(\"test query\")\n",
    "        if test_embedding.shape != (EMBEDDING_DIM,):\n",
    "            errors.append(f\"‚ùå embed_query() returned wrong shape: {test_embedding.shape}\")\n",
    "    except Exception as e:\n",
    "        errors.append(f\"‚ùå embed_query() test failed: {e}\")\n",
    "    \n",
    "    # Print results\n",
    "    if errors:\n",
    "        print(\"\\n‚ùå VERIFICATION FAILED:\")\n",
    "        print(\"\\n\".join(errors))\n",
    "        print(\"=\"*70)\n",
    "        return False\n",
    "    else:\n",
    "        print(\"\\n‚úÖ Objective 3 Complete - All variables and files verified\")\n",
    "        print(f\"   ‚Ä¢ Embedding Model: {embedding_model.get_sentence_embedding_dimension()} dimensions\")\n",
    "        print(f\"   ‚Ä¢ Embeddings: {qa_embeddings.shape[0]} vectors √ó {qa_embeddings.shape[1]} dimensions\")\n",
    "        print(f\"   ‚Ä¢ FAISS Index: {faiss_index.ntotal} vectors indexed\")\n",
    "        print(f\"   ‚Ä¢ embed_query(): Ready\")\n",
    "        print(f\"   ‚Ä¢ Files: Saved to data/vector_database/\")\n",
    "        print(\"=\"*70)\n",
    "        return True\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 7: EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"   OBJECTIVE 3: IMPLEMENT VECTOR DATABASE USING FAISS\")\n",
    "print(\"   Building Semantic Search for RAG System\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# --- Step 1: Validate Prerequisites ---\n",
    "print(\"\\nüîç STEP 1: Validate Prerequisites\")\n",
    "print(\"-\"*70)\n",
    "validate_prerequisites()\n",
    "\n",
    "qa_database = globals()['qa_database']\n",
    "\n",
    "# --- Step 2: Load Embedding Model ---\n",
    "print(\"\\nü§ñ STEP 2: Load Embedding Model\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "embedding_model = load_embedding_model()\n",
    "globals()['embedding_model'] = embedding_model\n",
    "\n",
    "# --- Step 3: Generate Embeddings for Q&A Database ---\n",
    "print(\"\\nüìä STEP 3: Generate Embeddings for Q&A Database\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Combine question and answer for richer embeddings\n",
    "qa_texts = [f\"{qa['question']} {qa['answer']}\" for qa in qa_database]\n",
    "print(f\"   Generating embeddings for {len(qa_texts)} Q&A pairs...\")\n",
    "\n",
    "qa_embeddings = generate_embeddings(qa_texts, embedding_model)\n",
    "globals()['qa_embeddings'] = qa_embeddings\n",
    "\n",
    "print(f\"   ‚úÖ Embeddings shape: {qa_embeddings.shape}\")\n",
    "\n",
    "# --- Step 4: Create FAISS Index ---\n",
    "print(\"\\nüóÑÔ∏è  STEP 4: Create FAISS Index\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Create FAISS index and add embeddings\n",
    "faiss_index = create_faiss_index(qa_embeddings)\n",
    "globals()['faiss_index'] = faiss_index\n",
    "\n",
    "# Create embed_query function for RAG pipeline (used in Objective 4)\n",
    "def embed_query(query: str) -> np.ndarray:\n",
    "    \"\"\"Convert query text to embedding vector for FAISS search.\"\"\"\n",
    "    return embedding_model.encode([query], convert_to_numpy=True).astype('float32')[0]\n",
    "\n",
    "globals()['embed_query'] = embed_query\n",
    "\n",
    "print(f\"   ‚úÖ FAISS index created\")\n",
    "print(f\"   ‚Ä¢ Index type: IndexFlatL2 (exact search)\")\n",
    "print(f\"   ‚Ä¢ Vectors indexed: {faiss_index.ntotal}\")\n",
    "print(f\"   ‚Ä¢ Embedding dimension: {qa_embeddings.shape[1]}\")\n",
    "\n",
    "# --- Step 5: Test Retrieval ---\n",
    "print(\"\\nüß™ STEP 5: Test Retrieval with Sample Queries\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "test_queries = [\n",
    "    \"How long does shipping take?\",\n",
    "    \"Can I return a product?\",\n",
    "    \"What are your business hours?\",\n",
    "    \"Do you price match with competitors?\",  # Unanswerable\n",
    "]\n",
    "\n",
    "test_results = []\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n   Testing: \\\"{query}\\\"\")\n",
    "    \n",
    "    retrieved = retrieve_context(\n",
    "        query=query,\n",
    "        model=embedding_model,\n",
    "        index=faiss_index,\n",
    "        qa_database=qa_database,\n",
    "        top_k=TOP_K\n",
    "    )\n",
    "    \n",
    "    test_results.append({\n",
    "        'query': query,\n",
    "        'retrieved': retrieved\n",
    "    })\n",
    "    \n",
    "    display_retrieval_results(query, retrieved)\n",
    "\n",
    "# --- Step 6: Show Formatted Context for LLM ---\n",
    "print(\"\\nüìù STEP 6: Example - Formatted Context for LLM\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "sample_query = \"What is your return policy?\"\n",
    "sample_retrieved = retrieve_context(sample_query, embedding_model, faiss_index, qa_database, TOP_K)\n",
    "formatted_context = format_context_for_llm(sample_retrieved)\n",
    "\n",
    "print(f\"Query: \\\"{sample_query}\\\"\\n\")\n",
    "print(\"Formatted Context for LLM:\")\n",
    "print(\"-\"*70)\n",
    "print(formatted_context)\n",
    "print(\"-\"*70)\n",
    "\n",
    "# --- Step 7: Save All Artifacts ---\n",
    "print(\"\\nüíæ STEP 7: Save Files to data/vector_database/\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "save_embeddings(qa_embeddings)\n",
    "save_faiss_index(faiss_index)\n",
    "save_retrieval_test_results(test_results)\n",
    "\n",
    "# --- Step 8: Verify Objective 3 ---\n",
    "print(\"\\n‚úÖ STEP 8: Verify Objective 3\")\n",
    "print(\"-\"*70)\n",
    "verify_objective3()\n",
    "\n",
    "# --- Summary ---\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ OBJECTIVE 3 COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\"\"\n",
    "Key Concepts Demonstrated:\n",
    "  1. Embeddings - Text to vector conversion using sentence-transformers\n",
    "  2. FAISS Index - Efficient similarity search with IndexFlatL2\n",
    "  3. RAG Retrieval - Finding relevant context for user queries\n",
    "\n",
    "üì¶ FILES SAVED (for submission):\n",
    "  ‚Ä¢ {OUTPUT_DIR}/qa_embeddings.npy - Embedding vectors\n",
    "  ‚Ä¢ {OUTPUT_DIR}/qa_index.faiss - FAISS index\n",
    "  ‚Ä¢ {OUTPUT_DIR}/retrieval_test_results.csv - Test results\n",
    "\n",
    "üì¶ GLOBAL VARIABLES:\n",
    "  ‚Ä¢ embedding_model: SentenceTransformer model\n",
    "  ‚Ä¢ qa_embeddings: numpy array ({qa_embeddings.shape})\n",
    "  ‚Ä¢ faiss_index: FAISS IndexFlatL2 ({faiss_index.ntotal} vectors)\n",
    "\n",
    "üîú READY FOR OBJECTIVE 4: RAG Pipeline Integration\n",
    "\"\"\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective 4: RAG Pipeline Integration\n",
    "\n",
    "### üéØ Goal\n",
    "\n",
    "Build a complete Retrieval-Augmented Generation (RAG) pipeline that combines semantic search (FAISS) with the Mistral LLM to answer questions using the custom knowledge base. The pipeline will retrieve relevant context from the Q&A database and generate accurate, context-aware responses.\n",
    "\n",
    "<details>\n",
    "<summary><b>üì• Prerequisites</b> (Click to expand)</summary>\n",
    "\n",
    "| Item | Source | Required | Description |\n",
    "|------|--------|----------|-------------|\n",
    "| `mistral_model`, `mistral_tokenizer` | Objective 1 | ‚úÖ Yes | Mistral model for answer generation |\n",
    "| `system_prompt` | Objective 1 | ‚úÖ Yes | System prompt for context-aware responses |\n",
    "| `qa_database` | Objective 2 | ‚úÖ Yes | Q&A pairs as knowledge base |\n",
    "| `embedding_model` | Objective 3 | ‚úÖ Yes | SentenceTransformer for query embeddings |\n",
    "| `faiss_index` | Objective 3 | ‚úÖ Yes | FAISS index for semantic search |\n",
    "| `embed_query()` | Objective 3 | ‚úÖ Yes | Function to convert text to embeddings |\n",
    "| GPU (recommended) | System | ‚ö†Ô∏è Optional | Faster inference for Mistral model |\n",
    "\n",
    "**Note:** This objective requires all previous objectives (1-3) to be completed successfully.\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>üîß Core Concepts</b> (Click to expand)</summary>\n",
    "\n",
    "| Concept | Description |\n",
    "|--------|-------------|\n",
    "| **RAG Pipeline** | Complete flow: Query ‚Üí Embed ‚Üí Retrieve ‚Üí Augment ‚Üí Generate |\n",
    "| **Semantic Retrieval** | Using FAISS to find most relevant Q&A pairs based on embedding similarity |\n",
    "| **Context Augmentation** | Formatting retrieved documents as context for the LLM |\n",
    "| **Prompt Engineering** | Combining system prompt, retrieved context, and user question |\n",
    "| **Response Generation** | Using Mistral to generate answers based on retrieved context |\n",
    "\n",
    "**Why RAG:**\n",
    "RAG combines the accuracy of retrieval (finding exact relevant information) with the fluency of generation (natural language responses). This ensures answers are grounded in the knowledge base while maintaining conversational quality.\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>üì¶ Outputs</b> (Click to expand)</summary>\n",
    "\n",
    "**Functions Created:**\n",
    "- `rag_query(query)` - Main pipeline function (query ‚Üí answer)\n",
    "- `search_faiss(embedding, top_k=3)` - FAISS similarity search\n",
    "- `format_context(retrieved_qa)` - Format Q&A pairs as context\n",
    "- `build_prompt(question, context)` - Combine system prompt + context + question\n",
    "- `generate_response(prompt)` - Generate answer using Mistral\n",
    "- `RAGResult` - Dataclass for pipeline results\n",
    "\n",
    "**Files Saved:**\n",
    "- `data/rag_pipeline/rag_test_results.csv` - Test query results\n",
    "- `data/rag_pipeline/pipeline_config.txt` - Pipeline configuration\n",
    "\n",
    "**Global Variables:**\n",
    "- `rag_query()` - Available for Objective 5 evaluation\n",
    "- `search_faiss()` - Available for Objective 5 evaluation\n",
    "- `format_context()` - Available for Objective 5 evaluation\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>üìö Learning Objectives Demonstrated</b> (Click to expand)</summary>\n",
    "\n",
    "1. **RAG Pipeline Design**: Building end-to-end retrieval-augmented generation systems\n",
    "2. **Component Integration**: Combining multiple objectives into a unified pipeline\n",
    "3. **Semantic Search Application**: Using FAISS for real-world question answering\n",
    "4. **Context Management**: Formatting and managing retrieved context for LLM generation\n",
    "5. **Prompt Engineering**: Designing effective prompts that combine system instructions, context, and queries\n",
    "6. **Modular Architecture**: Creating reusable, testable pipeline components\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>üí° Tips</b> (Click to expand)</summary>\n",
    "\n",
    "**Best Practices:**\n",
    "- Test with both answerable and unanswerable questions\n",
    "- Monitor retrieved context quality - if irrelevant, adjust top_k\n",
    "- Keep context formatting consistent for reliable LLM processing\n",
    "- Use the verification function to ensure all components work\n",
    "\n",
    "**Common Issues:**\n",
    "- **Irrelevant context**: Try adjusting top_k or check embedding quality\n",
    "- **Hallucinated answers**: Ensure context is properly formatted and included in prompt\n",
    "- **Slow generation**: Consider using GPU or reducing max_new_tokens\n",
    "\n",
    "**Performance Tips:**\n",
    "- Cache embeddings for frequently asked questions\n",
    "- Batch process multiple queries if evaluating many questions\n",
    "- Monitor token usage to stay within model limits\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "   OBJECTIVE 4: BUILD COMPLETE RAG PIPELINE\n",
      "======================================================================\n",
      "\n",
      "üîç STEP 1: Validate Prerequisites\n",
      "----------------------------------------------------------------------\n",
      "‚úÖ All prerequisites validated\n",
      "   ‚Ä¢ System prompt: 1995 chars\n",
      "   ‚Ä¢ Q&A database: 21 pairs\n",
      "   ‚Ä¢ FAISS index: 21 vectors\n",
      "   ‚Ä¢ embed_query(): Ready (from Objective 3)\n",
      "\n",
      "üìê STEP 2: RAG Pipeline Architecture\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ                      RAG PIPELINE FLOW                          ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ                                                                 ‚îÇ\n",
      "‚îÇ  User Query                                                     ‚îÇ\n",
      "‚îÇ      ‚îÇ                                                          ‚îÇ\n",
      "‚îÇ      ‚ñº                                                          ‚îÇ\n",
      "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                            ‚îÇ\n",
      "‚îÇ  ‚îÇ 1. embed_query()‚îÇ  Convert query to embedding (from Obj 3)   ‚îÇ\n",
      "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                            ‚îÇ\n",
      "‚îÇ           ‚ñº                                                     ‚îÇ\n",
      "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                            ‚îÇ\n",
      "‚îÇ  ‚îÇ 2. search_faiss()‚îÇ  Find similar Q&A pairs                   ‚îÇ\n",
      "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                            ‚îÇ\n",
      "‚îÇ           ‚ñº                                                     ‚îÇ\n",
      "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                           ‚îÇ\n",
      "‚îÇ  ‚îÇ 3. format_context()‚îÇ  Format as context string               ‚îÇ\n",
      "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                           ‚îÇ\n",
      "‚îÇ           ‚ñº                                                     ‚îÇ\n",
      "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                            ‚îÇ\n",
      "‚îÇ  ‚îÇ 4. build_prompt()‚îÇ  Combine system + context + query         ‚îÇ\n",
      "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                            ‚îÇ\n",
      "‚îÇ           ‚ñº                                                     ‚îÇ\n",
      "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                        ‚îÇ\n",
      "‚îÇ  ‚îÇ 5. generate_response()‚îÇ  Generate with Mistral               ‚îÇ\n",
      "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                        ‚îÇ\n",
      "‚îÇ           ‚ñº                                                     ‚îÇ\n",
      "‚îÇ      Response                                                   ‚îÇ\n",
      "‚îÇ                                                                 ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "\n",
      "üß™ STEP 3: Test with ANSWERABLE Questions\n",
      "----------------------------------------------------------------------\n",
      "   These questions CAN be answered from our knowledge base\n",
      "\n",
      "======================================================================\n",
      "üìó ANSWERABLE: \"What is your return policy?\"\n",
      "======================================================================\n",
      "   Step 1: embed_query() - Converting query to embedding...\n",
      "   Step 2: search_faiss() - Searching for similar Q&A pairs...\n",
      "           ‚Üí Found 3 relevant documents\n",
      "   Step 3: format_context() - Formatting retrieved context...\n",
      "   Step 4: build_prompt() - Building augmented prompt...\n",
      "   Step 5: generate_response() - Generating response with Mistral...\n",
      "           ‚Üí Response generated: 321 chars\n",
      "\n",
      "======================================================================\n",
      "ü§ñ RAG PIPELINE RESULT\n",
      "======================================================================\n",
      "\n",
      "üì• USER QUERY:\n",
      "   What is your return policy?\n",
      "\n",
      "üìö RETRIEVED CONTEXT (3 sources):\n",
      "   üìó [1] customer_service (Similarity: 64%)\n",
      "       Q: I'd like to return a product, but I'm not sure if it's withi...\n",
      "   üìó [2] returns (Similarity: 50%)\n",
      "       Q: I'm not completely satisfied with my purchase. Can I return ...\n",
      "   üìó [3] warranty (Similarity: 41%)\n",
      "       Q: What should I do if I want to make a claim after the first y...\n",
      "\n",
      "üì§ GENERATED RESPONSE:\n",
      "----------------------------------------------------------------------\n",
      "Hello there! I'm glad you reached out to us. Our return policy allows you to return any unopened item in its original packaging within 30 days of delivery for a full refund. Please don't hesitate to contact our support team to initiate the return process. We're here to help! Thank you for choosing GreenTech Marketplace.\n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üìó ANSWERABLE: \"How long does shipping take?\"\n",
      "======================================================================\n",
      "   Step 1: embed_query() - Converting query to embedding...\n",
      "   Step 2: search_faiss() - Searching for similar Q&A pairs...\n",
      "           ‚Üí Found 3 relevant documents\n",
      "   Step 3: format_context() - Formatting retrieved context...\n",
      "   Step 4: build_prompt() - Building augmented prompt...\n",
      "   Step 5: generate_response() - Generating response with Mistral...\n",
      "           ‚Üí Response generated: 537 chars\n",
      "\n",
      "======================================================================\n",
      "ü§ñ RAG PIPELINE RESULT\n",
      "======================================================================\n",
      "\n",
      "üì• USER QUERY:\n",
      "   How long does shipping take?\n",
      "\n",
      "üìö RETRIEVED CONTEXT (3 sources):\n",
      "   üìó [1] shipping (Similarity: 64%)\n",
      "       Q: How long will it take for my order to arrive?...\n",
      "   üìó [2] customer_service (Similarity: 59%)\n",
      "       Q: What are the shipping options available for my order?...\n",
      "   üìó [3] returns (Similarity: 56%)\n",
      "       Q: I'd like to know about your shipping options. How does it wo...\n",
      "\n",
      "üì§ GENERATED RESPONSE:\n",
      "----------------------------------------------------------------------\n",
      "Hello there! I'm delighted to help with your inquiry. Our standard shipping option takes 5-7 business days, and express shipping is available in 2-3 business days. All orders come with tracking numbers, so you can follow your package's progress. If you'd like to expedite your delivery, the express shipping option is available for an additional fee. Thank you for choosing GreenTech Marketplace for your sustainable technology needs! If you have any other questions or need further assistance, don't hesitate to ask. We're here to help!\n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üìó ANSWERABLE: \"What are your customer service hours?\"\n",
      "======================================================================\n",
      "   Step 1: embed_query() - Converting query to embedding...\n",
      "   Step 2: search_faiss() - Searching for similar Q&A pairs...\n",
      "           ‚Üí Found 3 relevant documents\n",
      "   Step 3: format_context() - Formatting retrieved context...\n",
      "   Step 4: build_prompt() - Building augmented prompt...\n",
      "   Step 5: generate_response() - Generating response with Mistral...\n",
      "           ‚Üí Response generated: 283 chars\n",
      "\n",
      "======================================================================\n",
      "ü§ñ RAG PIPELINE RESULT\n",
      "======================================================================\n",
      "\n",
      "üì• USER QUERY:\n",
      "   What are your customer service hours?\n",
      "\n",
      "üìö RETRIEVED CONTEXT (3 sources):\n",
      "   üìó [1] returns (Similarity: 58%)\n",
      "       Q: What are your customer service hours so I can reach out if I...\n",
      "   üìó [2] customer_service (Similarity: 54%)\n",
      "       Q: What are your customer service hours and contact details?...\n",
      "   üìó [3] returns (Similarity: 44%)\n",
      "       Q: I'd like to know about your shipping options. How does it wo...\n",
      "\n",
      "üì§ GENERATED RESPONSE:\n",
      "----------------------------------------------------------------------\n",
      "Our customer service team is available Monday through Friday from 9 AM to 6 PM EST, and on Saturdays from 10 AM to 4 PM EST. You can reach us via email at support@greentechmarketplace.com or by phone. If you have any other questions, please don't hesitate to ask! We're here to help.\n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üìó ANSWERABLE: \"Do you offer warranty on products?\"\n",
      "======================================================================\n",
      "   Step 1: embed_query() - Converting query to embedding...\n",
      "   Step 2: search_faiss() - Searching for similar Q&A pairs...\n",
      "           ‚Üí Found 3 relevant documents\n",
      "   Step 3: format_context() - Formatting retrieved context...\n",
      "   Step 4: build_prompt() - Building augmented prompt...\n",
      "   Step 5: generate_response() - Generating response with Mistral...\n",
      "           ‚Üí Response generated: 564 chars\n",
      "\n",
      "======================================================================\n",
      "ü§ñ RAG PIPELINE RESULT\n",
      "======================================================================\n",
      "\n",
      "üì• USER QUERY:\n",
      "   Do you offer warranty on products?\n",
      "\n",
      "üìö RETRIEVED CONTEXT (3 sources):\n",
      "   üìó [1] products (Similarity: 54%)\n",
      "       Q: How long are your products covered under warranty?...\n",
      "   üìó [2] warranty (Similarity: 49%)\n",
      "       Q: What happens if my product breaks down within the first year...\n",
      "   üìó [3] customer_service (Similarity: 49%)\n",
      "       Q: I'd like to return a product, but I'm not sure if it's withi...\n",
      "\n",
      "üì§ GENERATED RESPONSE:\n",
      "----------------------------------------------------------------------\n",
      "Absolutely! We offer warranties on our products, but the length of coverage varies depending on the specific product category. Most items come with a warranty of 1-3 years. If you have any questions about the warranty for a particular product, please let me know, and I'd be happy to help. Additionally, if you experience any issues with your product within the warranty period, you can submit a claim by contacting our support team. Thank you for choosing GreenTech Marketplace! If you have more questions or need further assistance, please don't hesitate to ask.\n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üìó ANSWERABLE: \"How can I track my order?\"\n",
      "======================================================================\n",
      "   Step 1: embed_query() - Converting query to embedding...\n",
      "   Step 2: search_faiss() - Searching for similar Q&A pairs...\n",
      "           ‚Üí Found 3 relevant documents\n",
      "   Step 3: format_context() - Formatting retrieved context...\n",
      "   Step 4: build_prompt() - Building augmented prompt...\n",
      "   Step 5: generate_response() - Generating response with Mistral...\n",
      "           ‚Üí Response generated: 581 chars\n",
      "\n",
      "======================================================================\n",
      "ü§ñ RAG PIPELINE RESULT\n",
      "======================================================================\n",
      "\n",
      "üì• USER QUERY:\n",
      "   How can I track my order?\n",
      "\n",
      "üìö RETRIEVED CONTEXT (3 sources):\n",
      "   üìó [1] shipping (Similarity: 75%)\n",
      "       Q: How can I track my order?...\n",
      "   üìó [2] orders (Similarity: 75%)\n",
      "       Q: How can I track my order?...\n",
      "   üìó [3] orders (Similarity: 52%)\n",
      "       Q: I haven't received my order yet, what should I do?...\n",
      "\n",
      "üì§ GENERATED RESPONSE:\n",
      "----------------------------------------------------------------------\n",
      "Dear Customer,\n",
      "\n",
      "Thank you for shopping with GreenTech Marketplace! To track your order, please check your email for a tracking number. If you haven't received it, don't worry! You can also find the tracking number in your order confirmation email. Once you have the tracking number, you can use it to follow the progress of your package on the courier's website. If you encounter any issues locating your tracking number, feel free to reach out to our support team during our business hours, and we'll be more than happy to assist you.\n",
      "\n",
      "Best regards,\n",
      "The GreenTech Marketplace Team\n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n",
      "\n",
      "üß™ STEP 4: Test with UNANSWERABLE Questions\n",
      "----------------------------------------------------------------------\n",
      "   These questions CANNOT be answered from our knowledge base\n",
      "   Testing system limitations and graceful handling\n",
      "\n",
      "======================================================================\n",
      "üìï UNANSWERABLE: \"How do your prices compare to Amazon?\"\n",
      "======================================================================\n",
      "   Step 1: embed_query() - Converting query to embedding...\n",
      "   Step 2: search_faiss() - Searching for similar Q&A pairs...\n",
      "           ‚Üí Found 3 relevant documents\n",
      "   Step 3: format_context() - Formatting retrieved context...\n",
      "   Step 4: build_prompt() - Building augmented prompt...\n",
      "   Step 5: generate_response() - Generating response with Mistral...\n",
      "           ‚Üí Response generated: 529 chars\n",
      "\n",
      "======================================================================\n",
      "ü§ñ RAG PIPELINE RESULT\n",
      "======================================================================\n",
      "\n",
      "üì• USER QUERY:\n",
      "   How do your prices compare to Amazon?\n",
      "\n",
      "üìö RETRIEVED CONTEXT (3 sources):\n",
      "   üìó [1] returns (Similarity: 44%)\n",
      "       Q: I'd like to know about your shipping options. How does it wo...\n",
      "   üìó [2] customer_service (Similarity: 42%)\n",
      "       Q: What are the shipping options available for my order?...\n",
      "   üìó [3] shipping (Similarity: 42%)\n",
      "       Q: Do you offer free shipping?...\n",
      "\n",
      "üì§ GENERATED RESPONSE:\n",
      "----------------------------------------------------------------------\n",
      "Hello! I'm glad to assist you today. Regarding your question about our prices compared to Amazon, I'd be happy to provide some insight. Our products are competitively priced and are focused on sustainable technology solutions. While I don't have specific price comparison information available, I can assure you that we strive to offer high-quality, eco-friendly products at reasonable prices. Thank you for choosing GreenTech Marketplace. If you have any other questions or need further assistance, please don't hesitate to ask!\n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üìï UNANSWERABLE: \"Should I buy solar panels for my house?\"\n",
      "======================================================================\n",
      "   Step 1: embed_query() - Converting query to embedding...\n",
      "   Step 2: search_faiss() - Searching for similar Q&A pairs...\n",
      "           ‚Üí Found 3 relevant documents\n",
      "   Step 3: format_context() - Formatting retrieved context...\n",
      "   Step 4: build_prompt() - Building augmented prompt...\n",
      "   Step 5: generate_response() - Generating response with Mistral...\n",
      "           ‚Üí Response generated: 942 chars\n",
      "\n",
      "======================================================================\n",
      "ü§ñ RAG PIPELINE RESULT\n",
      "======================================================================\n",
      "\n",
      "üì• USER QUERY:\n",
      "   Should I buy solar panels for my house?\n",
      "\n",
      "üìö RETRIEVED CONTEXT (3 sources):\n",
      "   üìó [1] products (Similarity: 44%)\n",
      "       Q: What types of products do you sell on your platform?...\n",
      "   üìó [2] products (Similarity: 39%)\n",
      "       Q: What are some examples of smart devices you have available?...\n",
      "   üìó [3] customer_service (Similarity: 36%)\n",
      "       Q: What are your customer service hours and contact details?...\n",
      "\n",
      "üì§ GENERATED RESPONSE:\n",
      "----------------------------------------------------------------------\n",
      "Hello there!\n",
      "\n",
      "Thank you for reaching out to GreenTech Marketplace. Solar panels can be a great investment for your home, as they help reduce your energy costs and contribute to a more sustainable future. We offer a variety of solar panels on our platform, each designed to meet different energy needs.\n",
      "\n",
      "If you're considering solar panels, it's essential to assess your energy consumption and the amount of sunlight your home receives. This will help you determine the size and type of solar panel system that would best suit your needs.\n",
      "\n",
      "Our team is always here to help guide you through the process. If you'd like more information on solar panels or have questions about other eco-friendly products we offer, please don't hesitate to ask. We're happy to help!\n",
      "\n",
      "Warm regards,\n",
      "The GreenTech Marketplace Customer Service Team\n",
      "\n",
      "Contact us at support@greentechmarketplace.com, 1-800-GREEN-TECH, or through our live chat during our business hours.\n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üìï UNANSWERABLE: \"Will you have a Black Friday sale this year?\"\n",
      "======================================================================\n",
      "   Step 1: embed_query() - Converting query to embedding...\n",
      "   Step 2: search_faiss() - Searching for similar Q&A pairs...\n",
      "           ‚Üí Found 3 relevant documents\n",
      "   Step 3: format_context() - Formatting retrieved context...\n",
      "   Step 4: build_prompt() - Building augmented prompt...\n",
      "   Step 5: generate_response() - Generating response with Mistral...\n",
      "           ‚Üí Response generated: 406 chars\n",
      "\n",
      "======================================================================\n",
      "ü§ñ RAG PIPELINE RESULT\n",
      "======================================================================\n",
      "\n",
      "üì• USER QUERY:\n",
      "   Will you have a Black Friday sale this year?\n",
      "\n",
      "üìö RETRIEVED CONTEXT (3 sources):\n",
      "   üìó [1] returns (Similarity: 41%)\n",
      "       Q: I'd like to know about your shipping options. How does it wo...\n",
      "   üìó [2] shipping (Similarity: 40%)\n",
      "       Q: Do you offer free shipping?...\n",
      "   üìó [3] customer_service (Similarity: 39%)\n",
      "       Q: What are your customer service hours and contact details?...\n",
      "\n",
      "üì§ GENERATED RESPONSE:\n",
      "----------------------------------------------------------------------\n",
      "Dear Customer,\n",
      "\n",
      "Thank you for reaching out to GreenTech Marketplace! While we don't have information about Black Friday sales available at this time, we encourage you to check our website or sign up for our newsletter to stay updated on any upcoming promotions. We appreciate your interest and look forward to helping you with any other questions you may have.\n",
      "\n",
      "Warm regards,\n",
      "The GreenTech Marketplace Team\n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üìï UNANSWERABLE: \"What is the CEO's email address?\"\n",
      "======================================================================\n",
      "   Step 1: embed_query() - Converting query to embedding...\n",
      "   Step 2: search_faiss() - Searching for similar Q&A pairs...\n",
      "           ‚Üí Found 3 relevant documents\n",
      "   Step 3: format_context() - Formatting retrieved context...\n",
      "   Step 4: build_prompt() - Building augmented prompt...\n",
      "   Step 5: generate_response() - Generating response with Mistral...\n",
      "           ‚Üí Response generated: 344 chars\n",
      "\n",
      "======================================================================\n",
      "ü§ñ RAG PIPELINE RESULT\n",
      "======================================================================\n",
      "\n",
      "üì• USER QUERY:\n",
      "   What is the CEO's email address?\n",
      "\n",
      "üìö RETRIEVED CONTEXT (3 sources):\n",
      "   üìï [1] personal_advice (Similarity: 62%)\n",
      "       Q: What is the email address of the CEO of a competitor company...\n",
      "   üìï [2] future_events (Similarity: 42%)\n",
      "       Q: What is the password for the admin panel?...\n",
      "   üìó [3] customer_service (Similarity: 39%)\n",
      "       Q: What are your customer service hours and contact details?...\n",
      "\n",
      "üì§ GENERATED RESPONSE:\n",
      "----------------------------------------------------------------------\n",
      "I'm sorry for any inconvenience, but I'm unable to provide contact information for executives of other companies, including the CEO's email address. I'd be more than happy to help you with questions about our products, shipping, returns, or order tracking at GreenTech Marketplace. If you have any other questions, please don't hesitate to ask!\n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üìï UNANSWERABLE: \"Can you recommend a good restaurant nearby?\"\n",
      "======================================================================\n",
      "   Step 1: embed_query() - Converting query to embedding...\n",
      "   Step 2: search_faiss() - Searching for similar Q&A pairs...\n",
      "           ‚Üí Found 3 relevant documents\n",
      "   Step 3: format_context() - Formatting retrieved context...\n",
      "   Step 4: build_prompt() - Building augmented prompt...\n",
      "   Step 5: generate_response() - Generating response with Mistral...\n",
      "           ‚Üí Response generated: 516 chars\n",
      "\n",
      "======================================================================\n",
      "ü§ñ RAG PIPELINE RESULT\n",
      "======================================================================\n",
      "\n",
      "üì• USER QUERY:\n",
      "   Can you recommend a good restaurant nearby?\n",
      "\n",
      "üìö RETRIEVED CONTEXT (3 sources):\n",
      "   üìï [1] competitor (Similarity: 39%)\n",
      "       Q: What is the best time to visit Paris?...\n",
      "   üìó [2] products (Similarity: 38%)\n",
      "       Q: What are some examples of smart devices you have available?...\n",
      "   üìó [3] customer_service (Similarity: 36%)\n",
      "       Q: What are the shipping options available for my order?...\n",
      "\n",
      "üì§ GENERATED RESPONSE:\n",
      "----------------------------------------------------------------------\n",
      "I'm sorry for any inconvenience, but I'm unable to provide recommendations for restaurants as it's outside GreenTech Marketplace's scope. I can assist you with questions about our sustainable technology products, shipping, returns, or order tracking. If you have any questions about our products, feel free to ask! Otherwise, I'd recommend using a local search engine or a restaurant review website to find great eateries near you. Have a wonderful day! If you have any other questions, please don't hesitate to ask.\n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n",
      "\n",
      "üíæ STEP 5: Save Results\n",
      "----------------------------------------------------------------------\n",
      "‚úÖ RAG results saved to: data/rag_pipeline/rag_test_results.csv\n",
      "‚úÖ Pipeline config saved to: data/rag_pipeline/pipeline_config.txt\n",
      "\n",
      "‚úÖ STEP 6: Verify Objective 4\n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n",
      "üîç OBJECTIVE 4 VERIFICATION\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Objective 4 Complete - All components verified\n",
      "   ‚Ä¢ Pipeline Functions: 6 verified\n",
      "   ‚Ä¢ RAGResult: Defined\n",
      "   ‚Ä¢ Test Results: 10 queries\n",
      "   ‚Ä¢ Files: Saved to data/rag_pipeline/\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "‚úÖ OBJECTIVE 4 COMPLETE\n",
      "======================================================================\n",
      "\n",
      "RAG Pipeline Components:\n",
      "  1. Query Processing: embed_query() - Convert to embedding (from Objective 3)\n",
      "  2. Retrieval: search_faiss() - FAISS similarity search (top-3)\n",
      "  3. Augmentation: format_context() + build_prompt()\n",
      "  4. Generation: generate_response() - Mistral-7B response\n",
      "\n",
      "Test Results:\n",
      "  üìó Answerable Questions: 5/5 successful\n",
      "  üìï Unanswerable Questions: 5/5 successful\n",
      "\n",
      "üì¶ FILES SAVED:\n",
      "  ‚Ä¢ data/rag_pipeline/rag_test_results.csv\n",
      "  ‚Ä¢ data/rag_pipeline/pipeline_config.txt\n",
      "\n",
      "üì¶ GLOBAL FUNCTIONS:\n",
      "  ‚Ä¢ rag_query(query) - Complete RAG pipeline\n",
      "  ‚Ä¢ search_faiss(), format_context(), build_prompt(), generate_response()\n",
      "  ‚Ä¢ embed_query() - Reused from Objective 3\n",
      "  ‚Ä¢ verify_objective4() - Verification function\n",
      "\n",
      "üîú READY FOR OBJECTIVE 5: Model Experimentation and Ranking\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# OBJECTIVE 4: BUILD COMPLETE RAG PIPELINE\n",
    "# ============================================================================\n",
    "#\n",
    "# PIPELINE COMPONENTS:\n",
    "#   1. Query Processing - Convert user question to embedding\n",
    "#   2. Retrieval - Use FAISS to find top-k most similar Q&A pairs\n",
    "#   3. Augmentation - Combine user question with retrieved context\n",
    "#   4. Generation - Use Mistral to generate answer from augmented context\n",
    "#\n",
    "# WHY THIS ARCHITECTURE:\n",
    "#   - RAG combines retrieval (accurate, up-to-date info) with generation (natural responses)\n",
    "#   - Grounds answers in knowledge base, reducing hallucinations\n",
    "#   - Allows easy updates to knowledge base without retraining models\n",
    "#\n",
    "# 100% REUSE FROM PREVIOUS OBJECTIVES:\n",
    "#   - system_prompt, mistral_tokenizer, mistral_model (Objective 1)\n",
    "#   - qa_database (Objective 2)\n",
    "#   - embedding_model, faiss_index, embed_query (Objective 3)\n",
    "#\n",
    "# PREREQUISITES: Run Objectives 1, 2, and 3 first\n",
    "#\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 1: IMPORTS & VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "from typing import List, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "except ImportError as e:\n",
    "    raise ImportError(f\"Missing: {e}. Run: pip install numpy pandas torch\")\n",
    "\n",
    "\n",
    "def validate_prerequisites():\n",
    "    \"\"\"Ensure Objectives 1, 2, and 3 were run first.\"\"\"\n",
    "    required = {\n",
    "        'Objective 1': ['system_prompt', 'mistral_tokenizer', 'mistral_model'],\n",
    "        'Objective 2': ['qa_database'],\n",
    "        'Objective 3': ['embedding_model', 'faiss_index', 'embed_query']\n",
    "    }\n",
    "    \n",
    "    all_missing = []\n",
    "    for objective, items in required.items():\n",
    "        missing = [item for item in items if item not in globals()]\n",
    "        if missing:\n",
    "            all_missing.append(f\"{objective}: {missing}\")\n",
    "    \n",
    "    if all_missing:\n",
    "        raise RuntimeError(f\"Missing prerequisites:\\n\" + \"\\n\".join(all_missing))\n",
    "    \n",
    "    print(\"‚úÖ All prerequisites validated\")\n",
    "    print(f\"   ‚Ä¢ System prompt: {len(globals()['system_prompt'])} chars\")\n",
    "    print(f\"   ‚Ä¢ Q&A database: {len(globals()['qa_database'])} pairs\")\n",
    "    print(f\"   ‚Ä¢ FAISS index: {globals()['faiss_index'].ntotal} vectors\")\n",
    "    print(f\"   ‚Ä¢ embed_query(): Ready (from Objective 3)\")\n",
    "    return True\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 2: CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = \"data/rag_pipeline\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# RAG CONFIGURATION PARAMETERS EXPLAINED\n",
    "# ============================================================================\n",
    "#\n",
    "# top_k: Number of documents to retrieve from FAISS\n",
    "#   - Higher value (5-10): More context, better coverage, but may include noise\n",
    "#   - Lower value (1-3): More focused, less noise, but may miss relevant info\n",
    "#   - Default 3: Good balance for small knowledge bases (21 Q&A pairs)\n",
    "#\n",
    "# max_new_tokens: Maximum tokens the model can generate in response\n",
    "#   - Higher value (500+): Longer, more detailed responses\n",
    "#   - Lower value (100-200): Concise responses, faster generation\n",
    "#   - Default 300: Allows comprehensive answers without being verbose\n",
    "#\n",
    "# temperature: Controls randomness/creativity in generation (0.0 - 1.0)\n",
    "#   - 0.0: Deterministic, always picks most likely token (factual tasks)\n",
    "#   - 0.5-0.7: Balanced creativity and coherence (recommended for QA)\n",
    "#   - 1.0+: More random/creative (creative writing, brainstorming)\n",
    "#   - Default 0.7: Allows natural variation while staying on-topic\n",
    "#\n",
    "# similarity_threshold: Minimum similarity score to include a document (0.0 - 1.0)\n",
    "#   - Higher value (0.5+): Only very relevant documents, may return few/none\n",
    "#   - Lower value (0.1-0.3): More documents included, may have lower relevance\n",
    "#   - Default 0.3: Filters out clearly irrelevant results while being inclusive\n",
    "#\n",
    "# ============================================================================\n",
    "\n",
    "RAG_CONFIG = {\n",
    "    \"top_k\": 3,                    # Number of documents to retrieve\n",
    "    \"max_new_tokens\": 300,         # Max tokens for generation\n",
    "    \"temperature\": 0.7,            # Generation temperature\n",
    "    \"similarity_threshold\": 0.3,   # Minimum similarity score (0-1)\n",
    "}\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 3: RAG RESULT DATA CLASS\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class RAGResult:\n",
    "    \"\"\"Container for RAG pipeline results.\"\"\"\n",
    "    query: str\n",
    "    response: str\n",
    "    retrieved_context: List[Dict]\n",
    "    success: bool\n",
    "    error_message: Optional[str] = None\n",
    "\n",
    "# Export RAGResult globally\n",
    "globals()['RAGResult'] = RAGResult\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 4: RAG PIPELINE CORE FUNCTIONS\n",
    "# ============================================================================\n",
    "#\n",
    "# These functions form the complete RAG pipeline:\n",
    "#   1. search_faiss()       - Search FAISS for similar Q&A pairs\n",
    "#   2. format_context()     - Format retrieved Q&A as context string\n",
    "#   3. build_prompt()       - Build augmented prompt\n",
    "#   4. generate_response()  - Generate response with Mistral\n",
    "#\n",
    "# NOTE: embed_query() is REUSED from Objective 3 (not redefined here)\n",
    "#\n",
    "# ============================================================================\n",
    "\n",
    "def search_faiss(query_embedding: np.ndarray, top_k: int = None) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    STEP 2: Search FAISS index for similar Q&A pairs.\n",
    "    \n",
    "    Reuses: faiss_index from Objective 3, qa_database from Objective 2\n",
    "    \n",
    "    Args:\n",
    "        query_embedding: Query vector from embed_query()\n",
    "        top_k: Number of results to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        List of Q&A dicts with similarity scores\n",
    "    \"\"\"\n",
    "    if top_k is None:\n",
    "        top_k = RAG_CONFIG[\"top_k\"]\n",
    "    \n",
    "    faiss_index = globals()['faiss_index']\n",
    "    qa_database = globals()['qa_database']\n",
    "    \n",
    "    # Ensure proper shape for FAISS search\n",
    "    if len(query_embedding.shape) == 1:\n",
    "        query_embedding = query_embedding.reshape(1, -1)\n",
    "    \n",
    "    # Search FAISS index\n",
    "    distances, indices = faiss_index.search(query_embedding, top_k)\n",
    "    \n",
    "    # Get Q&A pairs with similarity scores\n",
    "    results = []\n",
    "    for dist, idx in zip(distances[0], indices[0]):\n",
    "        if idx < len(qa_database):\n",
    "            similarity = 1 / (1 + float(dist))  # Convert distance to similarity\n",
    "            if similarity >= RAG_CONFIG[\"similarity_threshold\"]:\n",
    "                qa = qa_database[idx].copy()\n",
    "                qa['similarity_score'] = similarity\n",
    "                qa['distance'] = float(dist)\n",
    "                results.append(qa)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def format_context(retrieved_qa: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    STEP 3: Format retrieved Q&A pairs as context string.\n",
    "    \n",
    "    Args:\n",
    "        retrieved_qa: List of Q&A dicts from search_faiss()\n",
    "    \n",
    "    Returns:\n",
    "        Formatted context string for prompt\n",
    "    \"\"\"\n",
    "    if not retrieved_qa:\n",
    "        return \"No relevant information found in knowledge base.\"\n",
    "    \n",
    "    context_parts = [\"RELEVANT INFORMATION FROM KNOWLEDGE BASE:\", \"-\" * 40]\n",
    "    \n",
    "    for i, qa in enumerate(retrieved_qa, 1):\n",
    "        similarity_pct = qa.get('similarity_score', 0) * 100\n",
    "        context_parts.append(f\"\\n[Source {i}] (Relevance: {similarity_pct:.0f}%)\")\n",
    "        context_parts.append(f\"Q: {qa['question']}\")\n",
    "        context_parts.append(f\"A: {qa['answer']}\")\n",
    "    \n",
    "    context_parts.append(\"-\" * 40)\n",
    "    return \"\\n\".join(context_parts)\n",
    "\n",
    "\n",
    "def build_prompt(query: str, context: str) -> str:\n",
    "    \"\"\"\n",
    "    STEP 4: Build augmented prompt combining system prompt, context, and query.\n",
    "    \n",
    "    Reuses: system_prompt from Objective 1\n",
    "    \n",
    "    Args:\n",
    "        query: User's question\n",
    "        context: Formatted context from format_context()\n",
    "    \n",
    "    Returns:\n",
    "        Complete augmented prompt\n",
    "    \"\"\"\n",
    "    system_prompt = globals()['system_prompt']\n",
    "    \n",
    "    augmented_prompt = f\"\"\"{system_prompt}\n",
    "\n",
    "{context}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Answer using ONLY the information provided above\n",
    "- If information is not available, politely say so\n",
    "- Be helpful, accurate, and concise\n",
    "\n",
    "CUSTOMER QUESTION: {query}\n",
    "\n",
    "ASSISTANT RESPONSE:\"\"\"\n",
    "    \n",
    "    return augmented_prompt\n",
    "\n",
    "\n",
    "def generate_response(augmented_prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    STEP 5: Generate response with Mistral model.\n",
    "    \n",
    "    Reuses: mistral_tokenizer, mistral_model from Objective 1\n",
    "    \n",
    "    Args:\n",
    "        augmented_prompt: Complete prompt from build_prompt()\n",
    "    \n",
    "    Returns:\n",
    "        Generated response string\n",
    "    \"\"\"\n",
    "    tokenizer = globals()['mistral_tokenizer']\n",
    "    model = globals()['mistral_model']\n",
    "    \n",
    "    # Format for Mistral Instruct\n",
    "    formatted = f\"<s>[INST] {augmented_prompt} [/INST]\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\", truncation=True, max_length=4096)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=RAG_CONFIG[\"max_new_tokens\"],\n",
    "            temperature=RAG_CONFIG[\"temperature\"],\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode only new tokens\n",
    "    input_length = inputs['input_ids'].shape[1]\n",
    "    response = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True).strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 5: COMPLETE RAG PIPELINE FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def rag_query(query: str, top_k: int = None, verbose: bool = True) -> RAGResult:\n",
    "    \"\"\"\n",
    "    Complete RAG Pipeline: Query ‚Üí Retrieve ‚Üí Augment ‚Üí Generate\n",
    "    \n",
    "    This is the main entry point for the RAG system.\n",
    "    Orchestrates all pipeline functions in sequence.\n",
    "    \n",
    "    Args:\n",
    "        query: User's question\n",
    "        top_k: Number of documents to retrieve (default: from config)\n",
    "        verbose: Print step-by-step progress\n",
    "    \n",
    "    Returns:\n",
    "        RAGResult with response and metadata\n",
    "    \n",
    "    Example:\n",
    "        result = rag_query(\"What is your return policy?\")\n",
    "        print(result.response)\n",
    "    \"\"\"\n",
    "    if top_k is None:\n",
    "        top_k = RAG_CONFIG[\"top_k\"]\n",
    "    \n",
    "    # Get embed_query from Objective 3\n",
    "    embed_query_func = globals()['embed_query']\n",
    "    \n",
    "    try:\n",
    "        # ============================================================\n",
    "        # STEP 1: QUERY PROCESSING - Convert to embedding\n",
    "        # ============================================================\n",
    "        if verbose:\n",
    "            print(f\"   Step 1: embed_query() - Converting query to embedding...\")\n",
    "        query_embedding = embed_query_func(query)\n",
    "        \n",
    "        # Ensure proper shape\n",
    "        if len(query_embedding.shape) == 1:\n",
    "            query_embedding = query_embedding.reshape(1, -1)\n",
    "        \n",
    "        # ============================================================\n",
    "        # STEP 2: RETRIEVAL - Search FAISS for similar Q&A\n",
    "        # ============================================================\n",
    "        if verbose:\n",
    "            print(f\"   Step 2: search_faiss() - Searching for similar Q&A pairs...\")\n",
    "        retrieved_qa = search_faiss(query_embedding, top_k)\n",
    "        if verbose:\n",
    "            print(f\"           ‚Üí Found {len(retrieved_qa)} relevant documents\")\n",
    "        \n",
    "        # ============================================================\n",
    "        # STEP 3: AUGMENTATION (Part 1) - Format context\n",
    "        # ============================================================\n",
    "        if verbose:\n",
    "            print(f\"   Step 3: format_context() - Formatting retrieved context...\")\n",
    "        context = format_context(retrieved_qa)\n",
    "        \n",
    "        # ============================================================\n",
    "        # STEP 4: AUGMENTATION (Part 2) - Build prompt\n",
    "        # ============================================================\n",
    "        if verbose:\n",
    "            print(f\"   Step 4: build_prompt() - Building augmented prompt...\")\n",
    "        augmented_prompt = build_prompt(query, context)\n",
    "        \n",
    "        # ============================================================\n",
    "        # STEP 5: GENERATION - Generate response with Mistral\n",
    "        # ============================================================\n",
    "        if verbose:\n",
    "            print(f\"   Step 5: generate_response() - Generating response with Mistral...\")\n",
    "        response = generate_response(augmented_prompt)\n",
    "        if verbose:\n",
    "            print(f\"           ‚Üí Response generated: {len(response)} chars\")\n",
    "        \n",
    "        return RAGResult(\n",
    "            query=query,\n",
    "            response=response,\n",
    "            retrieved_context=retrieved_qa,\n",
    "            success=True\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        return RAGResult(\n",
    "            query=query,\n",
    "            response=\"I apologize, but I encountered an error processing your request.\",\n",
    "            retrieved_context=[],\n",
    "            success=False,\n",
    "            error_message=str(e)\n",
    "        )\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 6: DISPLAY & STORAGE FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def display_rag_result(result: RAGResult):\n",
    "    \"\"\"Display RAG result in formatted way.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ü§ñ RAG PIPELINE RESULT\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nüì• USER QUERY:\")\n",
    "    print(f\"   {result.query}\")\n",
    "    \n",
    "    print(f\"\\nüìö RETRIEVED CONTEXT ({len(result.retrieved_context)} sources):\")\n",
    "    for i, ctx in enumerate(result.retrieved_context, 1):\n",
    "        similarity = ctx.get('similarity_score', 0) * 100\n",
    "        answerable = 'üìó' if ctx.get('answerable', True) else 'üìï'\n",
    "        print(f\"   {answerable} [{i}] {ctx.get('category', 'N/A')} (Similarity: {similarity:.0f}%)\")\n",
    "        print(f\"       Q: {ctx['question'][:60]}...\")\n",
    "    \n",
    "    print(f\"\\nüì§ GENERATED RESPONSE:\")\n",
    "    print(\"-\"*70)\n",
    "    print(result.response)\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    if not result.success:\n",
    "        print(f\"\\n‚ö†Ô∏è  Error: {result.error_message}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "\n",
    "\n",
    "def save_rag_results(results: List[RAGResult], filename: str = \"rag_test_results.csv\"):\n",
    "    \"\"\"Save RAG test results to CSV.\"\"\"\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    \n",
    "    rows = []\n",
    "    for result in results:\n",
    "        rows.append({\n",
    "            'query': result.query,\n",
    "            'response': result.response[:500],\n",
    "            'num_sources': len(result.retrieved_context),\n",
    "            'top_source_similarity': result.retrieved_context[0]['similarity_score'] if result.retrieved_context else 0,\n",
    "            'success': result.success,\n",
    "            'error': result.error_message\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(filepath, index=False)\n",
    "    print(f\"‚úÖ RAG results saved to: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def save_pipeline_config(filename: str = \"pipeline_config.txt\"):\n",
    "    \"\"\"Save pipeline configuration.\"\"\"\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    \n",
    "    config_text = f\"\"\"RAG PIPELINE CONFIGURATION\n",
    "==========================\n",
    "\n",
    "Retrieval Settings:\n",
    "- Top-K: {RAG_CONFIG['top_k']}\n",
    "- Similarity Threshold: {RAG_CONFIG['similarity_threshold']}\n",
    "\n",
    "Generation Settings:\n",
    "- Max New Tokens: {RAG_CONFIG['max_new_tokens']}\n",
    "- Temperature: {RAG_CONFIG['temperature']}\n",
    "\n",
    "Components:\n",
    "- Embedding Model: sentence-transformers/all-MiniLM-L6-v2\n",
    "- Vector Store: FAISS IndexFlatL2\n",
    "- LLM: Mistral-7B-Instruct-v0.3\n",
    "- Q&A Database: {len(globals().get('qa_database', []))} pairs\n",
    "\"\"\"\n",
    "    \n",
    "    with open(filepath, 'w') as f:\n",
    "        f.write(config_text)\n",
    "    \n",
    "    print(f\"‚úÖ Pipeline config saved to: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 7: VERIFICATION FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def verify_objective4():\n",
    "    \"\"\"\n",
    "    Verify that Objective 4 completed successfully.\n",
    "    Checks all functions, files, and test results.\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"üîç OBJECTIVE 4 VERIFICATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    errors = []\n",
    "    \n",
    "    # Check required functions exist\n",
    "    required_functions = ['rag_query', 'search_faiss', 'format_context', \n",
    "                          'build_prompt', 'generate_response', 'embed_query']\n",
    "    for func_name in required_functions:\n",
    "        if func_name not in globals() or not callable(globals()[func_name]):\n",
    "            errors.append(f\"‚ùå Function '{func_name}' not found\")\n",
    "    \n",
    "    # Check RAGResult dataclass\n",
    "    if 'RAGResult' not in globals():\n",
    "        errors.append(\"‚ùå RAGResult dataclass not found\")\n",
    "    \n",
    "    # Check files exist\n",
    "    if not os.path.exists(\"data/rag_pipeline/rag_test_results.csv\"):\n",
    "        errors.append(\"‚ùå rag_test_results.csv not found\")\n",
    "    if not os.path.exists(\"data/rag_pipeline/pipeline_config.txt\"):\n",
    "        errors.append(\"‚ùå pipeline_config.txt not found\")\n",
    "    \n",
    "    # Check test results\n",
    "    if 'rag_results' not in globals():\n",
    "        errors.append(\"‚ùå rag_results not found\")\n",
    "    elif len(globals()['rag_results']) != 10:\n",
    "        errors.append(f\"‚ùå Expected 10 test results, got {len(globals()['rag_results'])}\")\n",
    "    \n",
    "    # Test rag_query function\n",
    "    try:\n",
    "        test_result = rag_query(\"test query\", verbose=False)\n",
    "        if not isinstance(test_result, RAGResult):\n",
    "            errors.append(\"‚ùå rag_query() does not return RAGResult\")\n",
    "    except Exception as e:\n",
    "        errors.append(f\"‚ùå rag_query() test failed: {e}\")\n",
    "    \n",
    "    # Print results\n",
    "    if errors:\n",
    "        print(\"\\n‚ùå VERIFICATION FAILED:\")\n",
    "        print(\"\\n\".join(errors))\n",
    "        print(\"=\"*70)\n",
    "        return False\n",
    "    else:\n",
    "        print(\"\\n‚úÖ Objective 4 Complete - All components verified\")\n",
    "        print(f\"   ‚Ä¢ Pipeline Functions: {len(required_functions)} verified\")\n",
    "        print(f\"   ‚Ä¢ RAGResult: Defined\")\n",
    "        print(f\"   ‚Ä¢ Test Results: {len(globals()['rag_results'])} queries\")\n",
    "        print(f\"   ‚Ä¢ Files: Saved to data/rag_pipeline/\")\n",
    "        print(\"=\"*70)\n",
    "        return True\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 8: EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"   OBJECTIVE 4: BUILD COMPLETE RAG PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# --- Step 1: Validate Prerequisites ---\n",
    "print(\"\\nüîç STEP 1: Validate Prerequisites\")\n",
    "print(\"-\"*70)\n",
    "validate_prerequisites()\n",
    "\n",
    "# --- Step 2: Show Pipeline Architecture ---\n",
    "print(\"\\nüìê STEP 2: RAG Pipeline Architecture\")\n",
    "print(\"-\"*70)\n",
    "print(\"\"\"\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                      RAG PIPELINE FLOW                          ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                 ‚îÇ\n",
    "‚îÇ  User Query                                                     ‚îÇ\n",
    "‚îÇ      ‚îÇ                                                          ‚îÇ\n",
    "‚îÇ      ‚ñº                                                          ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                            ‚îÇ\n",
    "‚îÇ  ‚îÇ 1. embed_query()‚îÇ  Convert query to embedding (from Obj 3)   ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                            ‚îÇ\n",
    "‚îÇ           ‚ñº                                                     ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                            ‚îÇ\n",
    "‚îÇ  ‚îÇ 2. search_faiss()‚îÇ  Find similar Q&A pairs                   ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                            ‚îÇ\n",
    "‚îÇ           ‚ñº                                                     ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                           ‚îÇ\n",
    "‚îÇ  ‚îÇ 3. format_context()‚îÇ  Format as context string               ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                           ‚îÇ\n",
    "‚îÇ           ‚ñº                                                     ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                            ‚îÇ\n",
    "‚îÇ  ‚îÇ 4. build_prompt()‚îÇ  Combine system + context + query         ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                            ‚îÇ\n",
    "‚îÇ           ‚ñº                                                     ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                        ‚îÇ\n",
    "‚îÇ  ‚îÇ 5. generate_response()‚îÇ  Generate with Mistral               ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                        ‚îÇ\n",
    "‚îÇ           ‚ñº                                                     ‚îÇ\n",
    "‚îÇ      Response                                                   ‚îÇ\n",
    "‚îÇ                                                                 ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\"\"\")\n",
    "\n",
    "# --- Step 3: Test RAG Pipeline with Answerable Questions ---\n",
    "print(\"\\nüß™ STEP 3: Test with ANSWERABLE Questions\")\n",
    "print(\"-\"*70)\n",
    "print(\"   These questions CAN be answered from our knowledge base\")\n",
    "\n",
    "answerable_questions = [\n",
    "    \"What is your return policy?\",\n",
    "    \"How long does shipping take?\",\n",
    "    \"What are your customer service hours?\",\n",
    "    \"Do you offer warranty on products?\",\n",
    "    \"How can I track my order?\",\n",
    "]\n",
    "\n",
    "answerable_results = []\n",
    "\n",
    "for query in answerable_questions:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìó ANSWERABLE: \\\"{query}\\\"\")\n",
    "    print('='*70)\n",
    "    \n",
    "    result = rag_query(query, verbose=True)\n",
    "    answerable_results.append(result)\n",
    "    display_rag_result(result)\n",
    "\n",
    "# --- Step 4: Test RAG Pipeline with Unanswerable Questions ---\n",
    "print(\"\\nüß™ STEP 4: Test with UNANSWERABLE Questions\")\n",
    "print(\"-\"*70)\n",
    "print(\"   These questions CANNOT be answered from our knowledge base\")\n",
    "print(\"   Testing system limitations and graceful handling\")\n",
    "\n",
    "unanswerable_questions = [\n",
    "    \"How do your prices compare to Amazon?\",\n",
    "    \"Should I buy solar panels for my house?\",\n",
    "    \"Will you have a Black Friday sale this year?\",\n",
    "    \"What is the CEO's email address?\",\n",
    "    \"Can you recommend a good restaurant nearby?\",\n",
    "]\n",
    "\n",
    "unanswerable_results = []\n",
    "\n",
    "for query in unanswerable_questions:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìï UNANSWERABLE: \\\"{query}\\\"\")\n",
    "    print('='*70)\n",
    "    \n",
    "    result = rag_query(query, verbose=True)\n",
    "    unanswerable_results.append(result)\n",
    "    display_rag_result(result)\n",
    "\n",
    "# --- Step 5: Save Results ---\n",
    "print(\"\\nüíæ STEP 5: Save Results\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "all_results = answerable_results + unanswerable_results\n",
    "save_rag_results(all_results)\n",
    "save_pipeline_config()\n",
    "\n",
    "# Store globally for reuse\n",
    "globals()['rag_query'] = rag_query\n",
    "globals()['rag_results'] = all_results\n",
    "\n",
    "# Export core functions globally\n",
    "globals()['search_faiss'] = search_faiss\n",
    "globals()['format_context'] = format_context\n",
    "globals()['build_prompt'] = build_prompt\n",
    "globals()['generate_response'] = generate_response\n",
    "globals()['verify_objective4'] = verify_objective4\n",
    "\n",
    "# --- Step 6: Verify Objective 4 ---\n",
    "print(\"\\n‚úÖ STEP 6: Verify Objective 4\")\n",
    "print(\"-\"*70)\n",
    "verify_objective4()\n",
    "\n",
    "# --- Summary ---\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ OBJECTIVE 4 COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "answerable_success = sum(1 for r in answerable_results if r.success)\n",
    "unanswerable_success = sum(1 for r in unanswerable_results if r.success)\n",
    "\n",
    "print(f\"\"\"\n",
    "RAG Pipeline Components:\n",
    "  1. Query Processing: embed_query() - Convert to embedding (from Objective 3)\n",
    "  2. Retrieval: search_faiss() - FAISS similarity search (top-{RAG_CONFIG['top_k']})\n",
    "  3. Augmentation: format_context() + build_prompt()\n",
    "  4. Generation: generate_response() - Mistral-7B response\n",
    "\n",
    "Test Results:\n",
    "  üìó Answerable Questions: {answerable_success}/{len(answerable_results)} successful\n",
    "  üìï Unanswerable Questions: {unanswerable_success}/{len(unanswerable_results)} successful\n",
    "\n",
    "üì¶ FILES SAVED:\n",
    "  ‚Ä¢ {OUTPUT_DIR}/rag_test_results.csv\n",
    "  ‚Ä¢ {OUTPUT_DIR}/pipeline_config.txt\n",
    "\n",
    "üì¶ GLOBAL FUNCTIONS:\n",
    "  ‚Ä¢ rag_query(query) - Complete RAG pipeline\n",
    "  ‚Ä¢ search_faiss(), format_context(), build_prompt(), generate_response()\n",
    "  ‚Ä¢ embed_query() - Reused from Objective 3\n",
    "  ‚Ä¢ verify_objective4() - Verification function\n",
    "\n",
    "üîú READY FOR OBJECTIVE 5: Model Experimentation and Ranking\n",
    "\"\"\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective 5: Model Evaluation & Ranking\n",
    "\n",
    "### üéØ Goal\n",
    "\n",
    "Evaluate 6 question-answering models using the RAG pipeline, compare their performance using **5 evaluation metrics**, and rank models to identify the best performer based on weighted scoring.\n",
    "\n",
    "<details>\n",
    "<summary><b>üì• Prerequisites</b> (Click to expand)</summary>\n",
    "\n",
    "| Item | Source | Required | Description |\n",
    "|------|--------|----------|-------------|\n",
    "| `qa_database` | Objective 2 | ‚úÖ Yes | Ground truth answers for comparison |\n",
    "| `embed_query()` | Objective 3 | ‚úÖ Yes | For semantic similarity calculation |\n",
    "| `rag_query()` | Objective 4 | ‚úÖ Yes | Complete RAG pipeline for dynamic context |\n",
    "| `search_faiss()`, `format_context()` | Objective 4 | ‚úÖ Yes | RAG pipeline components |\n",
    "\n",
    "**Note:** Requires Objectives 1-4 completed. Achieves **100% component reuse**.\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>üìä The 5 Evaluation Metrics</b> (Click to expand)</summary>\n",
    "\n",
    "### 1. Accuracy (BERTScore F1) - Weight: 25%\n",
    "**What it measures:** Semantic similarity between model answer and ground truth using BERTScore\n",
    "\n",
    "**Why BERTScore instead of token F1:**\n",
    "- Token F1 fails on paraphrases: \"30 day return\" vs \"30-day refund\" = low score\n",
    "- BERTScore understands semantics: same meaning = high score\n",
    "- Better for RAG evaluation where answers may be paraphrased\n",
    "\n",
    "**Implementation:**\n",
    "- Uses **BERTScore** with **DeBERTa-large-mnli** model\n",
    "- Calculates semantic F1 score (0-1 range)\n",
    "- Batch processing for efficiency\n",
    "- Rescaled with baseline for better calibration\n",
    "\n",
    "**Only computed for:** Answerable questions (unanswerable have no expected answer)\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Confidence (Calibration) - Weight: 20%\n",
    "**What it measures:** Whether model appropriately indicates uncertainty (calibration)\n",
    "\n",
    "**Scoring Logic:**\n",
    "\n",
    "**For Answerable Questions:**\n",
    "- Empty answer (< 3 chars) ‚Üí **Score: 0.2** ‚ö†Ô∏è\n",
    "- Non-empty answer ‚Üí **Score: raw_confidence** (0-1)\n",
    "\n",
    "**For Unanswerable Questions:**\n",
    "- Empty answer (abstains) ‚Üí **Score: 1.0** ‚úÖ (correct behavior)\n",
    "- Low confidence (< 0.3) ‚Üí **Score: 0.9** ‚úÖ\n",
    "- Medium confidence (0.3-0.5) ‚Üí **Score: 0.6** ‚ö†Ô∏è\n",
    "- High confidence (> 0.5) ‚Üí **Score: 0.2** ‚ùå (overconfident)\n",
    "\n",
    "**Computed for:** ALL questions (both answerable and unanswerable)\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Quality (Semantic Similarity) - Weight: 25%\n",
    "**What it measures:** Semantic meaning similarity using embeddings\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "Quality = cosine_similarity(embed_query(answer), embed_query(expected))\n",
    "```\n",
    "\n",
    "**Calculation:**\n",
    "- Uses `embed_query()` from Objective 3 (component reuse!)\n",
    "- Embed both answer and expected answer\n",
    "- Calculate cosine similarity between embeddings\n",
    "- Range: 0-1 (1 = identical meaning)\n",
    "\n",
    "**Only computed for:** Answerable questions\n",
    "\n",
    "**Key Insight:** Reuses `embed_query()` from Objective 3 - same function used in RAG retrieval!\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Speed - Weight: 15%\n",
    "**What it measures:** Response time performance\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "Speed = 1 - (response_time_ms / 2000)\n",
    "```\n",
    "\n",
    "**Calculation:**\n",
    "- Measure time from query to answer (milliseconds)\n",
    "- Normalize against 2000ms threshold\n",
    "- 0ms = 1.0 (perfect), 2000ms+ = 0.0 (too slow)\n",
    "- Faster responses = higher score\n",
    "- Range: 0-1 (clamped)\n",
    "\n",
    "**Computed for:** ALL questions\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Robustness - Weight: 15%\n",
    "**What it measures:** Edge case handling and error recovery\n",
    "\n",
    "**Scoring Logic:**\n",
    "\n",
    "**For Answerable Questions:**\n",
    "- **Error during inference** ‚Üí Score: 0.0 ‚ùå\n",
    "- **Empty answer** (< 3 chars) ‚Üí Score: 0.3 ‚ö†Ô∏è\n",
    "- **Long answer** (> 50 words) ‚Üí Score: 0.7 ‚ö†Ô∏è (possible verbosity)\n",
    "- **Normal answer** (3-50 words) ‚Üí Score: 1.0 ‚úÖ\n",
    "\n",
    "**For Unanswerable Questions:**\n",
    "- **Error during inference** ‚Üí Score: 0.0 ‚ùå\n",
    "- **Empty answer** (abstains) ‚Üí Score: 1.0 ‚úÖ (correct behavior)\n",
    "- **Long answer** (> 30 words) ‚Üí Score: 0.2 ‚ùå (hallucinating)\n",
    "- **Short answer** (3-30 words) ‚Üí Score: 0.6 ‚ö†Ô∏è\n",
    "\n",
    "**Computed for:** ALL questions\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>üèÜ Ranking Calculation</b> (Click to expand)</summary>\n",
    "\n",
    "### Step 1: Calculate Per-Question Metrics\n",
    "For each question, compute all 5 metrics:\n",
    "- Accuracy (F1) - if answerable\n",
    "- Confidence - always\n",
    "- Quality (Semantic) - if answerable\n",
    "- Speed - always\n",
    "- Robustness - always\n",
    "\n",
    "### Step 2: Aggregate Per Model\n",
    "For each model, average each metric across all questions:\n",
    "```\n",
    "avg_accuracy = mean(accuracy_scores for answerable questions)\n",
    "avg_confidence = mean(confidence_scores for all questions)\n",
    "avg_quality = mean(quality_scores for answerable questions)\n",
    "avg_speed = mean(speed_scores for all questions)\n",
    "avg_robustness = mean(robustness_scores for all questions)\n",
    "```\n",
    "\n",
    "### Step 3: Calculate Final Score\n",
    "Weighted combination of all 5 metrics:\n",
    "\n",
    "```\n",
    "Final Score = (Accuracy √ó 0.25) + \n",
    "              (Confidence √ó 0.20) + \n",
    "              (Quality √ó 0.25) + \n",
    "              (Speed √ó 0.15) + \n",
    "              (Robustness √ó 0.15)\n",
    "```\n",
    "\n",
    "**Weight Distribution:**\n",
    "- **Accuracy + Quality = 50%** (correctness and meaning)\n",
    "- **Confidence = 20%** (uncertainty handling)\n",
    "- **Speed + Robustness = 30%** (performance and reliability)\n",
    "\n",
    "### Step 4: Rank Models\n",
    "Sort all models by Final Score (descending):\n",
    "- Rank 1 = Highest Final Score (Best Model)\n",
    "- Rank 2 = Second highest\n",
    "- ... and so on\n",
    "\n",
    "### Example Calculation:\n",
    "\n",
    "**Model: RoBERTa-SQuAD2**\n",
    "- Accuracy: 0.847\n",
    "- Confidence: 0.756\n",
    "- Quality: 0.891\n",
    "- Speed: 0.823\n",
    "- Robustness: 0.950\n",
    "\n",
    "**Final Score:**\n",
    "```\n",
    "(0.847 √ó 0.25) + (0.756 √ó 0.20) + (0.891 √ó 0.25) + (0.823 √ó 0.15) + (0.950 √ó 0.15)\n",
    "= 0.212 + 0.151 + 0.223 + 0.123 + 0.143\n",
    "= 0.852\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ü§ñ Models Evaluated</b> (Click to expand)</summary>\n",
    "\n",
    "| Rank | Model Name | Model ID | Type | Size | Params |\n",
    "|------|-----------|----------|------|------|--------|\n",
    "| - | T5-QA-Generative | consciousAI/question-answering-generative-t5-v1-base-s-q-c | text2text-generation | Base | 220M |\n",
    "| - | RoBERTa-SQuAD2 | deepset/roberta-base-squad2 | question-answering | Base | 125M |\n",
    "| - | BERT-Large-SQuAD | google-bert/bert-large-cased-whole-word-masking-finetuned-squad | question-answering | Large | 340M |\n",
    "| - | DistilBERT-SQuAD | distilbert-base-uncased-distilled-squad | question-answering | Small | 66M |\n",
    "| - | BERT-Tiny-SQuAD | mrm8488/bert-tiny-finetuned-squadv2 | question-answering | Tiny | 4M |\n",
    "| - | MiniLM-SQuAD | deepset/minilm-uncased-squad2 | question-answering | Small | 33M |\n",
    "\n",
    "**Note:** Rankings determined by Final Score after evaluation.\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>üìã Test Questions</b> (Click to expand)</summary>\n",
    "\n",
    "**Answerable (5 questions):**\n",
    "1. \"What is your return policy?\" ‚Üí Expected: \"30-day return policy with full refund\"\n",
    "2. \"How long does shipping take?\" ‚Üí Expected: \"Standard shipping takes 5-7 business days\"\n",
    "3. \"What are your customer service hours?\" ‚Üí Expected: \"Monday to Friday 9am to 5pm\"\n",
    "4. \"Do you offer warranty on products?\" ‚Üí Expected: \"1 year warranty on all electronics\"\n",
    "5. \"How can I track my order?\" ‚Üí Expected: \"Use tracking number on our website\"\n",
    "\n",
    "**Unanswerable (3 questions):**\n",
    "1. \"What is the CEO's phone number?\" ‚Üí Not in knowledge base\n",
    "2. \"Will you have a Black Friday sale?\" ‚Üí Not in knowledge base\n",
    "3. \"How do prices compare to Amazon?\" ‚Üí Not in knowledge base\n",
    "\n",
    "**Why this split:**\n",
    "- Answerable questions test: Accuracy, Quality\n",
    "- Unanswerable questions test: Confidence Handling, Robustness\n",
    "- All questions test: Speed\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>üîÑ Evaluation Process</b> (Click to expand)</summary>\n",
    "\n",
    "**3-Stage Caching System:**\n",
    "1. **Contexts Cache** (`contexts.csv`) - RAG contexts from `rag_query()` (Obj 4)\n",
    "2. **Embeddings Cache** (`embeddings.csv`) - Embeddings computed via `embed_query()` (Obj 3)\n",
    "3. **Responses Cache** (`raw_responses.csv`) - Model inference results (6 models √ó 8 questions)\n",
    "\n",
    "**For Each Model:**\n",
    "1. Load model from HuggingFace\n",
    "2. For each test question:\n",
    "   - Get RAG context using `rag_query()` (reuses Obj 4, cached)\n",
    "   - Run model inference with retrieved context\n",
    "   - Measure response time\n",
    "   - Store raw response and confidence\n",
    "\n",
    "**Metric Calculation (Batch Processing):**\n",
    "- Uses **ScorePack** class for unified scoring\n",
    "- **Accuracy:** BERTScore F1 (DeBERTa-large-mnli) - batch processed\n",
    "- **Quality:** Cosine similarity using `embed_query()` (reuses Obj 3)\n",
    "- **Confidence:** Calibration scoring based on answerability\n",
    "- **Speed:** Normalized latency (2000ms threshold)\n",
    "- **Robustness:** Error handling and answer length checks\n",
    "\n",
    "**After All Models:**\n",
    "- Aggregate metrics per model (average across questions)\n",
    "- Calculate Final Scores (weighted combination)\n",
    "- Rank models by Final Score\n",
    "- Save to `model_rankings.csv`, `model_responses.csv`, `evaluation_summary.txt`\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>üì§ Outputs</b> (Click to expand)</summary>\n",
    "\n",
    "**Files Created:**\n",
    "- `model_rankings.csv` - Final rankings with all metrics\n",
    "- `model_responses.csv` - Detailed per-question results\n",
    "- `evaluation_summary.txt` - Text summary with winner\n",
    "- `contexts.csv` - Cached RAG contexts (cache)\n",
    "- `embeddings.csv` - Cached embeddings (cache)\n",
    "- `raw_responses.csv` - Cached model responses (cache)\n",
    "\n",
    "**Global Variables:**\n",
    "- `rankings_df` - DataFrame with model rankings (sorted by Final_Score)\n",
    "- `detailed_df` - DataFrame with per-question detailed results\n",
    "\n",
    "**CSV Columns (model_rankings.csv):**\n",
    "| Rank | Model | Accuracy | Quality | Confidence | Speed | Robustness | Final_Score |\n",
    "\n",
    "**Helper Functions:**\n",
    "- `run_evaluation(force_refresh=False)` - Full evaluation pipeline\n",
    "- `recalculate_metrics_only()` - Recalculate metrics from cache (~30 sec)\n",
    "- `clean()` - Delete all cache files\n",
    "- `show_cache_status()` - Display cache file status\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>üîó Component Reuse</b> (Click to expand)</summary>\n",
    "\n",
    "**100% Reuse from Previous Objectives:**\n",
    "\n",
    "| Component | From | Used For |\n",
    "|-----------|------|----------|\n",
    "| `rag_query()` | Obj 4 | Dynamic context retrieval per question (cached) |\n",
    "| `embed_query()` | Obj 3 | **Semantic similarity metric** (Quality) + embeddings cache |\n",
    "| `search_faiss()` | Obj 4 | Via `rag_query()` |\n",
    "| `format_context()` | Obj 4 | Via `rag_query()` |\n",
    "| `qa_database` | Obj 2 | Ground truth answers |\n",
    "| `faiss_index` | Obj 3 | Via `search_faiss()` |\n",
    "\n",
    "**Key Insights:**\n",
    "1. **`embed_query()` dual use:** Reused for BOTH RAG retrieval (Obj 4) AND semantic similarity (Obj 5)\n",
    "2. **ScorePack class:** Unified scoring system with batch processing and embedding caching\n",
    "3. **3-stage caching:** Contexts, embeddings, and responses cached for fast re-runs\n",
    "4. **Batch BERTScore:** Efficient batch processing using DeBERTa-large-mnli model\n",
    "\n",
    "This demonstrates true modular design with zero code duplication and efficient caching.\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>‚úÖ Verification</b> (Click to expand)</summary>\n",
    "\n",
    "**Verification:**\n",
    "- ‚úÖ `rankings_df` exists with 6 models\n",
    "- ‚úÖ `model_rankings.csv` file created\n",
    "- ‚úÖ `model_responses.csv` with detailed per-question results\n",
    "- ‚úÖ All 5 metrics present (Accuracy, Quality, Confidence, Speed, Robustness)\n",
    "- ‚úÖ Final_Score calculated using weighted combination\n",
    "- ‚úÖ Models ranked by Final_Score (descending)\n",
    "\n",
    "**Performance:**\n",
    "- **First run:** ~10-15 min (downloads models, runs inference, computes metrics)\n",
    "- **Subsequent runs:** ~30 sec (loads from cache, recalculates metrics only)\n",
    "- **Recalculate only:** Use `recalculate_metrics_only()` to update metrics from cached responses\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "**Next Step:** Proceed to Objective 6 for system analysis and recommendations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ BERTScore available\n",
      "‚úÖ ScorePack loaded\n",
      "üìÅ Output: data/model_evaluation/\n",
      "üñ•Ô∏è  Device: CPU\n",
      "‚úÖ All dependencies from Objectives 1-4 verified\n",
      "\n",
      "============================================================\n",
      "üìÅ CACHE STATUS\n",
      "============================================================\n",
      "   ‚úÖ contexts.csv\n",
      "   ‚úÖ embeddings.csv\n",
      "   ‚úÖ raw_responses.csv\n",
      "\n",
      "============================================================\n",
      "üöÄ OBJECTIVE 5: MODEL EVALUATION\n",
      "============================================================\n",
      "   Reusing: embed_query (Obj 3), rag_query (Obj 4)\n",
      "\n",
      "üí° Loading from cache (use force_refresh=True to rebuild)...\n",
      "\n",
      "üìä Calculating metrics (ScorePack + BERTScore)...\n",
      "   üîÑ Running BERTScore (DeBERTa-large-mnli)...\n",
      "   ‚úÖ Scored 48 responses\n",
      "   üîÑ Aggregating by model...\n",
      "   ‚úÖ T5-QA-Generative: 0.430\n",
      "   ‚úÖ RoBERTa-SQuAD2: 0.568\n",
      "   ‚úÖ BERT-Large-SQuAD: 0.562\n",
      "   ‚úÖ DistilBERT-SQuAD: 0.571\n",
      "   ‚úÖ BERT-Tiny-SQuAD: 0.465\n",
      "   ‚úÖ MiniLM-SQuAD: 0.598\n",
      "\n",
      "‚úÖ Saved: data/model_evaluation/model_rankings.csv\n",
      "‚úÖ Saved: data/model_evaluation/model_responses.csv\n",
      "‚úÖ Saved: data/model_evaluation/evaluation_summary.txt\n",
      "\n",
      "‚è±Ô∏è  Total time: 1.5s\n",
      "\n",
      "============================================================\n",
      "üèÜ FINAL RANKINGS\n",
      "============================================================\n",
      " Rank            Model  Accuracy  Quality  Confidence  Speed  Robustness  Final_Score\n",
      "    1     MiniLM-SQuAD     0.328    0.609       0.438  0.988        0.85        0.598\n",
      "    2 DistilBERT-SQuAD     0.297    0.550       0.419  0.988        0.85        0.571\n",
      "    3   RoBERTa-SQuAD2     0.366    0.477       0.416  0.976        0.85        0.568\n",
      "    4 BERT-Large-SQuAD     0.333    0.546       0.380  0.927        0.85        0.562\n",
      "    5  BERT-Tiny-SQuAD     0.018    0.459       0.341  0.998        0.85        0.465\n",
      "    6 T5-QA-Generative     0.178    0.438       0.512  0.311        0.85        0.430\n",
      "\n",
      "ü•á WINNER: MiniLM-SQuAD (0.598)\n",
      "\n",
      "============================================================\n",
      "üèÜ WINNER SUMMARY\n",
      "============================================================\n",
      "\n",
      "    ü•á BEST MODEL: MiniLM-SQuAD\n",
      "\n",
      "    Final Score: 0.598\n",
      "\n",
      "    SCORES:\n",
      "    ‚îú‚îÄ‚îÄ Accuracy (BERTScore): 0.328\n",
      "    ‚îú‚îÄ‚îÄ Quality (Embedding):  0.609\n",
      "    ‚îú‚îÄ‚îÄ Confidence:           0.438\n",
      "    ‚îú‚îÄ‚îÄ Speed:                0.988\n",
      "    ‚îî‚îÄ‚îÄ Robustness:           0.850\n",
      "    \n",
      "============================================================\n",
      "üîç MODEL INSIGHTS: SIZE vs PERFORMANCE\n",
      "============================================================\n",
      "\n",
      "   Model                Size     Params     Score   \n",
      "   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "   MiniLM-SQuAD         Small    33M        0.598\n",
      "   DistilBERT-SQuAD     Small    66M        0.571\n",
      "   RoBERTa-SQuAD2       Base     125M       0.568\n",
      "   BERT-Large-SQuAD     Large    340M       0.562\n",
      "   BERT-Tiny-SQuAD      Tiny     4M         0.465\n",
      "   T5-QA-Generative     Base     220M       0.430\n",
      "\n",
      "   RECOMMENDATIONS:\n",
      "   ‚Ä¢ Speed-critical:    BERT-Tiny-SQuAD\n",
      "   ‚Ä¢ Accuracy-critical: RoBERTa-SQuAD2\n",
      "   ‚Ä¢ Best overall:      MiniLM-SQuAD\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# OBJECTIVE 5: MODEL EVALUATION (ScorePack)\n",
    "# ============================================================================\n",
    "#\n",
    "# WHAT THIS DOES:\n",
    "#   - Evaluates 6 QA models on our RAG pipeline\n",
    "#   - Uses 5 metrics: Accuracy, Quality, Confidence, Speed, Robustness\n",
    "#   - Ranks models by weighted final score\n",
    "#\n",
    "# REUSES FROM PREVIOUS OBJECTIVES:\n",
    "#   - Objective 1: System prompt (SYSTEM_PROMPT)\n",
    "#   - Objective 2: Q&A database (qa_database)\n",
    "#   - Objective 3: FAISS index + embed_query() + search_faiss()\n",
    "#   - Objective 4: rag_query() + format_context()\n",
    "#\n",
    "# WHY SCOREPACK:\n",
    "#   - Uses BERTScore (semantic F1) instead of token F1 for better RAG evaluation\n",
    "#   - Token F1 fails on paraphrases: \"30 day return\" vs \"30-day refund\" = low score\n",
    "#   - BERTScore understands semantics: same meaning = high score\n",
    "#\n",
    "# CACHING:\n",
    "#   - First run: ~10-15 min (downloads models, runs inference)\n",
    "#   - Subsequent runs: ~30 sec (loads cached responses, recalculates metrics)\n",
    "#\n",
    "# USAGE:\n",
    "#   rankings_df, detailed_df = run_evaluation()           # Full run\n",
    "#   rankings_df, detailed_df = recalculate_metrics_only() # Recalc metrics only\n",
    "#   clean()                                               # Delete cache\n",
    "#\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Tuple, List, Optional, Callable\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# ============================================================================\n",
    "# VERIFY DEPENDENCIES FROM OBJECTIVES 1-4\n",
    "# ============================================================================\n",
    "def verify_dependencies():\n",
    "    \"\"\"Check that Objectives 1-4 have been run.\"\"\"\n",
    "    missing = []\n",
    "    \n",
    "    # Objective 1: System prompt\n",
    "    if 'SYSTEM_PROMPT' not in globals():\n",
    "        missing.append(\"SYSTEM_PROMPT (Objective 1)\")\n",
    "    \n",
    "    # Objective 2: Q&A database\n",
    "    if 'qa_database' not in globals():\n",
    "        missing.append(\"qa_database (Objective 2)\")\n",
    "    \n",
    "    # Objective 3: Embeddings\n",
    "    if 'embed_query' not in globals():\n",
    "        missing.append(\"embed_query() (Objective 3)\")\n",
    "    if 'search_faiss' not in globals():\n",
    "        missing.append(\"search_faiss() (Objective 3)\")\n",
    "    if 'faiss_index' not in globals():\n",
    "        missing.append(\"faiss_index (Objective 3)\")\n",
    "    \n",
    "    # Objective 4: RAG pipeline\n",
    "    if 'rag_query' not in globals():\n",
    "        missing.append(\"rag_query() (Objective 4)\")\n",
    "    if 'format_context' not in globals():\n",
    "        missing.append(\"format_context() (Objective 4)\")\n",
    "    \n",
    "    if missing:\n",
    "        print(\"‚ùå MISSING DEPENDENCIES - Run previous objectives first:\")\n",
    "        for m in missing:\n",
    "            print(f\"   ‚Ä¢ {m}\")\n",
    "        return False\n",
    "    \n",
    "    print(\"‚úÖ All dependencies from Objectives 1-4 verified\")\n",
    "    return True\n",
    "\n",
    "# ============================================================================\n",
    "# SCOREPACK: UNIFIED SCORING CLASS\n",
    "# ============================================================================\n",
    "# Why inline? Self-contained notebook, no separate file needed.\n",
    "# Why a class? Groups 5 metrics, caches embeddings, batch processing.\n",
    "#\n",
    "# METRICS:\n",
    "#   accuracy   - BERTScore F1 (semantic similarity via DeBERTa)\n",
    "#   quality    - Cosine similarity (same embedder as FAISS from Obj 3)\n",
    "#   confidence - Calibration (rewards correct confidence levels)\n",
    "#   speed      - Normalized latency (0ms=1.0, 2000ms=0.0)\n",
    "#   robustness - Error handling (penalizes crashes, empty, verbose)\n",
    "# ============================================================================\n",
    "\n",
    "try:\n",
    "    from bert_score import score as bert_score\n",
    "    _HAS_BERT = True\n",
    "    print(\"‚úÖ BERTScore available\")\n",
    "except ImportError:\n",
    "    _HAS_BERT = False\n",
    "    print(\"‚ö†Ô∏è BERTScore not installed - run: pip install bert-score\")\n",
    "\n",
    "class ScorePack:\n",
    "    \"\"\"\n",
    "    Unified scoring for RAG model evaluation.\n",
    "    \n",
    "    Usage:\n",
    "        scorer = ScorePack(embeddings=cached_embs, embed_fn=embed_query)\n",
    "        scores = scorer.score_all(pred, ref, conf, latency, is_ans, error)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings: Dict[str, np.ndarray] = None, \n",
    "                 embed_fn: Callable[[str], np.ndarray] = None):\n",
    "        self.embeddings = embeddings or {}\n",
    "        self.embed_fn = embed_fn  # Reuses embed_query from Objective 3\n",
    "        self.device = 0 if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    def _get_emb(self, text: str) -> Optional[np.ndarray]:\n",
    "        \"\"\"Get embedding from cache or compute via embed_query (Obj 3).\"\"\"\n",
    "        if not text or len(text.strip()) < 2:\n",
    "            return None\n",
    "        if text in self.embeddings:\n",
    "            return self.embeddings[text]\n",
    "        if self.embed_fn:\n",
    "            emb = self.embed_fn(text)\n",
    "            self.embeddings[text] = emb\n",
    "            return emb\n",
    "        return None\n",
    "    \n",
    "    # --- ACCURACY: BERTScore (semantic F1) ---\n",
    "    def accuracy(self, pred: str, ref: str) -> float:\n",
    "        \"\"\"BERTScore F1 using DeBERTa-large-mnli.\"\"\"\n",
    "        if not _HAS_BERT or not pred or not ref:\n",
    "            return 0.0\n",
    "        P, R, F1 = bert_score(\n",
    "            [pred], [ref], lang=\"en\",\n",
    "            model_type=\"microsoft/deberta-large-mnli\",\n",
    "            rescale_with_baseline=True,\n",
    "            device=self.device, verbose=False\n",
    "        )\n",
    "        return float(F1[0])\n",
    "    \n",
    "    def accuracy_batch(self, preds: List[str], refs: List[str]) -> List[float]:\n",
    "        \"\"\"Batch BERTScore - faster than loop.\"\"\"\n",
    "        if not _HAS_BERT:\n",
    "            return [0.0] * len(preds)\n",
    "        valid_idx = [i for i, (p, r) in enumerate(zip(preds, refs))\n",
    "                     if p and p.strip() and r and r.strip()]\n",
    "        if not valid_idx:\n",
    "            return [0.0] * len(preds)\n",
    "        P, R, F1 = bert_score(\n",
    "            [preds[i] for i in valid_idx],\n",
    "            [refs[i] for i in valid_idx],\n",
    "            lang=\"en\", model_type=\"microsoft/deberta-large-mnli\",\n",
    "            rescale_with_baseline=True,\n",
    "            device=self.device, verbose=False\n",
    "        )\n",
    "        result = [0.0] * len(preds)\n",
    "        for j, i in enumerate(valid_idx):\n",
    "            result[i] = float(F1[j])\n",
    "        return result\n",
    "    \n",
    "    # --- QUALITY: Embedding similarity (reuses Objective 3 embedder) ---\n",
    "    def quality(self, pred: str, ref: str) -> float:\n",
    "        \"\"\"Cosine similarity using same embedder as FAISS (Obj 3).\"\"\"\n",
    "        if not pred or not ref:\n",
    "            return 0.0\n",
    "        emb_p, emb_r = self._get_emb(pred), self._get_emb(ref)\n",
    "        if emb_p is None or emb_r is None:\n",
    "            return 0.0\n",
    "        return max(0.0, float(np.dot(emb_p, emb_r) / \n",
    "                              (np.linalg.norm(emb_p) * np.linalg.norm(emb_r))))\n",
    "    \n",
    "    # --- CONFIDENCE: Calibration ---\n",
    "    def confidence(self, raw_conf: float, answer: str, is_answerable: bool) -> float:\n",
    "        \"\"\"Rewards well-calibrated confidence.\"\"\"\n",
    "        is_empty = not answer or len(answer.strip()) < 3\n",
    "        conf = raw_conf if raw_conf else 0.5\n",
    "        if is_answerable:\n",
    "            return 0.2 if is_empty else conf\n",
    "        return 1.0 if is_empty else (0.9 if conf < 0.3 else (0.6 if conf < 0.5 else 0.2))\n",
    "    \n",
    "    # --- SPEED: Normalized latency ---\n",
    "    @staticmethod\n",
    "    def speed(latency_ms: float) -> float:\n",
    "        \"\"\"0ms=1.0, 2000ms+=0.0\"\"\"\n",
    "        return max(0.0, min(1.0, 1 - (latency_ms / 2000)))\n",
    "    \n",
    "    # --- ROBUSTNESS: Error handling ---\n",
    "    @staticmethod\n",
    "    def robustness(answer: str, is_answerable: bool, had_error: bool) -> float:\n",
    "        \"\"\"Penalizes errors, empty answers, verbosity.\"\"\"\n",
    "        if had_error:\n",
    "            return 0.0\n",
    "        is_empty = not answer or len(answer.strip()) < 3\n",
    "        length = len(answer.split()) if answer else 0\n",
    "        if is_answerable:\n",
    "            return 0.3 if is_empty else (0.7 if length > 50 else 1.0)\n",
    "        return 1.0 if is_empty else (0.2 if length > 30 else 0.6)\n",
    "    \n",
    "    # --- BATCH SCORING ---\n",
    "    def score_all_batch(self, preds: List[str], refs: List[str],\n",
    "                        confs: List[float], latencies: List[float],\n",
    "                        answerables: List[bool], errors: List[bool]) -> List[Dict[str, float]]:\n",
    "        \"\"\"Batch score all 5 metrics.\"\"\"\n",
    "        batch_preds = [p if a else \"\" for p, a in zip(preds, answerables)]\n",
    "        batch_refs = [r if a else \"\" for r, a in zip(refs, answerables)]\n",
    "        accuracies = self.accuracy_batch(batch_preds, batch_refs)\n",
    "        \n",
    "        results = []\n",
    "        for i in range(len(preds)):\n",
    "            results.append({\n",
    "                'accuracy': accuracies[i] if answerables[i] else 0.0,\n",
    "                'quality': self.quality(preds[i], refs[i]) if answerables[i] else 0.0,\n",
    "                'confidence': self.confidence(confs[i], preds[i], answerables[i]),\n",
    "                'speed': self.speed(latencies[i]),\n",
    "                'robustness': self.robustness(preds[i], answerables[i], errors[i])\n",
    "            })\n",
    "        return results\n",
    "\n",
    "print(\"‚úÖ ScorePack loaded\")\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "OUTPUT_DIR = \"data/model_evaluation\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Cache files\n",
    "CONTEXTS_FILE = os.path.join(OUTPUT_DIR, \"contexts.csv\")\n",
    "EMBEDDINGS_FILE = os.path.join(OUTPUT_DIR, \"embeddings.csv\")\n",
    "RAW_RESPONSES_FILE = os.path.join(OUTPUT_DIR, \"raw_responses.csv\")\n",
    "\n",
    "# Output files\n",
    "RANKINGS_FILE = os.path.join(OUTPUT_DIR, \"model_rankings.csv\")\n",
    "DETAILED_FILE = os.path.join(OUTPUT_DIR, \"model_responses.csv\")\n",
    "SUMMARY_FILE = os.path.join(OUTPUT_DIR, \"evaluation_summary.txt\")\n",
    "\n",
    "print(f\"üìÅ Output: {OUTPUT_DIR}/\")\n",
    "\n",
    "DEVICE = 0 if torch.cuda.is_available() else -1\n",
    "print(f\"üñ•Ô∏è  Device: {'GPU' if DEVICE == 0 else 'CPU'}\")\n",
    "\n",
    "# ============================================================================\n",
    "# MODELS TO EVALUATE (6 models: 5 extractive + 1 generative)\n",
    "# ============================================================================\n",
    "MODELS_CONFIG = [\n",
    "    (\"T5-QA-Generative\", \"consciousAI/question-answering-generative-t5-v1-base-s-q-c\", \"text2text-generation\"),\n",
    "    (\"RoBERTa-SQuAD2\", \"deepset/roberta-base-squad2\", \"question-answering\"),\n",
    "    (\"BERT-Large-SQuAD\", \"google-bert/bert-large-cased-whole-word-masking-finetuned-squad\", \"question-answering\"),\n",
    "    (\"DistilBERT-SQuAD\", \"distilbert-base-uncased-distilled-squad\", \"question-answering\"),\n",
    "    (\"BERT-Tiny-SQuAD\", \"mrm8488/bert-tiny-finetuned-squadv2\", \"question-answering\"),\n",
    "    (\"MiniLM-SQuAD\", \"deepset/minilm-uncased-squad2\", \"question-answering\"),\n",
    "]\n",
    "\n",
    "# ============================================================================\n",
    "# TEST QUESTIONS (5 answerable + 3 unanswerable)\n",
    "# ============================================================================\n",
    "TEST_QUESTIONS = [\n",
    "    # Answerable - should find in knowledge base (Objective 2)\n",
    "    (\"What is your return policy?\", \"30-day return policy with full refund\", True),\n",
    "    (\"How long does shipping take?\", \"Standard shipping takes 5-7 business days\", True),\n",
    "    (\"What are your customer service hours?\", \"Monday to Friday 9am to 5pm\", True),\n",
    "    (\"Do you offer warranty on products?\", \"1 year warranty on all electronics\", True),\n",
    "    (\"How can I track my order?\", \"Use tracking number on our website\", True),\n",
    "    # Unanswerable - not in knowledge base\n",
    "    (\"What is the CEO's phone number?\", \"\", False),\n",
    "    (\"Will you have a Black Friday sale?\", \"\", False),\n",
    "    (\"How do prices compare to Amazon?\", \"\", False),\n",
    "]\n",
    "\n",
    "# ============================================================================\n",
    "# METRIC WEIGHTS (total = 100%)\n",
    "# ============================================================================\n",
    "METRIC_WEIGHTS = {\n",
    "    'accuracy': 0.25,    # Semantic correctness (BERTScore)\n",
    "    'quality': 0.25,     # Embedding similarity (reuses Obj 3)\n",
    "    'confidence': 0.20,  # Calibration\n",
    "    'speed': 0.15,       # Latency\n",
    "    'robustness': 0.15   # Error handling\n",
    "}\n",
    "\n",
    "MODEL_SIZES = {\n",
    "    'BERT-Large-SQuAD': ('Large', '340M'),\n",
    "    'RoBERTa-SQuAD2': ('Base', '125M'),\n",
    "    'T5-QA-Generative': ('Base', '220M'),\n",
    "    'DistilBERT-SQuAD': ('Small', '66M'),\n",
    "    'MiniLM-SQuAD': ('Small', '33M'),\n",
    "    'BERT-Tiny-SQuAD': ('Tiny', '4M'),\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# CACHE FUNCTIONS (3-stage caching)\n",
    "# ============================================================================\n",
    "def show_cache_status():\n",
    "    \"\"\"Display cache status.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìÅ CACHE STATUS\")\n",
    "    print(\"=\"*60)\n",
    "    for name, path in [('contexts.csv', CONTEXTS_FILE), \n",
    "                       ('embeddings.csv', EMBEDDINGS_FILE),\n",
    "                       ('raw_responses.csv', RAW_RESPONSES_FILE)]:\n",
    "        print(f\"   {'‚úÖ' if os.path.exists(path) else '‚ùå'} {name}\")\n",
    "\n",
    "\n",
    "def load_or_fetch_contexts(force_refresh: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"Fetch contexts using rag_query from Objective 4.\"\"\"\n",
    "    if not force_refresh and os.path.exists(CONTEXTS_FILE):\n",
    "        print(\"\\nüìÇ Loading contexts from cache...\")\n",
    "        return pd.read_csv(CONTEXTS_FILE)\n",
    "    \n",
    "    print(\"\\nüì• Fetching contexts via rag_query (Objective 4)...\")\n",
    "    \n",
    "    data = []\n",
    "    for i, (q, expected, is_ans) in enumerate(TEST_QUESTIONS):\n",
    "        # REUSE: rag_query from Objective 4\n",
    "        result = rag_query(q, verbose=False)\n",
    "        context = \" \".join([f\"{c.get('question','')} {c.get('answer','')}\" \n",
    "                           for c in result.retrieved_context]) if result.success else \"\"\n",
    "        data.append({\n",
    "            'question': q, \n",
    "            'expected': expected, \n",
    "            'is_answerable': is_ans,\n",
    "            'context': context or \"No relevant information found.\",\n",
    "            'num_sources': len(result.retrieved_context) if result.success else 0\n",
    "        })\n",
    "        print(f\"   [{i+1}/{len(TEST_QUESTIONS)}] ‚úÖ {q[:40]}...\")\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(CONTEXTS_FILE, index=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_or_compute_embeddings(contexts_df: pd.DataFrame, force_refresh: bool = False) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"Compute embeddings using embed_query from Objective 3.\"\"\"\n",
    "    if not force_refresh and os.path.exists(EMBEDDINGS_FILE):\n",
    "        print(\"\\nüìÇ Loading embeddings from cache...\")\n",
    "        df = pd.read_csv(EMBEDDINGS_FILE)\n",
    "        return {row['text']: np.array(json.loads(row['embedding'])) for _, row in df.iterrows()}\n",
    "    \n",
    "    print(\"\\nüì• Computing embeddings via embed_query (Objective 3)...\")\n",
    "    \n",
    "    texts = list(set([row['expected'] for _, row in contexts_df.iterrows() \n",
    "                      if row['is_answerable'] and row['expected']]))\n",
    "    \n",
    "    embeddings = {}\n",
    "    data = []\n",
    "    for i, text in enumerate(texts):\n",
    "        # REUSE: embed_query from Objective 3\n",
    "        emb = embed_query(text)\n",
    "        embeddings[text] = emb\n",
    "        data.append({'text': text, 'embedding': json.dumps(emb.tolist())})\n",
    "        print(f\"   [{i+1}/{len(texts)}] ‚úÖ {text[:40]}...\")\n",
    "    \n",
    "    pd.DataFrame(data).to_csv(EMBEDDINGS_FILE, index=False)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def load_or_collect_responses(contexts_df: pd.DataFrame, force_refresh: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"Run all 6 models on test questions.\"\"\"\n",
    "    if not force_refresh and os.path.exists(RAW_RESPONSES_FILE):\n",
    "        print(\"\\nüìÇ Loading responses from cache...\")\n",
    "        return pd.read_csv(RAW_RESPONSES_FILE)\n",
    "    \n",
    "    print(\"\\nü§ñ Running model inference (6 models √ó 8 questions)...\")\n",
    "    all_responses = []\n",
    "    \n",
    "    for idx, (name, model_id, task_type) in enumerate(MODELS_CONFIG):\n",
    "        print(f\"\\n[{idx+1}/{len(MODELS_CONFIG)}] üìä {name}\")\n",
    "        \n",
    "        try:\n",
    "            pipe = pipeline(task_type, model=model_id, device=DEVICE,\n",
    "                          torch_dtype=torch.float16 if DEVICE == 0 else torch.float32)\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Failed to load: {str(e)[:50]}\")\n",
    "            for _, row in contexts_df.iterrows():\n",
    "                all_responses.append({'model': name, 'question': row['question'],\n",
    "                                     'answer': '', 'raw_confidence': 0.0,\n",
    "                                     'response_time_ms': 0.0, 'had_error': True})\n",
    "            continue\n",
    "        \n",
    "        for _, row in contexts_df.iterrows():\n",
    "            t0 = time.time()\n",
    "            try:\n",
    "                if task_type == \"text2text-generation\":\n",
    "                    out = pipe(f\"question: {row['question']} context: {row['context']}\", max_length=50)\n",
    "                    answer, raw_conf = out[0]['generated_text'].strip(), 0.7\n",
    "                else:\n",
    "                    out = pipe(question=row['question'], context=row['context'], max_answer_len=50)\n",
    "                    answer, raw_conf = out['answer'].strip(), out.get('score', 0.5)\n",
    "                had_error = False\n",
    "            except:\n",
    "                answer, raw_conf, had_error = '', 0.0, True\n",
    "            \n",
    "            all_responses.append({\n",
    "                'model': name, 'question': row['question'], 'answer': answer,\n",
    "                'raw_confidence': raw_conf, 'response_time_ms': (time.time() - t0) * 1000,\n",
    "                'had_error': had_error\n",
    "            })\n",
    "            print(f\"   ‚úÖ {row['question'][:30]}... ‚Üí {answer[:25] if answer else '(empty)'}...\")\n",
    "        \n",
    "        del pipe\n",
    "        if DEVICE == 0: torch.cuda.empty_cache()\n",
    "    \n",
    "    df = pd.DataFrame(all_responses)\n",
    "    df.to_csv(RAW_RESPONSES_FILE, index=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# METRIC CALCULATION\n",
    "# ============================================================================\n",
    "def calculate_metrics(contexts_df: pd.DataFrame, responses_df: pd.DataFrame,\n",
    "                      embeddings: Dict[str, np.ndarray]) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Calculate all 5 metrics using ScorePack.\"\"\"\n",
    "    print(\"\\nüìä Calculating metrics (ScorePack + BERTScore)...\")\n",
    "    \n",
    "    # REUSE: embed_query from Objective 3\n",
    "    scorer = ScorePack(embeddings=embeddings, embed_fn=embed_query)\n",
    "    \n",
    "    context_lookup = {row['question']: {'expected': row['expected'], 'is_answerable': row['is_answerable']}\n",
    "                      for _, row in contexts_df.iterrows()}\n",
    "    \n",
    "    # Prepare batch data\n",
    "    preds, refs, confs, latencies, answerables, errors = [], [], [], [], [], []\n",
    "    for _, resp in responses_df.iterrows():\n",
    "        ctx = context_lookup[resp['question']]\n",
    "        preds.append(resp['answer'] if resp['answer'] else \"\")\n",
    "        refs.append(ctx['expected'] if ctx['expected'] else \"\")\n",
    "        confs.append(resp['raw_confidence'])\n",
    "        latencies.append(resp['response_time_ms'])\n",
    "        answerables.append(ctx['is_answerable'])\n",
    "        errors.append(resp['had_error'])\n",
    "    \n",
    "    # Batch scoring\n",
    "    print(\"   üîÑ Running BERTScore (DeBERTa-large-mnli)...\")\n",
    "    all_scores = scorer.score_all_batch(preds, refs, confs, latencies, answerables, errors)\n",
    "    print(f\"   ‚úÖ Scored {len(all_scores)} responses\")\n",
    "    \n",
    "    # Build detailed results\n",
    "    detailed_results = []\n",
    "    for i, (_, resp) in enumerate(responses_df.iterrows()):\n",
    "        ctx = context_lookup[resp['question']]\n",
    "        scores = all_scores[i]\n",
    "        detailed_results.append({\n",
    "            'model': resp['model'],\n",
    "            'question': resp['question'],\n",
    "            'answer': resp['answer'],\n",
    "            'expected': ctx['expected'],\n",
    "            'is_answerable': ctx['is_answerable'],\n",
    "            'raw_confidence': resp['raw_confidence'],\n",
    "            'response_time_ms': resp['response_time_ms'],\n",
    "            **{k: round(v, 3) for k, v in scores.items()}\n",
    "        })\n",
    "    detailed_df = pd.DataFrame(detailed_results)\n",
    "    \n",
    "    # Aggregate by model\n",
    "    print(\"   üîÑ Aggregating by model...\")\n",
    "    model_results = []\n",
    "    for name, _, _ in MODELS_CONFIG:\n",
    "        model_data = detailed_df[detailed_df['model'] == name]\n",
    "        answerable = model_data[model_data['is_answerable'] == True]\n",
    "        \n",
    "        metrics = {\n",
    "            'Accuracy': answerable['accuracy'].mean() if len(answerable) > 0 else 0,\n",
    "            'Quality': answerable['quality'].mean() if len(answerable) > 0 else 0,\n",
    "            'Confidence': model_data['confidence'].mean(),\n",
    "            'Speed': model_data['speed'].mean(),\n",
    "            'Robustness': model_data['robustness'].mean()\n",
    "        }\n",
    "        \n",
    "        final = sum(metrics[m.capitalize()] * w for m, w in METRIC_WEIGHTS.items())\n",
    "        model_results.append({'Model': name, **{k: round(v, 3) for k, v in metrics.items()},\n",
    "                             'Final_Score': round(final, 3)})\n",
    "        print(f\"   ‚úÖ {name}: {final:.3f}\")\n",
    "    \n",
    "    rankings_df = pd.DataFrame(model_results)\n",
    "    rankings_df = rankings_df.sort_values('Final_Score', ascending=False).reset_index(drop=True)\n",
    "    rankings_df.insert(0, 'Rank', range(1, len(rankings_df) + 1))\n",
    "    \n",
    "    return rankings_df, detailed_df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# OUTPUT\n",
    "# ============================================================================\n",
    "def save_outputs(rankings_df: pd.DataFrame, detailed_df: pd.DataFrame):\n",
    "    \"\"\"Save results to CSV and text summary.\"\"\"\n",
    "    rankings_df.to_csv(RANKINGS_FILE, index=False)\n",
    "    detailed_df.to_csv(DETAILED_FILE, index=False)\n",
    "    \n",
    "    with open(SUMMARY_FILE, 'w') as f:\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        f.write(\"OBJECTIVE 5: MODEL EVALUATION SUMMARY\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\\n\")\n",
    "        f.write(f\"Date: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Scoring: ScorePack (BERTScore DeBERTa-large-mnli)\\n\\n\")\n",
    "        f.write(\"REUSES FROM PREVIOUS OBJECTIVES:\\n\")\n",
    "        f.write(\"  ‚Ä¢ Objective 3: embed_query() for quality metric\\n\")\n",
    "        f.write(\"  ‚Ä¢ Objective 4: rag_query() for context retrieval\\n\\n\")\n",
    "        f.write(\"METRIC WEIGHTS:\\n\")\n",
    "        for m, w in METRIC_WEIGHTS.items():\n",
    "            f.write(f\"  {m}: {w*100:.0f}%\\n\")\n",
    "        f.write(\"\\n\" + \"=\"*60 + \"\\nRANKINGS:\\n\" + \"=\"*60 + \"\\n\\n\")\n",
    "        f.write(rankings_df.to_string(index=False))\n",
    "        f.write(f\"\\n\\nü•á WINNER: {rankings_df.iloc[0]['Model']} ({rankings_df.iloc[0]['Final_Score']})\\n\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Saved: {RANKINGS_FILE}\")\n",
    "    print(f\"‚úÖ Saved: {DETAILED_FILE}\")\n",
    "    print(f\"‚úÖ Saved: {SUMMARY_FILE}\")\n",
    "\n",
    "\n",
    "def print_analysis(rankings_df: pd.DataFrame):\n",
    "    \"\"\"Print winner summary and model insights.\"\"\"\n",
    "    winner = rankings_df.iloc[0]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üèÜ WINNER SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\"\"\n",
    "    ü•á BEST MODEL: {winner['Model']}\n",
    "    \n",
    "    Final Score: {winner['Final_Score']:.3f}\n",
    "    \n",
    "    SCORES:\n",
    "    ‚îú‚îÄ‚îÄ Accuracy (BERTScore): {winner['Accuracy']:.3f}\n",
    "    ‚îú‚îÄ‚îÄ Quality (Embedding):  {winner['Quality']:.3f}\n",
    "    ‚îú‚îÄ‚îÄ Confidence:           {winner['Confidence']:.3f}\n",
    "    ‚îú‚îÄ‚îÄ Speed:                {winner['Speed']:.3f}\n",
    "    ‚îî‚îÄ‚îÄ Robustness:           {winner['Robustness']:.3f}\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"üîç MODEL INSIGHTS: SIZE vs PERFORMANCE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\n   {'Model':<20} {'Size':<8} {'Params':<10} {'Score':<8}\")\n",
    "    print(\"   \" + \"‚îÄ\"*46)\n",
    "    for _, row in rankings_df.iterrows():\n",
    "        size_info = MODEL_SIZES.get(row['Model'], ('?', '?'))\n",
    "        print(f\"   {row['Model']:<20} {size_info[0]:<8} {size_info[1]:<10} {row['Final_Score']:.3f}\")\n",
    "    \n",
    "    print(f\"\\n   RECOMMENDATIONS:\")\n",
    "    print(f\"   ‚Ä¢ Speed-critical:    {rankings_df.loc[rankings_df['Speed'].idxmax(), 'Model']}\")\n",
    "    print(f\"   ‚Ä¢ Accuracy-critical: {rankings_df.loc[rankings_df['Accuracy'].idxmax(), 'Model']}\")\n",
    "    print(f\"   ‚Ä¢ Best overall:      {rankings_df.iloc[0]['Model']}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN ENTRY POINTS\n",
    "# ============================================================================\n",
    "def run_evaluation(force_refresh: bool = False) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Run full evaluation pipeline.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üöÄ OBJECTIVE 5: MODEL EVALUATION\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"   Reusing: embed_query (Obj 3), rag_query (Obj 4)\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    cache_complete = all(os.path.exists(f) for f in [CONTEXTS_FILE, EMBEDDINGS_FILE, RAW_RESPONSES_FILE])\n",
    "    \n",
    "    if cache_complete and not force_refresh:\n",
    "        print(\"\\nüí° Loading from cache (use force_refresh=True to rebuild)...\")\n",
    "        contexts_df = pd.read_csv(CONTEXTS_FILE)\n",
    "        responses_df = pd.read_csv(RAW_RESPONSES_FILE)\n",
    "        emb_df = pd.read_csv(EMBEDDINGS_FILE)\n",
    "        embeddings = {row['text']: np.array(json.loads(row['embedding'])) for _, row in emb_df.iterrows()}\n",
    "    else:\n",
    "        contexts_df = load_or_fetch_contexts(force_refresh)\n",
    "        embeddings = load_or_compute_embeddings(contexts_df, force_refresh)\n",
    "        responses_df = load_or_collect_responses(contexts_df, force_refresh)\n",
    "    \n",
    "    rankings_df, detailed_df = calculate_metrics(contexts_df, responses_df, embeddings)\n",
    "    save_outputs(rankings_df, detailed_df)\n",
    "    \n",
    "    print(f\"\\n‚è±Ô∏è  Total time: {time.time() - start_time:.1f}s\")\n",
    "    return rankings_df, detailed_df\n",
    "\n",
    "\n",
    "def recalculate_metrics_only() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Recalculate metrics from cached responses (~30 sec).\"\"\"\n",
    "    print(\"\\nüìä Recalculating metrics from cache...\")\n",
    "    contexts_df = pd.read_csv(CONTEXTS_FILE)\n",
    "    responses_df = pd.read_csv(RAW_RESPONSES_FILE)\n",
    "    emb_df = pd.read_csv(EMBEDDINGS_FILE)\n",
    "    embeddings = {row['text']: np.array(json.loads(row['embedding'])) for _, row in emb_df.iterrows()}\n",
    "    rankings_df, detailed_df = calculate_metrics(contexts_df, responses_df, embeddings)\n",
    "    save_outputs(rankings_df, detailed_df)\n",
    "    return rankings_df, detailed_df\n",
    "\n",
    "\n",
    "def clean():\n",
    "    \"\"\"Delete all cache files.\"\"\"\n",
    "    print(\"\\nüóëÔ∏è  Cleaning cache...\")\n",
    "    for f in [CONTEXTS_FILE, EMBEDDINGS_FILE, RAW_RESPONSES_FILE, RANKINGS_FILE, DETAILED_FILE, SUMMARY_FILE]:\n",
    "        if os.path.exists(f):\n",
    "            os.remove(f)\n",
    "            print(f\"   Deleted: {os.path.basename(f)}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# RUN EVALUATION\n",
    "# ============================================================================\n",
    "# Verify Objectives 1-4 are available\n",
    "if verify_dependencies():\n",
    "    show_cache_status()\n",
    "    rankings_df, detailed_df = run_evaluation()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üèÜ FINAL RANKINGS\")\n",
    "    print(\"=\"*60)\n",
    "    print(rankings_df.to_string(index=False))\n",
    "    print(f\"\\nü•á WINNER: {rankings_df.iloc[0]['Model']} ({rankings_df.iloc[0]['Final_Score']})\")\n",
    "    \n",
    "    print_analysis(rankings_df)\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Please run Objectives 1-4 first, then re-run this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective 6: System Analysis & Reflection\n",
    "\n",
    "### üéØ Goal\n",
    "\n",
    "**Part 1 (Code):** Generate and display comprehensive analysis data from Objectives 1-5.  \n",
    "**Part 2 (Manual):** My reflection on system strengths, weaknesses, real-world applications, and critical insights based on the generated results.\n",
    "\n",
    "### üìã Structure\n",
    "\n",
    "This objective has **two parts**:\n",
    "\n",
    "1. **Code Cell (Below):** Runs analysis and displays key metrics, model rankings, and performance data\n",
    "2. **Markdown Cell (Next):** You manually write your reflection and critical analysis based on the results\n",
    "\n",
    "**Why this approach?**\n",
    "- Demonstrates you understand the results (not just running code)\n",
    "- Shows critical thinking and analysis skills\n",
    "- More authentic for assignment submission\n",
    "- Allows you to interpret data and provide insights\n",
    "\n",
    "<details>\n",
    "<summary><b>üì• Prerequisites</b> (Click to expand)</summary>\n",
    "\n",
    "| Item | Source | Required | Description |\n",
    "|------|--------|----------|-------------|\n",
    "| `rankings_df` | Objective 5 | ‚úÖ Yes | Model rankings with all metrics |\n",
    "| `detailed_df` | Objective 5 | ‚úÖ Yes | Per-question detailed results |\n",
    "| `qa_database` | Objective 2 | ‚úÖ Yes | Knowledge base for coverage analysis |\n",
    "| `faiss_index` | Objective 3 | ‚úÖ Yes | Retrieval system analysis |\n",
    "| `rag_query()` | Objective 4 | ‚úÖ Yes | RAG pipeline for analysis |\n",
    "\n",
    "**Note:** Requires Objectives 1-5 completed. Achieves **100% component reuse**.\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>üìä Analysis Framework</b> (Click to expand)</summary>\n",
    "\n",
    "### 1. System Performance Analysis\n",
    "**What it analyzes:** Overall system metrics from Objective 5 results\n",
    "\n",
    "**Metrics Computed:**\n",
    "- **Best Model Performance** - Winner from rankings with all 5 metrics\n",
    "- **Answerable Accuracy** - Average accuracy on answerable questions across models\n",
    "- **Unanswerable Detection** - How well models handle out-of-scope questions\n",
    "- **Average Response Time** - Mean latency across all models and questions\n",
    "- **Confidence Calibration** - Correlation between confidence and correctness\n",
    "\n",
    "**Source:** `rankings_df` and `detailed_df` from Objective 5\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Knowledge Base Analysis\n",
    "**What it analyzes:** Coverage and quality of the Q&A database\n",
    "\n",
    "**Metrics Computed:**\n",
    "- **Total Q&A Pairs** - Size of knowledge base\n",
    "- **Category Coverage** - Number of distinct categories\n",
    "- **Answerable vs Unanswerable** - Distribution in database\n",
    "- **Average Answer Length** - Typical response size\n",
    "\n",
    "**Source:** `qa_database` from Objective 2\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Retrieval System Analysis\n",
    "**What it analyzes:** FAISS vector database performance\n",
    "\n",
    "**Metrics Computed:**\n",
    "- **Index Size** - Number of vectors indexed\n",
    "- **Embedding Dimension** - Vector dimensionality (384 for all-MiniLM-L6-v2)\n",
    "- **Index Type** - FAISS index configuration (IndexFlatL2)\n",
    "\n",
    "**Source:** `faiss_index` from Objective 3\n",
    "\n",
    "---\n",
    "\n",
    "### 4. System Limitations\n",
    "**What it identifies:** What the system can and cannot handle\n",
    "\n",
    "**Categories:**\n",
    "- **Answerable Questions** - What the system handles well\n",
    "- **Unanswerable Questions** - What the system should decline\n",
    "- **Failure Modes** - Common error patterns (incomplete answers, false confidence, execution errors)\n",
    "- **Edge Cases** - Ambiguous queries, typos, multi-topic questions\n",
    "\n",
    "**Source:** Analysis of `detailed_df` from Objective 5\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Real-World Applications\n",
    "**What it evaluates:** Suitable deployment scenarios\n",
    "\n",
    "**Use Cases:**\n",
    "- Customer service chatbot (24/7 support)\n",
    "- Internal FAQ system (employee self-service)\n",
    "- Knowledge base assistant (documentation search)\n",
    "- Help desk automation (ticket deflection)\n",
    "\n",
    "**Business Value:**\n",
    "- Reduced response time\n",
    "- Cost savings\n",
    "- Consistency\n",
    "- Scalability\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Scalability Analysis\n",
    "**What it projects:** Performance at different scales\n",
    "\n",
    "**Scales Analyzed:**\n",
    "- **Current:** 21 Q&A pairs\n",
    "- **Small Scale:** 1,000 pairs\n",
    "- **Medium Scale:** 10,000 pairs\n",
    "- **Large Scale:** 100,000+ pairs\n",
    "\n",
    "**Bottlenecks Identified:**\n",
    "- Embedding generation time\n",
    "- FAISS search latency\n",
    "- Model inference time\n",
    "- Memory requirements\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Deployment Considerations\n",
    "**What it evaluates:** Production deployment requirements\n",
    "\n",
    "**Infrastructure:**\n",
    "- GPU vs CPU trade-offs\n",
    "- Model hosting options\n",
    "- Vector database scaling\n",
    "\n",
    "**Costs:**\n",
    "- Model hosting costs\n",
    "- API call expenses\n",
    "- Infrastructure requirements\n",
    "\n",
    "**Maintenance:**\n",
    "- Knowledge base updates\n",
    "- Model versioning\n",
    "- Performance monitoring\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>üîó Component Reuse</b> (Click to expand)</summary>\n",
    "\n",
    "**100% Reuse from Previous Objectives:**\n",
    "\n",
    "| Component | From | Used For |\n",
    "|-----------|------|----------|\n",
    "| `rankings_df` | Obj 5 | Model performance metrics and rankings |\n",
    "| `detailed_df` | Obj 5 | Per-question analysis, failure mode identification |\n",
    "| `qa_database` | Obj 2 | Knowledge base coverage and quality analysis |\n",
    "| `faiss_index` | Obj 3 | Retrieval system metrics |\n",
    "| `rag_query()` | Obj 4 | Pipeline performance analysis |\n",
    "\n",
    "**Key Insight:** Objective 6 synthesizes all previous objectives into actionable insights and recommendations.\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>üì§ Outputs</b> (Click to expand)</summary>\n",
    "\n",
    "**Files Created:**\n",
    "- `metrics_summary.csv` - Quantitative metrics summary\n",
    "\n",
    "**Global Variables:**\n",
    "- `system_analysis` - Dictionary containing all analysis results\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>‚úÖ Verification</b> (Click to expand)</summary>\n",
    "\n",
    "**Verification:**\n",
    "- ‚úÖ `rankings_df` and `detailed_df` from Objective 5 exist\n",
    "- ‚úÖ `qa_database` from Objective 2 exists\n",
    "- ‚úÖ `faiss_index` from Objective 3 exists\n",
    "- ‚úÖ `metrics_summary.csv` with quantitative metrics\n",
    "\n",
    "**Performance:**\n",
    "- **Analysis time:** ~5-10 seconds (reads data from previous objectives)\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "**Next Step:** Review the generated analysis data, then write your reflection in the markdown cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìä OBJECTIVE 6: SYSTEM ANALYSIS & REFLECTION\n",
      "============================================================\n",
      "‚úÖ All dependencies verified\n",
      "\n",
      "------------------------------------------------------------\n",
      "üèÜ MODEL RANKINGS\n",
      "------------------------------------------------------------\n",
      " Rank            Model  Accuracy  Quality  Speed  Final_Score\n",
      "    1     MiniLM-SQuAD     0.328    0.609  0.988        0.598\n",
      "    2 DistilBERT-SQuAD     0.297    0.550  0.988        0.571\n",
      "    3   RoBERTa-SQuAD2     0.366    0.477  0.976        0.568\n",
      "    4 BERT-Large-SQuAD     0.333    0.546  0.927        0.562\n",
      "    5  BERT-Tiny-SQuAD     0.018    0.459  0.998        0.465\n",
      "    6 T5-QA-Generative     0.178    0.438  0.311        0.430\n",
      "\n",
      "ü•á Best Model: MiniLM-SQuAD (Score: 0.598)\n",
      "‚ö° Fastest Model: BERT-Tiny-SQuAD (0.003s)\n",
      "\n",
      "------------------------------------------------------------\n",
      "üìà PERFORMANCE SUMMARY\n",
      "------------------------------------------------------------\n",
      "  ‚Ä¢ Avg Accuracy:   25.3%\n",
      "  ‚Ä¢ Avg Quality:    51.3%\n",
      "  ‚Ä¢ Avg Confidence: 41.8%\n",
      "  ‚Ä¢ Avg Speed:      86.5%\n",
      "  ‚Ä¢ Avg Robustness: 85.0%\n",
      "  ‚Ä¢ Avg Response:   0.274s\n",
      "\n",
      "------------------------------------------------------------\n",
      "üìä ANSWERABLE vs UNANSWERABLE PERFORMANCE\n",
      "------------------------------------------------------------\n",
      "\n",
      "  Answerable Questions (30 samples):\n",
      "    ‚Ä¢ Avg Accuracy:   25.3%\n",
      "    ‚Ä¢ Avg Quality:    51.3%\n",
      "    ‚Ä¢ Avg Confidence: 21.8%\n",
      "\n",
      "  Unanswerable Questions (18 samples):\n",
      "    ‚Ä¢ Avg Robustness: 60.0%\n",
      "    ‚Ä¢ Avg Confidence: 75.0%\n",
      "    ‚Ä¢ False Confidence (>0.7): 0/18\n",
      "\n",
      "------------------------------------------------------------\n",
      "üìö KNOWLEDGE BASE\n",
      "------------------------------------------------------------\n",
      "  ‚Ä¢ Total Q&A Pairs: 21\n",
      "  ‚Ä¢ Categories: 9\n",
      "  ‚Ä¢ FAISS Vectors: 21\n",
      "  ‚Ä¢ Embedding Dim: 384\n",
      "\n",
      "------------------------------------------------------------\n",
      "üí° KEY INSIGHTS\n",
      "------------------------------------------------------------\n",
      "  ‚ö†Ô∏è Low accuracy (25.3%) - models struggle with matching\n",
      "  ‚úÖ Good unanswerable detection (100.0%)\n",
      "  ‚úÖ Fast response time (0.27s)\n",
      "  üìä Model score range: 0.168\n",
      "\n",
      "üíæ Saved: data/system_analysis/analysis_summary.csv\n",
      "\n",
      "============================================================\n",
      "üìù REFLECTION PROMPTS\n",
      "============================================================\n",
      "\n",
      "Write your reflection addressing these questions:\n",
      "\n",
      "1. STRENGTHS: What does this RAG system do well?\n",
      "\n",
      "2. WEAKNESSES: What are the main limitations?\n",
      "\n",
      "3. MODEL COMPARISON: Why did the best model outperform others?\n",
      "\n",
      "4. REAL-WORLD USE: Where could this system be deployed?\n",
      "\n",
      "5. IMPROVEMENTS: What would you change to make it better?\n",
      "\n",
      "============================================================\n",
      "‚úÖ OBJECTIVE 6 COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# OBJECTIVE 6: SYSTEM ANALYSIS & REFLECTION (SIMPLIFIED)\n",
    "# ============================================================================\n",
    "#\n",
    "# PURPOSE: Analyze results from Objective 5 and provide insights for reflection\n",
    "#\n",
    "# PREREQUISITES: Run Objectives 1-5 first\n",
    "#   - qa_database (Objective 2)\n",
    "#   - faiss_index (Objective 3)\n",
    "#   - rankings_df, detailed_df (Objective 5)\n",
    "#\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = \"data/system_analysis\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 1: VERIFY DEPENDENCIES\n",
    "# ============================================================================\n",
    "\n",
    "def verify_dependencies():\n",
    "    \"\"\"Check that Objectives 1-5 have been run.\"\"\"\n",
    "    missing = []\n",
    "    \n",
    "    if 'qa_database' not in globals():\n",
    "        missing.append(\"qa_database (Objective 2)\")\n",
    "    if 'faiss_index' not in globals():\n",
    "        missing.append(\"faiss_index (Objective 3)\")\n",
    "    if 'rankings_df' not in globals():\n",
    "        missing.append(\"rankings_df (Objective 5)\")\n",
    "    if 'detailed_df' not in globals():\n",
    "        missing.append(\"detailed_df (Objective 5)\")\n",
    "    \n",
    "    if missing:\n",
    "        print(\"‚ùå MISSING DEPENDENCIES:\")\n",
    "        for m in missing:\n",
    "            print(f\"   ‚Ä¢ {m}\")\n",
    "        return False\n",
    "    \n",
    "    print(\"‚úÖ All dependencies verified\")\n",
    "    return True\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 2: RUN ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä OBJECTIVE 6: SYSTEM ANALYSIS & REFLECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if not verify_dependencies():\n",
    "    print(\"\\n‚ö†Ô∏è Run Objectives 1-5 first, then re-run this cell.\")\n",
    "else:\n",
    "    # Get data from previous objectives\n",
    "    qa_database = globals()['qa_database']\n",
    "    faiss_index = globals()['faiss_index']\n",
    "    rankings_df = globals()['rankings_df']\n",
    "    detailed_df = globals()['detailed_df']\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # 1. MODEL RANKINGS\n",
    "    # ------------------------------------------------------------------\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"üèÜ MODEL RANKINGS\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    display_cols = ['Rank', 'Model', 'Accuracy', 'Quality', 'Speed', 'Final_Score']\n",
    "    available_cols = [c for c in display_cols if c in rankings_df.columns]\n",
    "    print(rankings_df[available_cols].to_string(index=False))\n",
    "    \n",
    "    best = rankings_df.iloc[0]\n",
    "    print(f\"\\nü•á Best Model: {best['Model']} (Score: {best['Final_Score']:.3f})\")\n",
    "    \n",
    "    # Fastest model\n",
    "    fastest_idx = detailed_df['response_time_ms'].idxmin()\n",
    "    fastest = detailed_df.loc[fastest_idx]\n",
    "    print(f\"‚ö° Fastest Model: {fastest['model']} ({fastest['response_time_ms']/1000:.3f}s)\")\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # 2. PERFORMANCE SUMMARY\n",
    "    # ------------------------------------------------------------------\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"üìà PERFORMANCE SUMMARY\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    print(f\"  ‚Ä¢ Avg Accuracy:   {rankings_df['Accuracy'].mean():.1%}\")\n",
    "    print(f\"  ‚Ä¢ Avg Quality:    {rankings_df['Quality'].mean():.1%}\")\n",
    "    print(f\"  ‚Ä¢ Avg Confidence: {rankings_df['Confidence'].mean():.1%}\")\n",
    "    print(f\"  ‚Ä¢ Avg Speed:      {rankings_df['Speed'].mean():.1%}\")\n",
    "    print(f\"  ‚Ä¢ Avg Robustness: {rankings_df['Robustness'].mean():.1%}\")\n",
    "    print(f\"  ‚Ä¢ Avg Response:   {detailed_df['response_time_ms'].mean()/1000:.3f}s\")\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # 3. ANSWERABLE vs UNANSWERABLE\n",
    "    # ------------------------------------------------------------------\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"üìä ANSWERABLE vs UNANSWERABLE PERFORMANCE\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    ans_df = detailed_df[detailed_df['is_answerable'] == True]\n",
    "    unans_df = detailed_df[detailed_df['is_answerable'] == False]\n",
    "    \n",
    "    print(f\"\\n  Answerable Questions ({len(ans_df)} samples):\")\n",
    "    print(f\"    ‚Ä¢ Avg Accuracy:   {ans_df['accuracy'].mean():.1%}\")\n",
    "    print(f\"    ‚Ä¢ Avg Quality:    {ans_df['quality'].mean():.1%}\")\n",
    "    print(f\"    ‚Ä¢ Avg Confidence: {ans_df['confidence'].mean():.1%}\")\n",
    "    \n",
    "    print(f\"\\n  Unanswerable Questions ({len(unans_df)} samples):\")\n",
    "    print(f\"    ‚Ä¢ Avg Robustness: {unans_df['robustness'].mean():.1%}\")\n",
    "    print(f\"    ‚Ä¢ Avg Confidence: {unans_df['confidence'].mean():.1%}\")\n",
    "    \n",
    "    # False confidence detection\n",
    "    if 'raw_confidence' in unans_df.columns:\n",
    "        high_conf = (unans_df['raw_confidence'] > 0.7).sum()\n",
    "        print(f\"    ‚Ä¢ False Confidence (>0.7): {high_conf}/{len(unans_df)}\")\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # 4. KNOWLEDGE BASE STATS\n",
    "    # ------------------------------------------------------------------\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"üìö KNOWLEDGE BASE\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    total_pairs = len(qa_database)\n",
    "    categories = {}\n",
    "    for qa in qa_database:\n",
    "        cat = qa.get('category', 'unknown')\n",
    "        categories[cat] = categories.get(cat, 0) + 1\n",
    "    \n",
    "    print(f\"  ‚Ä¢ Total Q&A Pairs: {total_pairs}\")\n",
    "    print(f\"  ‚Ä¢ Categories: {len(categories)}\")\n",
    "    print(f\"  ‚Ä¢ FAISS Vectors: {faiss_index.ntotal}\")\n",
    "    print(f\"  ‚Ä¢ Embedding Dim: {faiss_index.d}\")\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # 5. KEY INSIGHTS\n",
    "    # ------------------------------------------------------------------\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"üí° KEY INSIGHTS\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Accuracy insight\n",
    "    avg_acc = rankings_df['Accuracy'].mean()\n",
    "    if avg_acc < 0.3:\n",
    "        print(f\"  ‚ö†Ô∏è Low accuracy ({avg_acc:.1%}) - models struggle with matching\")\n",
    "    elif avg_acc < 0.6:\n",
    "        print(f\"  üìä Moderate accuracy ({avg_acc:.1%}) - room for improvement\")\n",
    "    else:\n",
    "        print(f\"  ‚úÖ Good accuracy ({avg_acc:.1%}) - models perform well\")\n",
    "    \n",
    "    # Unanswerable detection insight\n",
    "    unans_detection = (unans_df['robustness'] > 0.5).mean() if len(unans_df) > 0 else 0\n",
    "    if unans_detection < 0.5:\n",
    "        print(f\"  ‚ö†Ô∏è Poor unanswerable detection ({unans_detection:.1%}) - may hallucinate\")\n",
    "    else:\n",
    "        print(f\"  ‚úÖ Good unanswerable detection ({unans_detection:.1%})\")\n",
    "    \n",
    "    # Speed insight\n",
    "    avg_time = detailed_df['response_time_ms'].mean() / 1000\n",
    "    if avg_time < 1.0:\n",
    "        print(f\"  ‚úÖ Fast response time ({avg_time:.2f}s)\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è Slow response time ({avg_time:.2f}s) - consider optimization\")\n",
    "    \n",
    "    # Model spread insight\n",
    "    score_range = rankings_df['Final_Score'].max() - rankings_df['Final_Score'].min()\n",
    "    print(f\"  üìä Model score range: {score_range:.3f}\")\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # 6. SAVE SUMMARY CSV\n",
    "    # ------------------------------------------------------------------\n",
    "    summary = {\n",
    "        'Metric': ['Best Model', 'Best Score', 'Avg Accuracy', 'Avg Quality', \n",
    "                   'Avg Response Time', 'Total Q&A Pairs', 'FAISS Vectors'],\n",
    "        'Value': [best['Model'], f\"{best['Final_Score']:.3f}\", \n",
    "                  f\"{avg_acc:.1%}\", f\"{rankings_df['Quality'].mean():.1%}\",\n",
    "                  f\"{avg_time:.3f}s\", total_pairs, faiss_index.ntotal]\n",
    "    }\n",
    "    summary_df = pd.DataFrame(summary)\n",
    "    summary_df.to_csv(f\"{OUTPUT_DIR}/analysis_summary.csv\", index=False)\n",
    "    print(f\"\\nüíæ Saved: {OUTPUT_DIR}/analysis_summary.csv\")\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # 7. REFLECTION PROMPTS\n",
    "    # ------------------------------------------------------------------\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìù REFLECTION PROMPTS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\"\"\n",
    "Write your reflection addressing these questions:\n",
    "\n",
    "1. STRENGTHS: What does this RAG system do well?\n",
    "   \n",
    "2. WEAKNESSES: What are the main limitations?\n",
    "   \n",
    "3. MODEL COMPARISON: Why did the best model outperform others?\n",
    "   \n",
    "4. REAL-WORLD USE: Where could this system be deployed?\n",
    "   \n",
    "5. IMPROVEMENTS: What would you change to make it better?\n",
    "\"\"\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚úÖ OBJECTIVE 6 COMPLETE\")\n",
    "    print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù System Reflection\n",
    "\n",
    "### 1. STRENGTHS: What does this RAG system do well?\n",
    "\n",
    "- **Excellent at detecting unanswerable questions (100%)** ‚Äî The system correctly identifies when it doesn't have enough information and avoids making up answers\n",
    "- **Very fast responses (0.27s average)** ‚Äî Users get near-instant answers\n",
    "- **Good robustness (85%)** ‚Äî Handles edge cases without crashing\n",
    "- **Lightweight** ‚Äî Works with only 21 Q&A pairs and small models (MiniLM at 33M parameters)\n",
    "\n",
    "---\n",
    "\n",
    "### 2. WEAKNESSES: What are the main limitations?\n",
    "\n",
    "- **Low accuracy (25.3%)** ‚Äî Models struggle to match questions to correct answers\n",
    "- **Small knowledge base** ‚Äî Only 21 Q&A pairs limits what the system can answer\n",
    "- **Underconfident on answerable questions (21.8%)** ‚Äî System doesn't trust itself when it should\n",
    "- **No reasoning ability** ‚Äî Can only retrieve, not think through complex questions\n",
    "\n",
    "---\n",
    "\n",
    "### 3. MODEL COMPARISON: Why did the best model outperform others?\n",
    "\n",
    "**MiniLM-SQuAD won (Score: 0.598)** because:\n",
    "- Best balance of accuracy (0.328) and quality (0.609)\n",
    "- Nearly as fast as tiny models (0.988 speed)\n",
    "- Trained on SQuAD2 which includes unanswerable questions\n",
    "\n",
    "**T5-Generative ranked last** because generative models are slower and unnecessary for simple extractive QA tasks.\n",
    "\n",
    "**Key learning:** Smaller, specialized models can outperform larger general models for specific tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. REAL-WORLD USE: Where could this system be deployed?\n",
    "\n",
    "- **Internal company FAQ bot** ‚Äî Answering employee HR/IT questions\n",
    "- **Customer support helper** ‚Äî Suggesting answers to agents (not fully automated)\n",
    "- **Documentation search** ‚Äî Finding relevant help articles quickly\n",
    "\n",
    "*Would need human oversight due to 25% accuracy.*\n",
    "\n",
    "---\n",
    "\n",
    "### 5. IMPROVEMENTS: What would you change to make it better?\n",
    "\n",
    "1. **Expand knowledge base** ‚Äî Add more Q&A pairs (100+) for better coverage\n",
    "2. **Hybrid search** ‚Äî Combine keyword + semantic search for better retrieval\n",
    "3. **Fine-tune embeddings** ‚Äî Train on domain-specific data\n",
    "4. **Add confidence calibration** ‚Äî So the system trusts itself appropriately\n",
    "\n",
    "---\n",
    "\n",
    "**Overall:** \n",
    "This RAG system demonstrates the core pipeline works ‚Äî retrieval, augmentation, and generation. The main limitation is the small knowledge base, which could be expanded for production use.\n",
    "\n",
    "![RAG System Architecture](diagrams/rag-pipeline-architecture.png)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
