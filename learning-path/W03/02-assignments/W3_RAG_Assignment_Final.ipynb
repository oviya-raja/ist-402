{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Assignment: Building an Intelligent Q&A System with FAISS and Mistral\n",
    "\n",
    "**IST402 - AI Agents & RAG Systems**\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udccb Assignment Overview\n",
    "\n",
    "**Objective:** Design and implement a Retrieval-Augmented Generation (RAG) system using Mistral-7B-Instruct, FAISS vector database, and custom business data.\n",
    "\n",
    "**Submission:** Submit the link to your completed notebook.\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udfaf Learning Objectives\n",
    "\n",
    "By completing this assignment, you will:\n",
    "\n",
    "1. **Understand Prompt Engineering**: Learn how to create effective system prompts for business-specific AI assistants\n",
    "2. **Implement RAG Systems**: Build a complete RAG system with vector database and similarity search\n",
    "3. **Evaluate AI Models**: Compare and rank multiple QA models based on performance metrics\n",
    "4. **Understand System Limitations**: Test how well your system knows what it can and cannot answer\n",
    "5. **Work with Modern AI Tools**: Gain hands-on experience with HuggingFace, Mistral, FAISS, and embedding models\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcda Prerequisites\n",
    "\n",
    "Before starting, ensure you have:\n",
    "\n",
    "- **HuggingFace Token**: Get a free token from [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
    "- **Google Colab Student Tier Account** (recommended) or local environment with GPU support\n",
    "  - GPU is recommended for faster model inference\n",
    "  - Colab provides free GPU access for students\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 - Research Guide: Key Technologies for Fun Response Mode Chatbot\n\n##Take a moment to reasearch the following Terms:\n\n\n1. LangChain\n2. FAISS\n3. RAG\n4. Embeddings\n\n\n\n---\n\n### LangChain, FAISS, RAG & Embeddings\n\n---\n\n## \ud83e\udd9c **LangChain**\n\n### **What is LangChain?**\nLangChain is a framework for developing applications powered by large language models (LLMs). LangChain simplifies every stage of the LLM application lifecycle by providing modular components that make building AI applications much easier.\n\n### **Key Features & Capabilities:**\n- **Modular Architecture**: LangChain's power lies in its modular architecture - you can mix and match components\n- **Tool Integration**: Connect LLMs to APIs, databases, search engines, and external services\n- **Memory Management**: It offers complete memory management, tool-chain organization, agent regulation, and context retention integration into one unified structure\n- **Multi-Agent Systems**: LangChain has solidified itself as the go-to framework for building sophisticated, autonomous multi-agent systems\n\n### **Why Use LangChain for Chatbots?**\n- **Chain Different Operations**: Link together prompts, models, and tools in sequence\n- **Context Retention**: Usually, whenever you request something from a model, it does not retain any information after providing the response - LangChain fixes this\n- **Real-time Data Access**: Connect your chatbot to live data sources\n- **Easy Integration**: Works with OpenAI, Anthropic, Google, and many other AI providers\n\n### **LangChain in 2025:**\nThe question now is: Is LangChain still needed in 2025? The answer is **YES** - it's more relevant than ever with enhanced features like:\n- **LangGraph**: For building complex, stateful agent workflows\n- **LangSmith**: For debugging and monitoring AI applications\n- **Better Documentation**: Improved learning resources and examples\n\n---\n\n## \ud83d\udd0d **FAISS (Facebook AI Similarity Search)**\n\n### **What is FAISS?**\nFaiss is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM\n\n### **Key Features:**\n- **Lightning Fast**: We've built nearest-neighbor search implementations for billion-scale data sets that are some 8.5x faster than the previous reported state-of-the-art\n- **Memory Efficient**: Some of the methods, like those based on binary vectors and compact quantization codes, solely use a compressed representation of the vectors and do not require to keep the original vectors\n- **GPU Acceleration**: Faiss supports GPU acceleration, significantly enhancing the speed of vector operations and making it suitable for real-time applications\n- **Scalable**: Can handle millions to billions of vectors\n\n### **Why Use FAISS in Chatbots?**\n- **Fast Document Retrieval**: Quickly find the most relevant information from your knowledge base\n- **Similarity Search**: Find documents similar to user queries in milliseconds\n- **Memory Optimization**: Faiss focuses on methods that compress the original vectors, because they're the only ones that scale to data sets of billions of vectors\n- **Easy Integration**: The integration lives in the langchain-community package\n\n### **FAISS vs Other Vector Databases:**\nWhile there are alternatives like Pinecone and ChromaDB, Faiss is an open-source library for the swift search of similarities and the clustering of dense vectors that's completely free and integrates seamlessly with LangChain.\n\n---\n\n##  **RAG (Retrieval-Augmented Generation)**\n\n### **What is RAG?**\nRetrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response\n\n### **How RAG Works:**\n1. **User Query**: Person asks a question\n2. **Retrieval**: System searches knowledge base for relevant information\n3. **Augmentation**: The RAG model augments the user input (or prompts) by adding the relevant retrieved data in context\n4. **Generation**: LLM generates answer using both its training and retrieved information\n\n### **Why RAG is Revolutionary:**\n- **Up-to-date Information**: RAG extends the already powerful capabilities of LLMs to specific domains or an organization's internal knowledge base, all without the need to retrain the model\n- **Reduces Hallucinations**: Grounds responses in factual, retrieved data\n- **Cost-Effective**: It is a cost-effective approach to improving LLM output so it remains relevant, accurate, and useful in various contexts\n- **Domain-Specific Knowledge**: Add company-specific or specialized information\n\n### **RAG in 2025:**\nIn 2025, advanced RAG systems address these and other limitations with a variety of innovations and architectural considerations. These enhancements push RAG from useful to indispensable\n\n**New RAG Capabilities:**\n- **Adaptive RAG**: Systems now dynamically adjust retrieval strategies based on query intent\n- **Multi-modal RAG**: Handle text, images, and other data types\n- **Self-Correcting RAG**: Systems that validate their own outputs\n\n---\n\n##  **Embeddings**\n\n### **What are Embeddings?**\nTechnically, embeddings are vectors created by machine learning models for the purpose of capturing meaningful data about each object - they convert words, images, and other data into numbers that computers can understand and compare.\n\n### **How Embeddings Work:**\nEmbeddings convert real-world objects into complex mathematical representations that capture inherent properties and relationships between real-world data\n\n**Simple Example:**\n- \"cat\" might become `[0.2, -0.4, 0.7]`\n- \"dog\" might become `[0.3, -0.5, 0.6]`\n- \"car\" might become `[0.8, 0.1, -0.2]`\n\nNotice how \"cat\" and \"dog\" are closer together (more similar) than either is to \"car\"!\n\n### **Why Embeddings Matter:**\n- **Semantic Understanding**: Since embeddings make it possible for computers to understand the relationships between words and other objects, they are foundational for artificial intelligence (AI)\n- **Similarity Search**: Essentially, embeddings enable machine learning models to find similar objects\n- **Foundation of Modern AI**: Vector embeddings thus underpin nearly all modern machine learning, powering models used in the fields of NLP and computer vision, and serving as the fundamental building blocks of generative AI\n\n### **Types of Embeddings:**\n- **Text Embeddings**: Convert words/sentences to vectors\n- **Image Embeddings**: Convert images to numerical representations\n- **Multimodal Embeddings**: Handle multiple data types together\n\n### **2025 State of Embeddings:**\nWith the exception of OpenAI (whose text-embedding-3 models from March 2023 are ancient in light of the pace of AI progress), all the prominent commercial vector embedding vendors released a new version of their flagship models in late 2024 or early 2025\n\n---\n\n## \ud83d\udd17 **How They Work Together in Your Chatbot**\n\n### **The Complete Pipeline:**\n\n1. **Knowledge Preparation** (Embeddings + FAISS):\n   - Convert your FAQ documents into embeddings (numerical vectors)\n   - Store these embeddings in FAISS for fast similarity search\n\n2. **User Query Processing** (RAG):\n   - User asks: \"What are your store hours?\"\n   - Convert question to embedding\n   - Use FAISS to find most similar FAQ items\n\n3. **Response Generation** (LangChain):\n   - LangChain retrieves relevant documents\n   - Adds mood/personality using system prompts\n   - Generates final response using LLM\n\n4. **Fun Response Mode**:\n   - Apply system prompt engineering for different personality modes\n   - Generate funny, mysterious, or serious versions of the same answer through prompt design\n\n### **Example Workflow:**\n```\nUser: \"What are your store hours?\" (Mood: Funny)\n\n1. Embedding: [0.1, 0.8, 0.3, ...]\n2. FAISS Search: Finds \"We are open 9am\u20139pm, Mon\u2013Sat\"\n3. RAG Retrieval: Gets store hours policy document\n4. LangChain + System Prompt: \"You are a witty assistant who loves humor.\n   Based on this context: 'We are open 9am\u20139pm, Mon\u2013Sat'\n   Answer: What are your store hours?\"\n5. Response: \"We're open 9am-9pm Monday through Saturday!\n   We're like vampires - we come alive when the sun goes down,\n   but we still close at 9pm because even vampires need sleep!\"\n```\n\n---\n\n## \ud83d\udca1 **Why This Matters for Your Week 2 Project**\n\n### **Learning Objectives Achieved:**\n- **Prompt Engineering**: Using system prompts for personality and mood control\n- **Data Processing**: Understanding how text becomes searchable embeddings\n- **System Integration**: Seeing how multiple AI components work together\n- **Real-world Application**: Building something that could actually be deployed\n\n### **Technical Skills Developed:**\n- **Vector Databases**: Understanding modern data storage for AI\n- **Information Retrieval**: Learning how search engines really work\n- **AI Frameworks**: Hands-on experience with industry-standard tools\n- **API Integration**: Connecting different AI services together\n\n### **Industry Relevance:**\nThese four technologies power virtually every AI application you use daily:\n- **ChatGPT**: Uses embeddings and retrieval techniques\n- **Google Search**: Employs vector similarity for results\n- **Recommendation Systems**: Netflix, Spotify use embeddings\n- **Customer Service Bots**: Built with LangChain + RAG architectures\n\n---\n\n## **Getting Started Tips**\n\n### **Installation Order:**\n1. **transformers** - For pre-trained AI models\n2. **langchain** - Main framework\n3. **langchain-community** - Additional integrations\n4. **sentence-transformers** - For creating embeddings\n5. **torch** - Deep learning backend\n6. **faiss-cpu** - Vector similarity search\n\n### **Best Practices:**\n- Start simple with basic RAG, then add complexity\n- Test each component individually before combining\n- Use small datasets while learning\n- Experiment with different embedding models\n- Design effective system prompts for different personality modes\n\n### **Common Pitfalls to Avoid:**\n- Don't try to implement everything at once\n- Make sure your embeddings model matches your language\n- Test with simple questions first\n- Keep system prompts clear and concise for consistent behavior\n\n---\n\n**Ready to build your Fun Response Mode Chatbot? These technologies will give you the foundation to create an AI assistant that's both intelligent and entertaining!** \u2728"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}