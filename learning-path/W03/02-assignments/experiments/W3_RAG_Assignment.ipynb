{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "# RAG Assignment: \n",
        "## Building an Intelligent Q&A System with FAISS and Mistral\n",
        "\n",
        "**IST402 - AI Agents & RAG Systems**  \n",
        "**Student**: Oviya Raja\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“‹ Assignment Overview\n",
        "\n",
        "### ğŸ¯ Goal\n",
        "I will design and implement a complete **Retrieval-Augmented Generation (RAG) system** that can answer questions based on a custom knowledge base, evaluate multiple AI models, and analyze system performance.\n",
        "\n",
        "**What I'm Building:**\n",
        "A production-ready RAG system that combines semantic search (FAISS) with large language models (Mistral) to provide accurate, context-aware answers for customer service scenarios.\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ¢ Business Context</b> (Click to expand)</summary>\n",
        "\n",
        "**Company:** GreenTech Marketplace\n",
        "\n",
        "**Company Description:**\n",
        "GreenTech Marketplace is an e-commerce platform specializing in sustainable technology products. The company focuses on eco-friendly technology solutions, renewable energy products, and smart home devices that help customers reduce their environmental footprint.\n",
        "\n",
        "**Company Details:**\n",
        "- **Type:** E-commerce platform\n",
        "- **Specialization:** Sustainable technology products\n",
        "- **Mission:** Providing eco-friendly technology solutions for modern living\n",
        "\n",
        "**Contact Information:**\n",
        "- **Email:** support@greentechmarketplace.com\n",
        "- **Phone:** 1-800-GREEN-TECH\n",
        "- **Business Hours:** \n",
        "  - Monday-Friday: 9 AM - 6 PM EST\n",
        "  - Saturday: 10 AM - 4 PM EST\n",
        "  - Sunday: Closed\n",
        "\n",
        "**Shipping & Policies:**\n",
        "- **Standard Shipping:** 5-7 business days (free over $75)\n",
        "- **Express Shipping:** 2-3 business days (additional fee)\n",
        "- **Return Policy:** 30-day return policy for unopened items in original packaging\n",
        "- **Refund Processing:** 5-7 business days after receipt\n",
        "- **Warranty:** 1-3 years depending on product category\n",
        "\n",
        "**Product Categories:**\n",
        "- **Solar Panels** - Renewable energy solutions for homes and businesses\n",
        "- **Energy-Efficient Appliances** - Eco-friendly home appliances\n",
        "- **Smart Home Devices** - Home automation and IoT solutions\n",
        "- **Eco-Friendly Accessories** - Sustainable lifestyle products\n",
        "\n",
        "**Q&A Database Categories (21 pairs total):**\n",
        "\n",
        "**Answerable Categories (18 pairs - 6 categories Ã— 3 pairs each):**\n",
        "\n",
        "| Category | Count | Description |\n",
        "|----------|-------|-------------|\n",
        "| **products** | 3 pairs | Types of products sold, solar panels, smart home devices, eco-friendly items |\n",
        "| **shipping** | 3 pairs | Delivery times, shipping costs, free shipping threshold, tracking |\n",
        "| **returns** | 3 pairs | Return policy, refund process, conditions, 30-day policy |\n",
        "| **customer_service** | 3 pairs | Business hours (Mon-Fri 9AM-6PM, Sat 10AM-4PM), contact email and phone |\n",
        "| **warranty** | 3 pairs | Warranty duration (1-3 years), coverage, claims process |\n",
        "| **orders** | 3 pairs | Order status, tracking, modifications, cancellations |\n",
        "\n",
        "**Unanswerable Types (3 pairs - 3 types Ã— 1 pair each):**\n",
        "\n",
        "| Type | Count | Description |\n",
        "|------|-------|-------------|\n",
        "| **competitor** | 1 pair | Questions about competitor pricing, products, or comparisons |\n",
        "| **personal_advice** | 1 pair | Questions asking for personal recommendations or opinions |\n",
        "| **future_events** | 1 pair | Questions about future sales, unreleased products, or predictions |\n",
        "\n",
        "**Total:** 18 answerable + 3 unanswerable = 21 Q&A pairs\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ”§ Technologies & Tools</b> (Click to expand)</summary>\n",
        "\n",
        "| Technology | Purpose | Version/Model |\n",
        "|------------|---------|---------------|\n",
        "| **Mistral-7B-Instruct** | LLM for generating Q&A pairs and system prompts | `mistralai/Mistral-7B-Instruct-v0.3` |\n",
        "| **FAISS** | Vector database for efficient similarity search | `faiss-cpu` |\n",
        "| **Sentence Transformers** | Text embeddings for semantic search | `all-MiniLM-L6-v2` |\n",
        "| **Hugging Face Transformers** | Multiple QA models for evaluation | Various models |\n",
        "| **RAGAS** | RAG Assessment framework for evaluation | Latest version |\n",
        "| **LangChain** | LLM pipeline integration | Latest version |\n",
        "\n",
        "**Key Libraries:**\n",
        "- `transformers` - Model loading and inference\n",
        "- `torch` - Deep learning framework\n",
        "- `sentence-transformers` - Embedding generation\n",
        "- `faiss-cpu` - Vector similarity search\n",
        "- `datasets` - Data handling\n",
        "- `ragas` - RAG evaluation metrics\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ—ï¸ System Architecture</b> (Click to expand)</summary>\n",
        "\n",
        "**Complete Left-to-Right Flow:**\n",
        "\n",
        "```\n",
        "USER QUESTION\n",
        "     â”‚\n",
        "     â–¼\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚Objective â”‚â”€â”€â”€â–¶â”‚Objective â”‚â”€â”€â”€â–¶â”‚Objective â”‚â”€â”€â”€â–¶â”‚Objective â”‚â”€â”€â”€â–¶â”‚Objective â”‚â”€â”€â”€â–¶â”‚Objective â”‚\n",
        "â”‚    0     â”‚    â”‚    1     â”‚    â”‚    2     â”‚    â”‚    3     â”‚    â”‚    4     â”‚    â”‚    5     â”‚\n",
        "â”‚  Setup   â”‚    â”‚  Mistral â”‚    â”‚  Q&A DB  â”‚    â”‚  FAISS   â”‚    â”‚   RAG    â”‚    â”‚ Evaluate â”‚\n",
        "â”‚          â”‚    â”‚  Model   â”‚    â”‚ 21 pairs â”‚    â”‚  Index   â”‚    â”‚ Pipeline â”‚    â”‚  Models  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
        "                                                                                      â”‚\n",
        "                                                                                      â–¼\n",
        "                                                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "                                                                              â”‚Objective â”‚\n",
        "                                                                              â”‚    6     â”‚\n",
        "                                                                              â”‚ Analysis â”‚\n",
        "                                                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "**RAG Pipeline Flow (Left-to-Right):**\n",
        "\n",
        "```\n",
        "User Question\n",
        "     â”‚\n",
        "     â”œâ”€â–¶ [Embed] â”€â”€â–¶ [FAISS Search] â”€â”€â–¶ [Retrieve Context] â”€â”€â–¶ [Mistral LLM] â”€â”€â–¶ Final Answer\n",
        "     â”‚\n",
        "     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "**Objective Mapping:**\n",
        "\n",
        "| Objective | Component | Input | Output | Purpose |\n",
        "|-----------|-----------|-------|--------|---------|\n",
        "| **0: Setup** | Environment | None | `hf_token`, packages | Configure environment and install dependencies |\n",
        "| **1: System Prompt** | Mistral Model | `hf_token` | `mistral_model`, `system_prompt` | Load LLM and create customer service prompt |\n",
        "| **2: Q&A Database** | Knowledge Base | `mistral_model` | `qa_database` (21 pairs) | Generate Q&A pairs for the knowledge base |\n",
        "| **3: FAISS Index** | Vector DB | `qa_database` | `faiss_index`, `embed_query()` | Build semantic search index |\n",
        "| **4: RAG Pipeline** | Complete System | `mistral_model`, `qa_database`, `faiss_index` | `rag_query()`, `search_faiss()` | Create end-to-end RAG system |\n",
        "| **5: Model Evaluation** | Evaluation | `rag_query()`, test questions | `model_evaluations`, rankings | Evaluate 6 QA models using RAGAS |\n",
        "| **6: System Analysis** | Analysis | `model_evaluations`, all components | `system_analysis` report | Analyze limitations and provide recommendations |\n",
        "\n",
        "**Dependency Chain:**\n",
        "```\n",
        "0 (Setup) â†’ 1 (Mistral) â†’ 2 (Q&A DB) â†’ 3 (FAISS) â†’ 4 (RAG) â†’ 5 (Evaluate) â†’ 6 (Analysis)\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ“š Objectives Breakdown</b> (Click to expand)</summary>\n",
        "\n",
        "| Objective | Purpose | Key Deliverable | Estimated Time |\n",
        "|-----------|---------|----------------|----------------|\n",
        "| **0: Setup** | Install packages, configure environment | `hf_token`, environment variables | 1-2 min |\n",
        "| **1: System Prompt** | Design customer service system prompt | `system_prompt`, `mistral_model` | 3-5 min (CPU) / 1-2 min (GPU) |\n",
        "| **2: Q&A Database** | Generate 21 Q&A pairs (18 answerable + 3 unanswerable) | `qa_database` (21 pairs) | 2-3 min |\n",
        "| **3: FAISS Index** | Build vector database for semantic search | `faiss_index`, `embed_query()` | 30 sec |\n",
        "| **4: RAG Pipeline** | Create complete RAG system | `rag_query()`, `search_faiss()` | 30 sec |\n",
        "| **5: Model Evaluation** | Evaluate 6 QA models using RAGAS | `model_evaluations`, rankings | 15-25 min (CPU) / 8-15 min (GPU) |\n",
        "| **6: System Analysis** | Analyze limitations and provide recommendations | `system_analysis` report | 30 sec |\n",
        "\n",
        "**Total Estimated Time:**\n",
        "- **CPU**: ~25-30 minutes\n",
        "- **GPU**: ~15-20 minutes\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>âœ… Requirements Checklist</b> (Click to expand)</summary>\n",
        "\n",
        "**Technical Requirements:**\n",
        "\n",
        "| Requirement | Status | Description |\n",
        "|-------------|--------|-------------|\n",
        "| System Prompt Design | âœ… Required | E-commerce customer service context |\n",
        "| Generate Business Database | âœ… Required | 21 Q&A pairs (18 answerable + 3 unanswerable) |\n",
        "| FAISS Implementation | âœ… Required | Vector database with semantic search |\n",
        "| Test Questions | âœ… Required | 5 answerable + 5 unanswerable questions |\n",
        "| RAG Pipeline | âœ… Required | Complete retrieval-augmented generation system |\n",
        "| Model Experimentation | âœ… Required | 4 required + 2 additional models (6 total) |\n",
        "| Model Ranking | âœ… Required | Rankings with explanations |\n",
        "| Confidence Analysis | âš ï¸ Partial | BERTScore used, detailed analysis recommended |\n",
        "| Reflection Section | âœ… Required | System analysis and recommendations |\n",
        "\n",
        "**Deliverables:**\n",
        "- âœ… Working RAG system\n",
        "- âœ… Model evaluation results\n",
        "- âœ… System analysis report\n",
        "- âœ… CSV files with results\n",
        "- âœ… Well-documented notebook\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ“ Learning Outcomes</b> (Click to expand)</summary>\n",
        "\n",
        "By completing this assignment, I will:\n",
        "\n",
        "1. **Understand RAG Architecture**\n",
        "   - How retrieval and generation work together\n",
        "   - Semantic search vs keyword search\n",
        "   - Context window management\n",
        "\n",
        "2. **Master Vector Databases**\n",
        "   - FAISS indexing and search\n",
        "   - Embedding generation and storage\n",
        "   - Similarity metrics (L2, cosine)\n",
        "\n",
        "3. **Evaluate AI Models**\n",
        "   - Multiple evaluation approaches (RAGAS, LLM-as-Judge)\n",
        "   - Model comparison and ranking\n",
        "   - Confidence score analysis\n",
        "\n",
        "4. **System Design Skills**\n",
        "   - Modular code design (SOLID principles)\n",
        "   - Error handling and validation\n",
        "   - Performance optimization\n",
        "\n",
        "5. **Critical Analysis**\n",
        "   - System limitations identification\n",
        "   - Real-world application assessment\n",
        "   - Improvement recommendations\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ“Š Business Context Options</b> (Click to expand)</summary>\n",
        "\n",
        "I could choose any business context for my RAG system. Examples:\n",
        "\n",
        "| Context | Use Case | Example Questions |\n",
        "|---------|----------|-------------------|\n",
        "| **E-commerce** | Customer service | \"What is your return policy?\", \"How long does shipping take?\" |\n",
        "| **Healthcare** | Patient information | \"What are visiting hours?\", \"Do you accept insurance?\" |\n",
        "| **Education** | Student support | \"When is registration?\", \"What courses are available?\" |\n",
        "| **Finance** | Banking support | \"What are account fees?\", \"How do I transfer money?\" |\n",
        "| **Real Estate** | Property inquiries | \"What properties are available?\", \"What are the prices?\" |\n",
        "\n",
        "**My Assignment Context:** E-commerce Customer Service (GreenTech Marketplace)\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>â±ï¸ Time Management Tips</b> (Click to expand)</summary>\n",
        "\n",
        "**Recommended Workflow:**\n",
        "\n",
        "1. **Setup (5 min)**\n",
        "   - Install packages\n",
        "   - Configure Hugging Face token\n",
        "   - Verify GPU access (if available)\n",
        "\n",
        "2. **Objectives 1-3 (10-15 min)**\n",
        "   - Load Mistral model\n",
        "   - Generate Q&A database\n",
        "   - Build FAISS index\n",
        "\n",
        "3. **Objective 4 (5 min)**\n",
        "   - Create RAG pipeline\n",
        "   - Test with sample questions\n",
        "\n",
        "4. **Objective 5 (15-25 min)**\n",
        "   - Evaluate 6 models\n",
        "   - Generate rankings\n",
        "   - **Longest step - be patient!**\n",
        "\n",
        "5. **Objective 6 (5 min)**\n",
        "   - Analyze system\n",
        "   - Write reflection\n",
        "\n",
        "**Total Time:** ~40-55 minutes\n",
        "\n",
        "**Tips:**\n",
        "- Use GPU for faster model loading and inference\n",
        "- Run Objective 5 overnight if needed (models take time)\n",
        "- Save intermediate results to avoid re-running\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ“ Output Files Generated</b> (Click to expand)</summary>\n",
        "\n",
        "I will generate several output files:\n",
        "\n",
        "| File | Location | Description |\n",
        "|------|----------|-------------|\n",
        "| `qa_database.csv` | `data/qa_database/` | All 21 Q&A pairs with metadata |\n",
        "| `evaluation_results.csv` | `data/qa_database/` | LLM-as-Judge quality scores |\n",
        "| `qa_embeddings.npy` | `data/vector_database/` | Embedding vectors (numpy array) |\n",
        "| `qa_index.faiss` | `data/vector_database/` | FAISS index file (serialized) |\n",
        "| `retrieval_test_results.csv` | `data/vector_database/` | Test query results |\n",
        "| `model_scores.csv` | `data/model_evaluation/` | All model evaluation scores |\n",
        "| `model_responses.csv` | `data/model_evaluation/` | All model responses |\n",
        "| `model_rankings.csv` | `data/model_evaluation/` | Final model rankings |\n",
        "| `system_analysis.txt` | `data/system_analysis/` | Complete analysis report |\n",
        "\n",
        "**All files will be saved automatically during execution.**\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ’¡ Getting Started</b> (Click to expand)</summary>\n",
        "\n",
        "**Step 1: Environment Setup**\n",
        "1. I'll open Google Colab (recommended) or Jupyter Notebook\n",
        "2. Enable GPU runtime (Colab: Runtime â†’ Change runtime type â†’ GPU)\n",
        "3. Get Hugging Face token from [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
        "\n",
        "**Step 2: Run Setup Cell**\n",
        "- Execute Objective 0 (Setup & Prerequisites)\n",
        "- Install required packages\n",
        "- Authenticate with Hugging Face\n",
        "\n",
        "**Step 3: Execute Objectives Sequentially**\n",
        "- Run objectives in order (0 â†’ 1 â†’ 2 â†’ 3 â†’ 4 â†’ 5 â†’ 6)\n",
        "- Each objective builds on the previous one\n",
        "- Don't skip ahead!\n",
        "\n",
        "**Step 4: Review Results**\n",
        "- Check generated CSV files\n",
        "- Review model rankings\n",
        "- Read system analysis report\n",
        "\n",
        "**If I Encounter Issues:**\n",
        "- Check error messages carefully\n",
        "- Verify prerequisites are met\n",
        "- Ensure GPU is enabled for faster execution\n",
        "\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "**Ready to begin?** I'll start with **Objective 0: Setup & Prerequisites** to configure my environment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "## ğŸ“– Personal Notes\n",
        "\n",
        "### ğŸ“š Learning Resources\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ“„ Reference Documentation</b> (Click to expand)</summary>\n",
        "\n",
        "For detailed explanations of key concepts, I refer to:\n",
        "\n",
        "**ğŸ“„ `learning-path/W02/03-learnings/langchain-faiss-rag-embeddings.md`**\n",
        "\n",
        "This comprehensive reference file contains detailed explanations of:\n",
        "\n",
        "| Topic | What I Learned |\n",
        "|-------|----------------|\n",
        "| **ğŸ¦œ LangChain Framework** | How to build AI systems with modular components, chain operations, and integrate with various LLMs |\n",
        "| **ğŸ” FAISS Vector Database** | Similarity search algorithms, indexing strategies, and performance optimization |\n",
        "| **ğŸ”„ RAG Architecture** | How retrieval and generation work together, context window management, and pipeline design |\n",
        "| **ğŸ§® Embeddings & Vectors** | Semantic understanding, vector representations, and similarity metrics |\n",
        "| **ğŸ”— Component Integration** | How LangChain, FAISS, and embeddings work together in a complete RAG pipeline |\n",
        "| **ğŸ’¡ Best Practices** | Getting started tips, common pitfalls, and implementation strategies |\n",
        "\n",
        "**Key Takeaways:**\n",
        "- RAG combines the accuracy of retrieval with the fluency of generation\n",
        "- FAISS enables fast semantic search at scale\n",
        "- Embeddings capture meaning, not just keywords\n",
        "- Proper context formatting is crucial for good answers\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ’­ My Learning Journey</b> (Click to expand)</summary>\n",
        "\n",
        "**What I'm Learning:**\n",
        "\n",
        "1. **RAG System Design**\n",
        "   - How to structure a complete RAG pipeline\n",
        "   - Balancing retrieval quality with generation quality\n",
        "   - Context window management strategies\n",
        "\n",
        "2. **Vector Databases**\n",
        "   - FAISS indexing and search mechanisms\n",
        "   - Embedding model selection and optimization\n",
        "   - Similarity metrics and their trade-offs\n",
        "\n",
        "3. **Model Evaluation**\n",
        "   - Multiple evaluation approaches (RAGAS, library-based)\n",
        "   - Interpreting evaluation metrics\n",
        "   - Model comparison and selection\n",
        "\n",
        "4. **System Analysis**\n",
        "   - Identifying limitations and edge cases\n",
        "   - Scalability considerations\n",
        "   - Real-world deployment challenges\n",
        "\n",
        "**Challenges I Encountered:**\n",
        "- Understanding embedding dimensions and their impact\n",
        "- Balancing model size vs. performance\n",
        "- Interpreting evaluation metrics correctly\n",
        "- Optimizing retrieval for better context quality\n",
        "\n",
        "**Solutions I Found:**\n",
        "- Using pre-trained embedding models (sentence-transformers)\n",
        "- Implementing proper error handling\n",
        "- Testing with both answerable and unanswerable questions\n",
        "- Documenting design choices for clarity\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ”§ Implementation Notes</b> (Click to expand)</summary>\n",
        "\n",
        "**Design Decisions I Made:**\n",
        "\n",
        "| Decision | Choice | Rationale |\n",
        "|----------|--------|-----------|\n",
        "| **Embedding Model** | `all-MiniLM-L6-v2` | Fast, 384 dims, good quality for small datasets |\n",
        "| **FAISS Index Type** | `IndexFlatL2` | Exact search, best for <100k vectors |\n",
        "| **Top-K Retrieval** | 3 documents | Balance between context and noise |\n",
        "| **Business Context** | E-commerce | Rich domain with clear policies |\n",
        "| **Q&A Count** | 21 pairs | Sufficient for demo, covers key topics |\n",
        "| **Evaluation Approach** | RAGAS + Library | Multiple perspectives for comprehensive evaluation |\n",
        "\n",
        "**Code Organization:**\n",
        "- Modular functions following SOLID principles\n",
        "- Clear separation of concerns\n",
        "- Reusable components\n",
        "- Comprehensive error handling\n",
        "\n",
        "**Performance Optimizations:**\n",
        "- Model caching to avoid redundant loads\n",
        "- Batch processing where possible\n",
        "- GPU utilization when available\n",
        "- Efficient data structures\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ“ Key Concepts Reference</b> (Click to expand)</summary>\n",
        "\n",
        "**Quick Reference for Key Terms:**\n",
        "\n",
        "| Term | Definition | Example |\n",
        "|------|------------|--------|\n",
        "| **RAG** | Retrieval-Augmented Generation - combines search with LLM | Search KB â†’ Get context â†’ Generate answer |\n",
        "| **Embedding** | Vector representation of text capturing semantic meaning | \"shipping\" â†’ [0.12, -0.45, 0.78, ...] |\n",
        "| **FAISS** | Facebook AI Similarity Search - fast vector search library | Find similar documents in milliseconds |\n",
        "| **Semantic Search** | Finding content by meaning, not just keywords | \"delivery time\" matches \"shipping duration\" |\n",
        "| **Context Window** | Text provided to LLM along with the question | Retrieved Q&A pairs formatted as context |\n",
        "| **Top-K Retrieval** | Get K most similar documents from index | Top 3 most relevant Q&A pairs |\n",
        "| **RAGAS** | RAG Assessment - framework for evaluating RAG systems | Metrics: faithfulness, answer relevance |\n",
        "\n",
        "**How They Work Together:**\n",
        "1. **Embedding** converts text to vectors\n",
        "2. **FAISS** searches for similar vectors\n",
        "3. **Semantic Search** finds relevant content by meaning\n",
        "4. **Context Window** provides retrieved info to LLM\n",
        "5. **RAG** combines retrieval + generation\n",
        "6. **RAGAS** evaluates the complete system\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ“ Assignment Insights</b> (Click to expand)</summary>\n",
        "\n",
        "**What This Assignment Teaches Me:**\n",
        "\n",
        "1. **End-to-End System Building**\n",
        "   - From data generation to deployment considerations\n",
        "   - How components integrate in a production system\n",
        "   - Trade-offs between different approaches\n",
        "\n",
        "2. **Evaluation is Critical**\n",
        "   - Multiple metrics provide different perspectives\n",
        "   - Both answerable and unanswerable questions are important\n",
        "   - Model selection requires careful analysis\n",
        "\n",
        "3. **Documentation Matters**\n",
        "   - Clear documentation helps understand design choices\n",
        "   - Well-structured code is easier to maintain\n",
        "   - Reflection reveals improvement opportunities\n",
        "\n",
        "4. **Real-World Considerations**\n",
        "   - Scalability planning is essential\n",
        "   - Edge cases must be handled gracefully\n",
        "   - User experience depends on system design\n",
        "\n",
        "**Skills I'm Developing:**\n",
        "- System architecture design\n",
        "- Model evaluation and comparison\n",
        "- Critical analysis and reflection\n",
        "- Technical documentation\n",
        "- Problem-solving and debugging\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ”— Additional Resources</b> (Click to expand)</summary>\n",
        "\n",
        "**Documentation I Referenced:**\n",
        "\n",
        "1. **LangChain & FAISS Guide**\n",
        "   - Location: `learning-path/W02/03-learnings/langchain-faiss-rag-embeddings.md`\n",
        "   - Covers: LangChain, FAISS, RAG, embeddings concepts\n",
        "\n",
        "2. **Hugging Face Documentation**\n",
        "   - Models: [huggingface.co/models](https://huggingface.co/models)\n",
        "   - Transformers: [huggingface.co/docs/transformers](https://huggingface.co/docs/transformers)\n",
        "\n",
        "3. **FAISS Documentation**\n",
        "   - GitHub: [github.com/facebookresearch/faiss](https://github.com/facebookresearch/faiss)\n",
        "   - Index types and performance guides\n",
        "\n",
        "4. **RAGAS Documentation**\n",
        "   - Framework: [docs.ragas.io](https://docs.ragas.io)\n",
        "   - Evaluation metrics and best practices\n",
        "\n",
        "**Useful Commands:**\n",
        "```python\n",
        "# Check GPU\n",
        "import torch\n",
        "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
        "\n",
        "# Check installed packages\n",
        "!pip list | grep -E \"transformers|faiss|ragas|langchain\"\n",
        "\n",
        "# Verify token\n",
        "import os\n",
        "print(f\"HF Token: {'Set' if os.getenv('HUGGINGFACE_HUB_TOKEN') else 'Not set'}\")\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”§ Objective 0: Setup & Prerequisites\n",
        "\n",
        "### ğŸ¯ Goal\n",
        "I will set up my environment with all required packages, configure authentication, and verify system capabilities before starting the RAG system implementation.\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ“‹ Prerequisites Checklist</b> (Click to expand)</summary>\n",
        "\n",
        "**Knowledge Prerequisites:**\n",
        "\n",
        "| Requirement | Level | Description |\n",
        "|-------------|-------|-------------|\n",
        "| **Python Programming** | Basic | Variables, functions, data structures, imports |\n",
        "| **Jupyter Notebooks** | Basic | Running cells, markdown formatting, code execution |\n",
        "| **Machine Learning Concepts** | High-level | Neural networks, transformers, embeddings, model inference |\n",
        "| **Evaluation Metrics** | Basic | Understanding of accuracy, confidence scores |\n",
        "\n",
        "**Technical Prerequisites:**\n",
        "\n",
        "| Item | Required | How to Get |\n",
        "|------|----------|------------|\n",
        "| **Hugging Face Account** | âœ… Yes | Sign up at [huggingface.co](https://huggingface.co) |\n",
        "| **Hugging Face Token** | âœ… Yes | Get from [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens) |\n",
        "| **Google Colab Account** | âš ï¸ Recommended | Free account at [colab.research.google.com](https://colab.research.google.com) |\n",
        "| **GPU Access** | âš ï¸ Optional | Colab provides free GPU, or local GPU setup |\n",
        "| **Python 3.8+** | âœ… Yes | Pre-installed in Colab, or install locally |\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ“¦ Required Packages</b> (Click to expand)</summary>\n",
        "\n",
        "**Core Packages:**\n",
        "\n",
        "| Package | Purpose | Version |\n",
        "|---------|---------|---------|\n",
        "| `transformers` | Load and use Hugging Face models (Mistral, QA models) | Latest |\n",
        "| `torch` | Deep learning framework for model inference | Latest |\n",
        "| `sentence-transformers` | Generate embeddings for semantic search | Latest |\n",
        "| `faiss-cpu` | Vector similarity search library | Latest |\n",
        "| `datasets` | Data handling and processing | Latest |\n",
        "| `accelerate` | Model loading optimization | Latest |\n",
        "\n",
        "**Evaluation & Integration:**\n",
        "\n",
        "| Package | Purpose | Version |\n",
        "|---------|---------|---------|\n",
        "| `ragas` | RAG Assessment framework for evaluation | Latest |\n",
        "| `langchain` | LLM pipeline integration | Latest |\n",
        "| `langchain-community` | Community integrations | Latest |\n",
        "| `langchain-huggingface` | Hugging Face integration for LangChain | Latest |\n",
        "| `nest_asyncio` | Async support for RAGAS | Latest |\n",
        "\n",
        "**Installation Command:**\n",
        "```python\n",
        "!pip install transformers torch sentence-transformers faiss-cpu datasets accelerate ragas langchain langchain-community langchain-huggingface nest_asyncio\n",
        "```\n",
        "\n",
        "**Note:** All packages will be installed automatically by the setup code cell.\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ–¥ï¸ Environment Setup</b> (Click to expand)</summary>\n",
        "\n",
        "**Option 1: Google Colab (Recommended)**\n",
        "\n",
        "**Advantages:**\n",
        "- âœ… Free GPU access (T4 GPU)\n",
        "- âœ… No local installation needed\n",
        "- âœ… Pre-configured environment\n",
        "- âœ… Easy sharing and collaboration\n",
        "\n",
        "**Setup Steps:**\n",
        "1. Go to [colab.research.google.com](https://colab.research.google.com)\n",
        "2. Create a new notebook\n",
        "3. Enable GPU: **Runtime â†’ Change runtime type â†’ GPU â†’ Save**\n",
        "4. Upload or create the notebook file\n",
        "5. Run the setup cell\n",
        "\n",
        "**Option 2: Local Jupyter Notebook**\n",
        "\n",
        "**Requirements:**\n",
        "- Python 3.8 or higher\n",
        "- Jupyter Notebook installed\n",
        "- GPU optional (CPU works but slower)\n",
        "\n",
        "**Setup Steps:**\n",
        "1. Install Python 3.8+\n",
        "2. Install Jupyter: `pip install jupyter`\n",
        "3. Install required packages (see above)\n",
        "4. Launch: `jupyter notebook`\n",
        "5. Open the notebook file\n",
        "\n",
        "**GPU Setup (Optional but Recommended):**\n",
        "\n",
        "| Platform | GPU Type | Setup |\n",
        "|----------|----------|-------|\n",
        "| **Colab** | T4 (Free) | Runtime â†’ Change runtime type â†’ GPU |\n",
        "| **Local** | NVIDIA GPU | Install CUDA toolkit, PyTorch with CUDA |\n",
        "| **CPU Only** | N/A | Works but 2-3x slower |\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ”‘ Hugging Face Authentication</b> (Click to expand)</summary>\n",
        "\n",
        "**Step 1: Get Your Token**\n",
        "\n",
        "1. Go to [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
        "2. Click **\"New token\"**\n",
        "3. Name it (e.g., \"RAG Assignment\")\n",
        "4. Select **\"Read\"** access (sufficient for this assignment)\n",
        "5. Click **\"Generate token\"**\n",
        "6. **Copy the token immediately** (you won't see it again!)\n",
        "\n",
        "**Step 2: Store Your Token**\n",
        "\n",
        "**In Google Colab:**\n",
        "```python\n",
        "from google.colab import userdata\n",
        "userdata.set('HUGGINGFACE_HUB_TOKEN', 'your_token_here')\n",
        "```\n",
        "\n",
        "**Or use Colab Secrets:**\n",
        "1. Click the ğŸ”‘ icon in the left sidebar\n",
        "2. Add secret: `HUGGINGFACE_HUB_TOKEN` = `your_token_here`\n",
        "\n",
        "**Locally (Environment Variable):**\n",
        "```bash\n",
        "export HUGGINGFACE_HUB_TOKEN=your_token_here\n",
        "```\n",
        "\n",
        "**Or create `.env` file:**\n",
        "```\n",
        "HUGGINGFACE_HUB_TOKEN=your_token_here\n",
        "```\n",
        "\n",
        "**Step 3: Verify Authentication**\n",
        "\n",
        "The setup code will automatically:\n",
        "- Try Colab userdata first\n",
        "- Try environment variables\n",
        "- Prompt for manual input if needed\n",
        "- Authenticate with Hugging Face Hub\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>âœ… Setup Verification</b> (Click to expand)</summary>\n",
        "\n",
        "After running the setup cell, verify these are set:\n",
        "\n",
        "**Environment Variables:**\n",
        "- âœ… `IN_COLAB` - True if running in Colab\n",
        "- âœ… `HAS_GPU` - True if GPU is available\n",
        "- âœ… `hf_token` - Your Hugging Face token\n",
        "\n",
        "**Verification Code:**\n",
        "```python\n",
        "# Check setup\n",
        "print(\"ğŸ” Setup Verification:\")\n",
        "print(f\"   IN_COLAB: {IN_COLAB}\")\n",
        "print(f\"   HAS_GPU: {HAS_GPU}\")\n",
        "print(f\"   hf_token: {'âœ… Set' if hf_token else 'âŒ Not set'}\")\n",
        "\n",
        "if HAS_GPU:\n",
        "    import torch\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "```\n",
        "\n",
        "**Expected Output:**\n",
        "```\n",
        "ğŸ” Setup Verification:\n",
        "   IN_COLAB: True\n",
        "   HAS_GPU: True\n",
        "   hf_token: âœ… Set\n",
        "   GPU: Tesla T4\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>âš™ï¸ Setup Functions</b> (Click to expand)</summary>\n",
        "\n",
        "The setup code provides modular functions following SOLID principles:\n",
        "\n",
        "**Available Functions:**\n",
        "\n",
        "| Function | Purpose | Returns |\n",
        "|----------|---------|---------|\n",
        "| `check_environment()` | Detect Colab and GPU availability | `(is_colab, has_gpu)` |\n",
        "| `get_hf_token()` | Retrieve Hugging Face token from various sources | `str` (token) |\n",
        "| `install_packages()` | Install required packages if missing | None |\n",
        "| `import_libraries()` | Import all required libraries with error handling | `bool` (success) |\n",
        "| `authenticate_hf(token)` | Authenticate with Hugging Face Hub | `bool` (success) |\n",
        "\n",
        "**Design Principles:**\n",
        "- **KISS** (Keep It Simple, Stupid) - Each function has single responsibility\n",
        "- **DRY** (Don't Repeat Yourself) - Reusable functions\n",
        "- **SOLID** - Modular, extensible design\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸš€ Quick Start Guide</b> (Click to expand)</summary>\n",
        "\n",
        "**Step-by-Step Setup:**\n",
        "\n",
        "1. **Open Environment**\n",
        "   - Colab: Create new notebook\n",
        "   - Local: Launch Jupyter Notebook\n",
        "\n",
        "2. **Enable GPU (Recommended)**\n",
        "   - Colab: Runtime â†’ Change runtime type â†’ GPU\n",
        "   - Local: Ensure CUDA is installed\n",
        "\n",
        "3. **Run Setup Cell**\n",
        "   - Execute the \"Prerequisites & Setup\" code cell\n",
        "   - Wait for packages to install (~2-3 minutes first time)\n",
        "\n",
        "4. **Authenticate**\n",
        "   - Enter Hugging Face token when prompted\n",
        "   - Or set it in Colab secrets/environment variable\n",
        "\n",
        "5. **Verify**\n",
        "   - Check console output for âœ… marks\n",
        "   - Verify `hf_token` is set\n",
        "   - Confirm GPU is detected (if available)\n",
        "\n",
        "**Expected Setup Time:**\n",
        "- First run: 2-3 minutes (package installation)\n",
        "- Subsequent runs: <30 seconds (packages cached)\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>âš ï¸ Troubleshooting</b> (Click to expand)</summary>\n",
        "\n",
        "**Common Issues:**\n",
        "\n",
        "| Issue | Solution |\n",
        "|-------|----------|\n",
        "| **Token not found** | Check Colab secrets or environment variables. Re-enter token manually. |\n",
        "| **GPU not detected** | In Colab: Runtime â†’ Change runtime type â†’ GPU. Local: Install CUDA toolkit. |\n",
        "| **Package installation fails** | Restart runtime/kernel and try again. Check internet connection. |\n",
        "| **Import errors** | Run `pip install --upgrade <package>` for the failing package. |\n",
        "| **Out of memory** | Use CPU instead of GPU, or restart runtime to clear memory. |\n",
        "\n",
        "**Getting Help:**\n",
        "- Check error messages carefully - they usually indicate the issue\n",
        "- Verify all prerequisites are met\n",
        "- Ensure internet connection is stable\n",
        "- Restart runtime/kernel if issues persist\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ“Š System Requirements</b> (Click to expand)</summary>\n",
        "\n",
        "**Minimum Requirements (CPU):**\n",
        "- Python 3.8+\n",
        "- 8GB RAM\n",
        "- 20GB free disk space (for model downloads)\n",
        "- Stable internet connection\n",
        "\n",
        "**Recommended (GPU):**\n",
        "- Python 3.8+\n",
        "- 16GB+ RAM\n",
        "- NVIDIA GPU with 8GB+ VRAM\n",
        "- 30GB+ free disk space\n",
        "- Fast internet connection\n",
        "\n",
        "**Colab Specifications:**\n",
        "- **Free Tier:** T4 GPU (16GB VRAM), 12GB RAM\n",
        "- **Pro Tier:** V100/A100 GPU, more RAM\n",
        "- **Storage:** 15GB free space\n",
        "\n",
        "**Model Download Sizes:**\n",
        "- Mistral-7B: ~14GB (first download)\n",
        "- QA Models: ~500MB - 3GB each\n",
        "- Embedding Model: ~90MB\n",
        "- **Total:** ~20-25GB for all models\n",
        "\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "**Next Step:** After setup is complete, proceed to **Objective 1: Design System Prompts** to load the Mistral model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PREREQUISITES & SETUP\n",
            "================================================================================\n",
            "\n",
            "ğŸ” Checking environment...\n",
            "   Python version: 3.12.10\n",
            "   âš ï¸  Not running in Google Colab (local environment)\n",
            "   âš ï¸  GPU NOT detected\n",
            "\n",
            "âœ… Hugging Face token loaded from environment!\n",
            "   Token preview: hf_ThdSIol...ustv\n",
            "\n",
            "ğŸ“¦ Installing required packages...\n",
            "   âœ… transformers already installed\n",
            "   âœ… torch already installed\n",
            "   âœ… sentence-transformers already installed\n",
            "   âœ… datasets already installed\n",
            "   â³ Installing python-dotenv...\n",
            "   âœ… python-dotenv installed\n",
            "   â³ Installing faiss-cpu...\n",
            "   âœ… faiss-cpu installed\n",
            "\n",
            "âœ… All required libraries imported successfully!\n",
            "âœ… Authenticated with Hugging Face\n",
            "\n",
            "================================================================================\n",
            "âœ… PREREQUISITES & SETUP COMPLETED!\n",
            "================================================================================\n",
            "\n",
            "ğŸ“Œ Available Globals:\n",
            "   - IN_COLAB: False\n",
            "   - HAS_GPU: False\n",
            "   - hf_token: Set\n",
            "\n",
            "ğŸ’¡ You can now use these functions in other cells:\n",
            "   - check_environment()\n",
            "   - get_hf_token()\n",
            "   - authenticate_hf(token)\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Prerequisites & Setup - Modular Functions\n",
        "# ============================================================================\n",
        "# This cell contains reusable setup functions following KISS, DRY, and SOLID principles\n",
        "# Run this cell FIRST before any other cells\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Function 1: Check Environment (KISS - Single Responsibility)\n",
        "# ----------------------------------------------------------------------------\n",
        "def check_environment():\n",
        "    \"\"\"Check if running in Colab and detect GPU availability.\"\"\"\n",
        "    print(\"ğŸ” Checking environment...\")\n",
        "    print(f\"   Python version: {sys.version.split()[0]}\")\n",
        "    \n",
        "    # Check Colab\n",
        "    try:\n",
        "        import google.colab\n",
        "        is_colab = True\n",
        "        print(\"   âœ… Running in Google Colab\")\n",
        "    except ImportError:\n",
        "        is_colab = False\n",
        "        print(\"   âš ï¸  Not running in Google Colab (local environment)\")\n",
        "    \n",
        "    # Check GPU\n",
        "    try:\n",
        "        import torch\n",
        "        has_gpu = torch.cuda.is_available()\n",
        "        if has_gpu:\n",
        "            print(f\"   âœ… GPU Available: {torch.cuda.get_device_name(0)}\")\n",
        "            print(f\"   âœ… CUDA Version: {torch.version.cuda}\")\n",
        "        else:\n",
        "            print(\"   âš ï¸  GPU NOT detected\")\n",
        "            if is_colab:\n",
        "                print(\"   ğŸ’¡ TIP: Runtime â†’ Change runtime type â†’ Select GPU â†’ Save\")\n",
        "    except ImportError:\n",
        "        has_gpu = False\n",
        "        print(\"   âš ï¸  PyTorch not installed yet\")\n",
        "    \n",
        "    return is_colab, has_gpu\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Function 2: Get Hugging Face Token (DRY - Don't Repeat Yourself)\n",
        "# ----------------------------------------------------------------------------\n",
        "def get_hf_token():\n",
        "    \"\"\"Get Hugging Face token from Colab userdata or environment variable.\"\"\"\n",
        "    # Try Colab userdata first\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        token = userdata.get('HUGGINGFACE_HUB_TOKEN')\n",
        "        if token:\n",
        "            print(\"âœ… Hugging Face token loaded from Colab userdata!\")\n",
        "            print(f\"   Token preview: {token[:10]}...{token[-4:] if len(token) > 14 else '****'}\")\n",
        "            return token\n",
        "    except (ImportError, ValueError):\n",
        "        pass\n",
        "    \n",
        "    # Try environment variable\n",
        "    try:\n",
        "        from dotenv import load_dotenv\n",
        "        load_dotenv()\n",
        "    except ImportError:\n",
        "        pass\n",
        "    \n",
        "    token = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
        "    if token:\n",
        "        print(\"âœ… Hugging Face token loaded from environment!\")\n",
        "        print(f\"   Token preview: {token[:10]}...{token[-4:] if len(token) > 14 else '****'}\")\n",
        "        return token\n",
        "    \n",
        "    # No token found\n",
        "    print(\"âŒ Hugging Face token not found!\")\n",
        "    print(\"   Get your token from: https://huggingface.co/settings/tokens\")\n",
        "    print(\"\\n   In Colab: userdata.set('HUGGINGFACE_HUB_TOKEN', 'your_token')\")\n",
        "    print(\"   Locally: export HUGGINGFACE_HUB_TOKEN=your_token\")\n",
        "    print(\"\\nâš ï¸  Some models may require authentication!\")\n",
        "    return None\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Function 3: Install Packages (SOLID - Open/Closed Principle)\n",
        "# ----------------------------------------------------------------------------\n",
        "def install_packages():\n",
        "    \"\"\"Install required packages if not already installed.\"\"\"\n",
        "    packages = [\n",
        "        \"transformers\",\n",
        "        \"torch\",\n",
        "        \"sentence-transformers\",\n",
        "        \"datasets\",\n",
        "        \"python-dotenv\",\n",
        "        \"faiss-cpu\"\n",
        "    ]\n",
        "    \n",
        "    print(\"ğŸ“¦ Installing required packages...\")\n",
        "    for package in packages:\n",
        "        try:\n",
        "            __import__(package.replace(\"-\", \"_\"))\n",
        "            print(f\"   âœ… {package} already installed\")\n",
        "        except ImportError:\n",
        "            print(f\"   â³ Installing {package}...\")\n",
        "            os.system(f\"pip install -q {package}\")\n",
        "            print(f\"   âœ… {package} installed\")\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Function 4: Import Libraries (KISS - Keep It Simple)\n",
        "# ----------------------------------------------------------------------------\n",
        "def import_libraries():\n",
        "    \"\"\"Import all required libraries with error handling.\"\"\"\n",
        "    try:\n",
        "        from transformers import (\n",
        "            pipeline, \n",
        "            AutoModelForCausalLM, \n",
        "            AutoTokenizer, \n",
        "            logging as transformers_logging\n",
        "        )\n",
        "        from sentence_transformers import SentenceTransformer\n",
        "        import torch\n",
        "        import numpy as np\n",
        "        import faiss\n",
        "        \n",
        "        transformers_logging.set_verbosity_error()\n",
        "        \n",
        "        print(\"âœ… All required libraries imported successfully!\")\n",
        "        return True\n",
        "    except ImportError as e:\n",
        "        print(f\"âŒ Import error: {e}\")\n",
        "        print(\"   Run: pip install transformers torch sentence-transformers faiss-cpu\")\n",
        "        return False\n",
        "    except RuntimeError as e:\n",
        "        if \"register_fake\" in str(e) or \"torch.library\" in str(e):\n",
        "            print(\"âŒ Dependency version mismatch!\")\n",
        "            print(\"   Fix: pip install --upgrade torch torchvision\")\n",
        "            print(\"   Then restart kernel and run this cell again.\")\n",
        "        return False\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Function 5: Authenticate Hugging Face (DRY - Reusable)\n",
        "# ----------------------------------------------------------------------------\n",
        "def authenticate_hf(token=None):\n",
        "    \"\"\"Authenticate with Hugging Face using provided token or global token.\"\"\"\n",
        "    if token is None:\n",
        "        token = globals().get('hf_token')\n",
        "    \n",
        "    if not token:\n",
        "        print(\"âš ï¸  No token provided, skipping authentication\")\n",
        "        return False\n",
        "    \n",
        "    try:\n",
        "        from huggingface_hub import login\n",
        "        login(token=token)\n",
        "        print(\"âœ… Authenticated with Hugging Face\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸  Authentication failed: {e}\")\n",
        "        return False\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Main Setup Execution\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"=\" * 80)\n",
        "print(\"PREREQUISITES & SETUP\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "# Step 1: Check environment\n",
        "IN_COLAB, HAS_GPU = check_environment()\n",
        "globals()['IN_COLAB'] = IN_COLAB\n",
        "globals()['HAS_GPU'] = HAS_GPU\n",
        "print()\n",
        "\n",
        "# Step 2: Get token\n",
        "hf_token = get_hf_token()\n",
        "globals()['hf_token'] = hf_token\n",
        "print()\n",
        "\n",
        "# Step 3: Install packages\n",
        "install_packages()\n",
        "print()\n",
        "\n",
        "# Step 4: Import libraries\n",
        "if import_libraries():\n",
        "    # Step 5: Authenticate if token available\n",
        "    if hf_token:\n",
        "        authenticate_hf(hf_token)\n",
        "    \n",
        "    print()\n",
        "    print(\"=\" * 80)\n",
        "    print(\"âœ… PREREQUISITES & SETUP COMPLETED!\")\n",
        "    print(\"=\" * 80)\n",
        "    print()\n",
        "    print(\"ğŸ“Œ Available Globals:\")\n",
        "    print(f\"   - IN_COLAB: {IN_COLAB}\")\n",
        "    print(f\"   - HAS_GPU: {HAS_GPU}\")\n",
        "    print(f\"   - hf_token: {'Set' if hf_token else 'Not set'}\")\n",
        "    print()\n",
        "    print(\"ğŸ’¡ You can now use these functions in other cells:\")\n",
        "    print(\"   - check_environment()\")\n",
        "    print(\"   - get_hf_token()\")\n",
        "    print(\"   - authenticate_hf(token)\")\n",
        "    print(\"=\" * 80)\n",
        "else:\n",
        "    print(\"âŒ Setup incomplete. Please fix errors above.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Objective 1: Design System Prompts\n",
        "\n",
        "### ğŸ¯ Goal\n",
        "Create a system prompt that defines Mistral's role as a customer service assistant for an e-commerce business context, enabling consistent, context-aware responses aligned with business requirements.\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ“¥ Prerequisites</b> (Click to expand)</summary>\n",
        "\n",
        "| Item | Source | Required | Description |\n",
        "|------|--------|----------|-------------|\n",
        "| `hf_token` | Setup cell (Objective 0) | âœ… Yes | Hugging Face API token for model access |\n",
        "| GPU (recommended) | Colab settings | âš ï¸ Optional | Faster model loading (~1-2 min vs 3-5 min on CPU) |\n",
        "| Python packages | Setup cell | âœ… Yes | `transformers`, `torch`, `huggingface_hub` |\n",
        "\n",
        "**Note:** If running locally without GPU, model loading will be slower but fully functional.\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ”§ Core Concepts</b> (Click to expand)</summary>\n",
        "\n",
        "| Concept | Description |\n",
        "|--------|-------------|\n",
        "| **System Prompt** | Instructions that define the AI's role, personality, constraints, and knowledge boundaries. Acts as the \"constitution\" for LLM behavior. |\n",
        "| **Mistral-7B-Instruct** | Open-source 7 billion parameter model, instruction-tuned for following prompts and generating structured responses. |\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ“Š Design Choices</b> (Click to expand)</summary>\n",
        "\n",
        "| Choice | Selected | Rationale |\n",
        "|--------|----------|-----------|\n",
        "| **Model** | Mistral-7B-Instruct-v0.3 | Open-source, instruction-tuned, good quality, no API costs |\n",
        "| **Precision** | float16 (GPU) / float32 (CPU) | GPU: Memory efficient. CPU: Avoids numerical issues |\n",
        "| **Business Context** | E-commerce Customer Service | Rich domain with clear policies, procedures, and common customer inquiries |\n",
        "| **Prompt Structure** | Multi-section with role, knowledge base, guidelines | Ensures comprehensive coverage of all customer service aspects |\n",
        "\n",
        "**Why This Approach:**\n",
        "Well-defined system prompts ensure consistent, context-aware responses aligned with business requirements. The e-commerce context provides diverse Q&A pairs covering products, shipping, returns, support, etc.\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ”„ Process Flow</b> (Click to expand)</summary>\n",
        "\n",
        "**Left-to-Right Flow:**\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  Authenticateâ”‚â”€â”€â”€â–¶â”‚  Load        â”‚â”€â”€â”€â–¶â”‚  Create      â”‚â”€â”€â”€â–¶â”‚  Test        â”‚â”€â”€â”€â–¶â”‚  Save        â”‚\n",
        "â”‚  with HF     â”‚    â”‚  Mistral     â”‚    â”‚  System      â”‚    â”‚  Generation  â”‚    â”‚  Files       â”‚\n",
        "â”‚              â”‚    â”‚  Model       â”‚    â”‚  Prompt      â”‚    â”‚              â”‚    â”‚              â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "**Detailed Steps:**\n",
        "1. **Authenticate**: Login to Hugging Face with API token\n",
        "2. **Load Model**: Download and load Mistral-7B-Instruct (first run: ~14GB download)\n",
        "3. **Create System Prompt**: Define role, business context, communication guidelines\n",
        "4. **Test Generation**: Generate sample response to verify prompt effectiveness\n",
        "5. **Save Files**: Persist system prompt and test response to disk\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ“¤ Outputs</b> (Click to expand)</summary>\n",
        "\n",
        "| Variable | Type | Description |\n",
        "|----------|------|-------------|\n",
        "| `mistral_model` | `AutoModelForCausalLM` | Loaded Mistral-7B model |\n",
        "| `mistral_tokenizer` | `AutoTokenizer` | Model tokenizer for text processing and encoding |\n",
        "| `system_prompt` | `str` | Customer service system prompt with e-commerce context (~2000 chars) |\n",
        "\n",
        "**Files Created:**\n",
        "| File | Location | Description |\n",
        "|------|----------|-------------|\n",
        "| `system_prompt.txt` | `data/system_prompt_engineering/` | Full system prompt text |\n",
        "| `test_response.txt` | `data/system_prompt_engineering/` | Generated response to test question |\n",
        "\n",
        "**Key Functions Created:**\n",
        "- `load_mistral_model()`: Loads and caches Mistral model\n",
        "- `create_system_prompt()`: Generates optimized system prompt\n",
        "- `format_mistral_prompt()`: Formats prompt for Mistral Instruct template\n",
        "- `generate_response()`: Generates text using Mistral with system prompt\n",
        "- `save_system_prompt()`: Saves system prompt to file\n",
        "- `save_response()`: Saves generated response to file\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ“‹ System Prompt Components</b> (Click to expand)</summary>\n",
        "\n",
        "The system prompt includes:\n",
        "\n",
        "| Component | Content |\n",
        "|-----------|---------|\n",
        "| **Role Definition** | Customer service assistant for e-commerce platform |\n",
        "| **Business Context** | Company name, product categories, policies |\n",
        "| **Knowledge Base** | Products, shipping, returns, customer service hours, contact methods |\n",
        "| **Communication Guidelines** | Tone (warm, professional), style (clear, concise) |\n",
        "| **Limitation Handling** | Instructions for unknown information (\"I don't have that information\") |\n",
        "| **Response Format** | Structure expectations for consistent outputs |\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>â–¶ï¸ How to Run</b> (Click to expand)</summary>\n",
        "\n",
        "1. **Ensure Setup Complete**: \n",
        "   - Run Objective 0 (Setup) first\n",
        "   - Verify `hf_token` is set\n",
        "   - Check GPU availability (optional but recommended)\n",
        "\n",
        "2. **Run the Code Cell**: \n",
        "   - Execute the Objective 1 code cell\n",
        "   - First run will download ~14GB model (takes 5-10 minutes)\n",
        "   - Subsequent runs use cached model (instant)\n",
        "\n",
        "3. **Verify Output**: \n",
        "   - Check console for: \"âœ… Loaded on GPU (CUDA)\" or \"âœ… Loaded on CPU\"\n",
        "   - Verify system prompt is created\n",
        "   - Test response should be generated\n",
        "   - Files saved to `data/system_prompt_engineering/`\n",
        "\n",
        "**Expected Runtime:**\n",
        "- GPU: 1-2 minutes (after initial download)\n",
        "- CPU: 3-5 minutes (after initial download)\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>âœ… Verification</b> (Click to expand)</summary>\n",
        "\n",
        "After running, verify these exist:\n",
        "\n",
        "```python\n",
        "verify_objective1()\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ”— Dependencies</b> (Click to expand)</summary>\n",
        "\n",
        "**This Objective:**\n",
        "- âœ… Requires Objective 0 (Setup) for `hf_token`\n",
        "\n",
        "**Used By:**\n",
        "- Objective 2: Generates Q&A pairs using `mistral_model` and `system_prompt`\n",
        "- Objective 4: Uses `mistral_model` for RAG answer generation\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ“š Learning Objectives Demonstrated</b> (Click to expand)</summary>\n",
        "\n",
        "1. **System Prompt Engineering**: Crafting prompts that shape LLM behavior\n",
        "2. **Modular Design**: SOLID, KISS, DRY principles in practice\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ’¡ Tips</b> (Click to expand)</summary>\n",
        "\n",
        "- **First Run**: Be patient - model download is large (~14GB)\n",
        "- **GPU Recommended**: Significantly faster inference (2-3x speedup)\n",
        "- **Prompt Tuning**: Experiment with different prompt structures to optimize responses\n",
        "- **Caching**: Model is cached globally - subsequent runs are instant\n",
        "\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "**Next Step:** Proceed to Objective 2 to generate Q&A database using the system prompt.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "   OBJECTIVE 1: DESIGN SYSTEM PROMPTS\n",
            "   Demonstrating Prompt Engineering\n",
            "======================================================================\n",
            "\n",
            "ğŸ”‘ STEP 1: Authenticate with Hugging Face\n",
            "----------------------------------------------------------------------\n",
            "âœ… Authenticated with Hugging Face\n",
            "\n",
            "ğŸ¤– STEP 2: Load Mistral Model\n",
            "----------------------------------------------------------------------\n",
            "   â„¹ï¸  Reusing cached model instance\n",
            "\n",
            "ğŸ“ STEP 3: Create Optimized System Prompt\n",
            "----------------------------------------------------------------------\n",
            "   âœ… Created (1995 chars, 57 lines)\n",
            "\n",
            "ğŸ“„ System Prompt Preview (first 500 chars):\n",
            "----------------------------------------------------------------------\n",
            "You are a friendly and knowledgeable customer service assistant for GreenTech Marketplace, a leading e-commerce platform specializing in sustainable technology products.\n",
            "\n",
            "## YOUR ROLE\n",
            "You are the first point of contact for customers. Your expertise includes product information, orders, shipping, returns, and general inquiries. You embody the company's commitment to sustainability and excellent service.\n",
            "\n",
            "## KNOWLEDGE BASE\n",
            "\n",
            "**Products:**\n",
            "- Solar panels, energy-efficient appliances, smart home devi...\n",
            "\n",
            "ğŸ§ª STEP 4: Test with Sample Question\n",
            "----------------------------------------------------------------------\n",
            "   Question: What are your store hours and how can I contact customer support?\n",
            "\n",
            "   â³ Generating response...\n",
            "\n",
            "======================================================================\n",
            "ğŸ“¥ GENERATED RESPONSE:\n",
            "======================================================================\n",
            "Greetings!\n",
            "\n",
            "I'm thrilled to assist you with your inquiry.\n",
            "\n",
            "Our customer support hours are as follows:\n",
            "- Monday-Friday: 9 AM - 6 PM EST\n",
            "- Saturday: 10 AM - 4 PM EST\n",
            "- Closed Sunday\n",
            "\n",
            "You can reach us during these hours via email at support@greentechmarketplace.com, by phone at 1-800-GREEN-TECH, or through our live chat service.\n",
            "\n",
            "We're always here to help, and I'm looking forward to assisting you further. If you have any other questions or need additional information, please don't hesitate to ask!\n",
            "\n",
            "Best regards,\n",
            "[Your Name]\n",
            "GreenTech Marketplace Customer Support\n",
            "======================================================================\n",
            "\n",
            "ğŸ’¾ STEP 5: Save Files\n",
            "----------------------------------------------------------------------\n",
            "   âœ… System prompt saved to: data/system_prompt_engineering/system_prompt.txt\n",
            "   âœ… Response saved to: data/system_prompt_engineering/test_response.txt\n",
            "\n",
            "âœ… STEP 6: Verify Objective 1\n",
            "----------------------------------------------------------------------\n",
            "======================================================================\n",
            "ğŸ” OBJECTIVE 1 VERIFICATION\n",
            "======================================================================\n",
            "\n",
            "âœ… Objective 1 Complete - All variables and files verified\n",
            "   â€¢ Model: Loaded on CPU\n",
            "   â€¢ System Prompt: 1995 characters\n",
            "   â€¢ Tokenizer: Ready\n",
            "   â€¢ Files: Saved to data/system_prompt_engineering/\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "âœ… OBJECTIVE 1 COMPLETE\n",
            "======================================================================\n",
            "\n",
            "Key Concepts Demonstrated:\n",
            "  1. System Prompt Engineering - Structured prompt with role, context, guidelines\n",
            "  2. Modular Design - Reusable functions following SOLID/DRY/KISS\n",
            "\n",
            "ğŸ“¦ FILES SAVED:\n",
            "  â€¢ data/system_prompt_engineering/system_prompt.txt\n",
            "  â€¢ data/system_prompt_engineering/test_response.txt\n",
            "\n",
            "Available Functions:\n",
            "  â€¢ load_mistral_model()     - Load and cache Mistral model\n",
            "  â€¢ create_system_prompt()   - Generate customer service prompt\n",
            "  â€¢ format_mistral_prompt()  - Format for Mistral Instruct template\n",
            "  â€¢ generate_response()      - Generate text with error handling\n",
            "  â€¢ save_system_prompt()     - Save prompt to file\n",
            "  â€¢ save_response()          - Save response to file\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# OBJECTIVE 1: DESIGN SYSTEM PROMPTS FOR LLM-BASED CUSTOMER SERVICE\n",
        "# ============================================================================\n",
        "#\n",
        "# LEARNING OBJECTIVES DEMONSTRATED:\n",
        "#   1. System Prompt Engineering - Crafting prompts that shape LLM behavior\n",
        "#   2. Modular Design - SOLID, KISS, DRY principles in practice\n",
        "#\n",
        "# THEORETICAL BACKGROUND:\n",
        "#   System prompts serve as the \"constitution\" for LLM behavior, establishing:\n",
        "#   - Role Identity: Who the model should act as\n",
        "#   - Knowledge Boundaries: What the model knows and doesn't know\n",
        "#   - Behavioral Constraints: Response style, escalation rules\n",
        "#   - Domain Context: Business-specific information\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 1: IMPORTS & ENVIRONMENT VALIDATION\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "    from huggingface_hub import login\n",
        "except ImportError as e:\n",
        "    raise ImportError(f\"Missing: {e}. Run: pip install torch transformers huggingface_hub\")\n",
        "\n",
        "if 'authenticate_hf' not in globals():\n",
        "    def authenticate_hf(token=None):\n",
        "        \"\"\"Authenticate with Hugging Face Hub.\"\"\"\n",
        "        token = token or globals().get('hf_token')\n",
        "        if token:\n",
        "            login(token=token)\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 2: CONFIGURATION CONSTANTS\n",
        "# ============================================================================\n",
        "\n",
        "# Model configuration\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "\n",
        "# Generation parameters\n",
        "MAX_NEW_TOKENS = 200      # Maximum tokens to generate per response\n",
        "TEMPERATURE = 0.7         # Randomness: 0=deterministic, 1=creative\n",
        "TOP_P = 0.9               # Nucleus sampling: considers top 90% probability mass\n",
        "\n",
        "# Output directory\n",
        "OUTPUT_DIR = \"data/system_prompt_engineering\"\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 3: CORE FUNCTIONS - MODEL MANAGEMENT\n",
        "# ============================================================================\n",
        "\n",
        "def load_mistral_model(model_name=MODEL_NAME, force_reload=False):\n",
        "    \"\"\"\n",
        "    Load Mistral model with caching support.\n",
        "    \n",
        "    Design Decisions:\n",
        "    - float16 on GPU for memory efficiency\n",
        "    - float32 on CPU (float16 can cause numerical issues)\n",
        "    - Global caching to avoid redundant loads (DRY principle)\n",
        "    \n",
        "    Args:\n",
        "        model_name: HuggingFace model identifier\n",
        "        force_reload: If True, reload even if cached\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (tokenizer, model)\n",
        "        \n",
        "    Raises:\n",
        "        RuntimeError: If model loading fails\n",
        "    \"\"\"\n",
        "    if not force_reload and 'mistral_tokenizer' in globals() and 'mistral_model' in globals():\n",
        "        print(\"   â„¹ï¸  Reusing cached model instance\")\n",
        "        return globals()['mistral_tokenizer'], globals()['mistral_model']\n",
        "    \n",
        "    print(f\"   Loading: {model_name}\")\n",
        "    print(\"   â³ First run downloads ~14GB model...\")\n",
        "    \n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        \n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "            low_cpu_mem_usage=True\n",
        "        )\n",
        "        \n",
        "        if not torch.cuda.is_available():\n",
        "            model = model.to(\"cpu\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Failed to load model: {e}\")\n",
        "    \n",
        "    globals()['mistral_tokenizer'] = tokenizer\n",
        "    globals()['mistral_model'] = model\n",
        "    \n",
        "    device = 'GPU (CUDA)' if torch.cuda.is_available() else 'CPU'\n",
        "    print(f\"   âœ… Loaded on {device}\")\n",
        "    \n",
        "    return tokenizer, model\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 4: CORE FUNCTIONS - PROMPT ENGINEERING\n",
        "# ============================================================================\n",
        "#\n",
        "# Best Practices for System Prompts:\n",
        "#   1. Clear Role Definition - Explicit identity and expertise\n",
        "#   2. Comprehensive Business Context - All relevant facts\n",
        "#   3. Detailed Communication Guidelines - Tone, style, format\n",
        "#   4. Explicit Limitation Handling - What to do when uncertain\n",
        "#   5. Response Format Instructions - Structure expectations\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "def create_system_prompt(business_name=\"GreenTech Marketplace\", \n",
        "                         business_type=\"e-commerce\",\n",
        "                         support_email=\"support@greentechmarketplace.com\",\n",
        "                         support_phone=\"1-800-GREEN-TECH\"):\n",
        "    \"\"\"\n",
        "    Create an optimized customer service system prompt.\n",
        "    \n",
        "    Prompt Engineering Best Practices Applied:\n",
        "    1. Role Definition: Clear, specific identity with expertise level\n",
        "    2. Task Boundaries: Explicit scope of responsibilities\n",
        "    3. Knowledge Base: Comprehensive domain information\n",
        "    4. Behavioral Rules: Specific guidelines for tone and style\n",
        "    5. Edge Case Handling: Instructions for unknowns/limitations\n",
        "    6. Output Format: Clear expectations for response structure\n",
        "    \n",
        "    Args:\n",
        "        business_name: Company name to use in prompt\n",
        "        business_type: Type of business (e.g., \"e-commerce\")\n",
        "        support_email: Customer support email address\n",
        "        support_phone: Customer support phone number\n",
        "        \n",
        "    Returns:\n",
        "        str: Formatted system prompt\n",
        "    \"\"\"\n",
        "    return f\"\"\"You are a friendly and knowledgeable customer service assistant for {business_name}, a leading {business_type} platform specializing in sustainable technology products.\n",
        "\n",
        "## YOUR ROLE\n",
        "You are the first point of contact for customers. Your expertise includes product information, orders, shipping, returns, and general inquiries. You embody the company's commitment to sustainability and excellent service.\n",
        "\n",
        "## KNOWLEDGE BASE\n",
        "\n",
        "**Products:**\n",
        "- Solar panels, energy-efficient appliances, smart home devices, eco-friendly accessories\n",
        "- Warranty: 1-3 years depending on product category\n",
        "\n",
        "**Shipping:**\n",
        "- Standard: 5-7 business days (free over $75)\n",
        "- Express: 2-3 business days (additional fee)\n",
        "\n",
        "**Returns & Refunds:**\n",
        "- 30-day return policy for unopened items in original packaging\n",
        "- Refunds processed within 5-7 business days after receipt\n",
        "\n",
        "**Customer Service Hours:**\n",
        "- Monday-Friday: 9 AM - 6 PM EST\n",
        "- Saturday: 10 AM - 4 PM EST\n",
        "- Closed Sunday\n",
        "\n",
        "**Contact Methods:**\n",
        "- Email: {support_email}\n",
        "- Phone: {support_phone}\n",
        "- Live chat: Available during business hours in EST \n",
        "\n",
        "## COMMUNICATION GUIDELINES\n",
        "\n",
        "**Tone:** Warm, professional, solution-oriented\n",
        "**Style:** Clear, concise, helpful\n",
        "\n",
        "**Always:**\n",
        "- Greet customers warmly\n",
        "- Acknowledge their concerns before providing solutions\n",
        "- Provide specific, actionable information\n",
        "- Include relevant timeframes and next steps\n",
        "- Thank them for choosing {business_name}\n",
        "\n",
        "**Never:**\n",
        "- Make promises you cannot keep\n",
        "- Provide information not in your knowledge base\n",
        "- Use technical jargon without explanation\n",
        "\n",
        "## HANDLING LIMITATIONS\n",
        "\n",
        "If you don't know the answer:\n",
        "1. Acknowledge the question honestly\n",
        "2. Explain that the information is not in your current knowledge base\n",
        "3. Offer to connect them with a specialist or provide contact information\n",
        "4. Suggest alternative resources if available\n",
        "\n",
        "## RESPONSE FORMAT\n",
        "\n",
        "Keep responses concise but complete. Structure longer responses with clear sections. Always end with an offer to help further.\"\"\"\n",
        "\n",
        "\n",
        "def format_mistral_prompt(system_prompt, user_input):\n",
        "    \"\"\"\n",
        "    Format for Mistral Instruct template.\n",
        "    \n",
        "    Template: <s>[INST] {system} {user} [/INST]\n",
        "    \n",
        "    Args:\n",
        "        system_prompt: The system prompt defining assistant behavior\n",
        "        user_input: The user's question or message\n",
        "        \n",
        "    Returns:\n",
        "        str: Formatted prompt string\n",
        "    \"\"\"\n",
        "    return f\"<s>[INST] {system_prompt}\\n\\nCustomer Question: {user_input} [/INST]\"\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 5: CORE FUNCTIONS - GENERATION\n",
        "# ============================================================================\n",
        "\n",
        "def generate_response(tokenizer, model, formatted_prompt, \n",
        "                      max_new_tokens=MAX_NEW_TOKENS, \n",
        "                      temperature=TEMPERATURE, \n",
        "                      top_p=TOP_P):\n",
        "    \"\"\"\n",
        "    Generate model response.\n",
        "    \n",
        "    Args:\n",
        "        tokenizer: Model tokenizer\n",
        "        model: Loaded model instance\n",
        "        formatted_prompt: Prompt formatted with format_mistral_prompt()\n",
        "        max_new_tokens: Maximum tokens to generate\n",
        "        temperature: Sampling temperature (0=deterministic, 1=creative)\n",
        "        top_p: Nucleus sampling threshold\n",
        "        \n",
        "    Returns:\n",
        "        str: Generated response text\n",
        "        \n",
        "    Raises:\n",
        "        RuntimeError: If generation fails (e.g., OOM error)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        inputs = tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                temperature=temperature,\n",
        "                do_sample=True,\n",
        "                top_p=top_p,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        # Decode only the NEW tokens (not the input)\n",
        "        input_length = inputs['input_ids'].shape[1]\n",
        "        generated_tokens = outputs[0][input_length:]\n",
        "        response = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
        "        \n",
        "        return response\n",
        "        \n",
        "    except torch.cuda.OutOfMemoryError:\n",
        "        raise RuntimeError(\"GPU out of memory. Try reducing max_new_tokens or use CPU.\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Generation failed: {e}\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 6: FILE STORAGE UTILITIES\n",
        "# ============================================================================\n",
        "\n",
        "def save_system_prompt(system_prompt, output_dir=OUTPUT_DIR, filename=\"system_prompt.txt\"):\n",
        "    \"\"\"\n",
        "    Save system prompt to text file.\n",
        "    \n",
        "    Args:\n",
        "        system_prompt: The system prompt text\n",
        "        output_dir: Directory to save to\n",
        "        filename: Output filename\n",
        "        \n",
        "    Returns:\n",
        "        str: Path to saved file\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    filepath = os.path.join(output_dir, filename)\n",
        "    with open(filepath, 'w') as f:\n",
        "        f.write(system_prompt)\n",
        "    print(f\"   âœ… System prompt saved to: {filepath}\")\n",
        "    return filepath\n",
        "\n",
        "\n",
        "def save_response(response, question, output_dir=OUTPUT_DIR, filename=\"test_response.txt\"):\n",
        "    \"\"\"\n",
        "    Save generated response to text file.\n",
        "    \n",
        "    Args:\n",
        "        response: The generated response text\n",
        "        question: The question that was asked\n",
        "        output_dir: Directory to save to\n",
        "        filename: Output filename\n",
        "        \n",
        "    Returns:\n",
        "        str: Path to saved file\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    filepath = os.path.join(output_dir, filename)\n",
        "    with open(filepath, 'w') as f:\n",
        "        f.write(f\"Question: {question}\\n\")\n",
        "        f.write(f\"\\n{'='*50}\\n\\n\")\n",
        "        f.write(f\"Response:\\n{response}\")\n",
        "    print(f\"   âœ… Response saved to: {filepath}\")\n",
        "    return filepath\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "def verify_objective1():\n",
        "    \"\"\"\n",
        "    Verify that Objective 1 completed successfully.\n",
        "    Checks all variables, files, and model loading.\n",
        "    \"\"\"\n",
        "    import os\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    print(\"ğŸ” OBJECTIVE 1 VERIFICATION\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    errors = []\n",
        "    \n",
        "    # Check if variables exist\n",
        "    if 'mistral_model' not in globals():\n",
        "        errors.append(\"âŒ mistral_model not found\")\n",
        "    if 'mistral_tokenizer' not in globals():\n",
        "        errors.append(\"âŒ mistral_tokenizer not found\")\n",
        "    if 'system_prompt' not in globals():\n",
        "        errors.append(\"âŒ system_prompt not found\")\n",
        "    \n",
        "    if errors:\n",
        "        print(\"\\n\".join(errors))\n",
        "        print(\"=\"*70)\n",
        "        return False\n",
        "    \n",
        "    # Check system prompt\n",
        "    if len(system_prompt) < 100:\n",
        "        errors.append(f\"âŒ System prompt too short ({len(system_prompt)} chars)\")\n",
        "    \n",
        "    # Check files exist\n",
        "    if not os.path.exists(\"data/system_prompt_engineering/system_prompt.txt\"):\n",
        "        errors.append(\"âŒ system_prompt.txt not found\")\n",
        "    if not os.path.exists(\"data/system_prompt_engineering/test_response.txt\"):\n",
        "        errors.append(\"âŒ test_response.txt not found\")\n",
        "    \n",
        "    # Check model is loaded\n",
        "    try:\n",
        "        import torch\n",
        "        device = 'GPU (CUDA)' if torch.cuda.is_available() else 'CPU'\n",
        "        model_loaded = mistral_model is not None and mistral_tokenizer is not None\n",
        "        if not model_loaded:\n",
        "            errors.append(\"âŒ Model or tokenizer not properly loaded\")\n",
        "    except Exception as e:\n",
        "        errors.append(f\"âŒ Error checking model: {e}\")\n",
        "    \n",
        "    # Print results\n",
        "    if errors:\n",
        "        print(\"\\nâŒ VERIFICATION FAILED:\")\n",
        "        print(\"\\n\".join(errors))\n",
        "        print(\"=\"*70)\n",
        "        return False\n",
        "    else:\n",
        "        print(\"\\nâœ… Objective 1 Complete - All variables and files verified\")\n",
        "        print(f\"   â€¢ Model: Loaded on {device}\")\n",
        "        print(f\"   â€¢ System Prompt: {len(system_prompt)} characters\")\n",
        "        print(f\"   â€¢ Tokenizer: Ready\")\n",
        "        print(f\"   â€¢ Files: Saved to data/system_prompt_engineering/\")\n",
        "        print(\"=\"*70)\n",
        "        return True\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# SECTION 7: EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"   OBJECTIVE 1: DESIGN SYSTEM PROMPTS\")\n",
        "print(\"   Demonstrating Prompt Engineering\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# --- Step 1: Authentication ---\n",
        "print(\"\\nğŸ”‘ STEP 1: Authenticate with Hugging Face\")\n",
        "print(\"-\"*70)\n",
        "authenticate_hf()\n",
        "\n",
        "# --- Step 2: Load Model ---\n",
        "print(\"\\nğŸ¤– STEP 2: Load Mistral Model\")\n",
        "print(\"-\"*70)\n",
        "tokenizer, model = load_mistral_model()\n",
        "\n",
        "# --- Step 3: Create System Prompt ---\n",
        "print(\"\\nğŸ“ STEP 3: Create Optimized System Prompt\")\n",
        "print(\"-\"*70)\n",
        "system_prompt = create_system_prompt()\n",
        "print(f\"   âœ… Created ({len(system_prompt)} chars, {len(system_prompt.split(chr(10)))} lines)\")\n",
        "print(\"\\nğŸ“„ System Prompt Preview (first 500 chars):\")\n",
        "print(\"-\"*70)\n",
        "print(system_prompt[:500] + \"...\")\n",
        "\n",
        "globals()['system_prompt'] = system_prompt\n",
        "\n",
        "# --- Step 4: Test Generation ---\n",
        "print(\"\\nğŸ§ª STEP 4: Test with Sample Question\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "test_question = \"What are your store hours and how can I contact customer support?\"\n",
        "print(f\"   Question: {test_question}\\n\")\n",
        "\n",
        "formatted_prompt = format_mistral_prompt(system_prompt, test_question)\n",
        "print(\"   â³ Generating response...\")\n",
        "\n",
        "generated_response = generate_response(tokenizer, model, formatted_prompt)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸ“¥ GENERATED RESPONSE:\")\n",
        "print(\"=\"*70)\n",
        "print(generated_response)\n",
        "print(\"=\"*70)\n",
        "\n",
        "# --- Step 5: Save Files ---\n",
        "print(\"\\nğŸ’¾ STEP 5: Save Files\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "save_system_prompt(system_prompt)\n",
        "save_response(generated_response, test_question)\n",
        "\n",
        "# --- Step 6: Verify Objective 1 ---\n",
        "print(\"\\nâœ… STEP 6: Verify Objective 1\")\n",
        "print(\"-\"*70)\n",
        "verify_objective1()\n",
        "\n",
        "\n",
        "# --- Summary ---\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âœ… OBJECTIVE 1 COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\"\"\n",
        "Key Concepts Demonstrated:\n",
        "  1. System Prompt Engineering - Structured prompt with role, context, guidelines\n",
        "  2. Modular Design - Reusable functions following SOLID/DRY/KISS\n",
        "\n",
        "ğŸ“¦ FILES SAVED:\n",
        "  â€¢ {OUTPUT_DIR}/system_prompt.txt\n",
        "  â€¢ {OUTPUT_DIR}/test_response.txt\n",
        "\n",
        "Available Functions:\n",
        "  â€¢ load_mistral_model()     - Load and cache Mistral model\n",
        "  â€¢ create_system_prompt()   - Generate customer service prompt\n",
        "  â€¢ format_mistral_prompt()  - Format for Mistral Instruct template\n",
        "  â€¢ generate_response()      - Generate text with error handling\n",
        "  â€¢ save_system_prompt()     - Save prompt to file\n",
        "  â€¢ save_response()          - Save response to file\n",
        "\"\"\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Objective 2: Generate Custom Q&A Databases for E-commerce Customer Service\n",
        "\n",
        "### ğŸ¯ Goal\n",
        "Generate 21 Q&A pairs (18 answerable + 3 unanswerable) covering e-commerce customer service topics to create a domain-specific knowledge base for the RAG system.\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ“¥ Prerequisites</b> (Click to expand)</summary>\n",
        "\n",
        "| Item | Source | Required | Description |\n",
        "|------|--------|----------|-------------|\n",
        "| `mistral_model` | Objective 1 | âœ… Yes | Mistral model for generating Q&A pairs |\n",
        "| `mistral_tokenizer` | Objective 1 | âœ… Yes | Tokenizer for text processing and encoding |\n",
        "| `system_prompt` | Objective 1 | âœ… Yes | System prompt defining business context and role |\n",
        "| `hf_token` | Objective 0 | âœ… Yes | Hugging Face API token for model access |\n",
        "\n",
        "**Note:** Objective 1 must be completed first to provide the model and system prompt.\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ”§ Core Concepts</b> (Click to expand)</summary>\n",
        "\n",
        "| Concept | Description |\n",
        "|--------|-------------|\n",
        "| **Q&A Generation** | Using LLMs to create domain-specific question-answer pairs that serve as the knowledge base for RAG systems |\n",
        "| **Zero-Shot Prompting** | Providing instructions without examples - the model generates Q&A pairs based solely on the system prompt and task description |\n",
        "| **Few-Shot Prompting** | Providing examples in the prompt to guide the model's output format and style - improves consistency and quality |\n",
        "| **System Prompt Reuse** | Leveraging the system prompt from Objective 1 to ensure Q&A pairs align with the business context and customer service role |\n",
        "| **Answerable Questions** | Questions that can be answered from the business knowledge base (products, policies, procedures) |\n",
        "| **Unanswerable Questions** | Questions outside the knowledge base scope (competitor info, personal advice, future events) |\n",
        "| **Delimiter Parsing** | Using a consistent delimiter (`|||`) to reliably extract structured data from LLM-generated text |\n",
        "| **DataFrame Structure** | Converting Q&A pairs to pandas DataFrame for easy filtering, querying, and analysis |\n",
        "\n",
        "**Why This Matters:**\n",
        "A well-structured Q&A database is the foundation of the RAG system. It provides the knowledge that will be retrieved and used to generate accurate, context-aware responses. Using the system prompt from Objective 1 ensures consistency with the customer service role and business context.\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ“Š Design Choices</b> (Click to expand)</summary>\n",
        "\n",
        "| Choice | Selected | Rationale |\n",
        "|--------|----------|-----------|\n",
        "| **Prompt Technique** | Zero-Shot with System Prompt | Uses the system prompt from Objective 1 to provide business context, ensuring Q&A pairs align with customer service role and domain |\n",
        "| **System Prompt Integration** | Reuse from Objective 1 | The `system_prompt` variable is embedded in generation prompts to maintain consistency with business context, role definition, and knowledge boundaries |\n",
        "| **Total Q&A Pairs** | 21 (18 answerable + 3 unanswerable) | Sufficient coverage for testing while manageable in size |\n",
        "| **Answerable Categories** | 6 categories Ã— 3 pairs = 18 | Products, shipping, returns, customer service, warranty, orders - core business areas |\n",
        "| **Unanswerable Types** | 3 types Ã— 1 pair = 3 | Competitor, personal advice, future events - tests system limitations |\n",
        "| **Data Format** | List of dictionaries + DataFrame | Flexible access patterns (list for iteration, DataFrame for filtering) |\n",
        "| **Delimiter** | `|||` | Uncommon in natural text, reliable for parsing LLM output |\n",
        "| **Answerable Flag** | Boolean (`True`/`False`) | Enables easy filtering and testing of system's ability to decline appropriately |\n",
        "\n",
        "**Prompt Engineering Approach:**\n",
        "- **Zero-Shot Technique**: Instructions are provided without examples, relying on the model's pre-training and the system prompt's context\n",
        "- **System Prompt Embedding**: The first 1200 characters of `system_prompt` from Objective 1 are included in each generation prompt to ensure:\n",
        "  - Business context (GreenTech Marketplace, e-commerce)\n",
        "  - Role definition (customer service assistant)\n",
        "  - Knowledge boundaries (what can/cannot be answered)\n",
        "  - Communication style (warm, professional, helpful)\n",
        "- **Format Instructions**: Clear delimiter-based format (`category ||| question ||| answer`) ensures reliable parsing\n",
        "\n",
        "**Why This Approach:**\n",
        "- **System Prompt Reuse**: Ensures Q&A pairs are consistent with the customer service role and business context defined in Objective 1\n",
        "- **Zero-Shot Efficiency**: Faster than few-shot (no example formatting needed) while still producing quality results\n",
        "- **21 pairs**: Balances comprehensiveness with generation time and evaluation complexity\n",
        "- **6 answerable categories**: Covers all major customer service touchpoints\n",
        "- **3 unanswerable types**: Tests system's ability to recognize and handle out-of-scope questions\n",
        "- **Dual format**: List for programmatic access, DataFrame for analysis and filtering\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ”„ Process Flow</b> (Click to expand)</summary>\n",
        "\n",
        "**Left-to-Right Flow:**\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  Validate    â”‚â”€â”€â”€â–¶â”‚  Generate    â”‚â”€â”€â”€â–¶â”‚  Convert to  â”‚â”€â”€â”€â–¶â”‚  Display     â”‚â”€â”€â”€â–¶â”‚  Save to     â”‚\n",
        "â”‚ Prerequisitesâ”‚    â”‚  Q&A Pairs   â”‚    â”‚  DataFrame   â”‚    â”‚  Statistics  â”‚    â”‚  CSV Files   â”‚\n",
        "â”‚              â”‚    â”‚  (21 total)  â”‚    â”‚              â”‚    â”‚              â”‚    â”‚              â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "**Detailed Steps:**\n",
        "1. **Validate Prerequisites**: Ensure Objective 1 completed (model, tokenizer, system prompt available)\n",
        "2. **Generate Q&A Pairs**: Use Mistral with zero-shot prompting, embedding the system prompt from Objective 1 to generate:\n",
        "   - 18 answerable pairs (6 categories Ã— 3) using business context from system prompt\n",
        "   - 3 unanswerable pairs (3 types Ã— 1) to test system limitations\n",
        "3. **Create DataFrame**: Convert list of dictionaries to pandas DataFrame with computed metrics\n",
        "4. **Display Statistics**: Show coverage, distribution, and quality metrics\n",
        "5. **Display All Q&A Pairs**: Print all pairs with comments for documentation\n",
        "6. **Save to CSV**: Persist Q&A database to `data/qa_database/qa_database.csv`\n",
        "\n",
        "**Prompt Technique Details:**\n",
        "- **Zero-Shot Approach**: Each generation prompt includes:\n",
        "  - First 1200 characters of `system_prompt` from Objective 1 (business context, role, knowledge base)\n",
        "  - Category-specific instructions\n",
        "  - Format requirements (delimiter-based structure)\n",
        "- **System Prompt Integration**: The system prompt ensures generated Q&A pairs:\n",
        "  - Align with customer service role\n",
        "  - Use correct business terminology\n",
        "  - Follow communication guidelines\n",
        "  - Respect knowledge boundaries\n",
        "\n",
        "**Category Breakdown:**\n",
        "- **Answerable (18 pairs)**: products (3), shipping (3), returns (3), customer_service (3), warranty (3), orders (3)\n",
        "- **Unanswerable (3 pairs)**: competitor (1), personal_advice (1), future_events (1)\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ“¤ Outputs</b> (Click to expand)</summary>\n",
        "\n",
        "| Variable | Type | Description |\n",
        "|----------|------|-------------|\n",
        "| `qa_database` | `List[Dict]` | List of 21 Q&A dictionaries with keys: `category`, `answerable`, `question`, `answer` |\n",
        "| `qa_df` | `pd.DataFrame` | DataFrame with Q&A pairs plus computed columns: `answer_length`, `word_count` |\n",
        "\n",
        "**Files Created:**\n",
        "| File | Location | Description |\n",
        "|------|----------|-------------|\n",
        "| `qa_database.csv` | `data/qa_database/` | All 21 Q&A pairs with answerable flag, ready for RAG system |\n",
        "\n",
        "**Key Functions Created:**\n",
        "- `generate_qa_for_category()`: Generates Q&A pairs for a specific category\n",
        "- `generate_full_qa_database()`: Generates complete 21-pair database\n",
        "- `qa_to_dataframe()`: Converts Q&A list to DataFrame with metrics\n",
        "- `display_statistics()`: Shows database coverage and quality metrics\n",
        "- `display_all_qa_pairs()`: Prints all Q&A pairs with comments\n",
        "- `save_qa_to_csv()`: Saves Q&A database to CSV file\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ“‹ Q&A Database Structure</b> (Click to expand)</summary>\n",
        "\n",
        "**Answerable Categories (18 pairs):**\n",
        "\n",
        "| Category | Count | Topics Covered |\n",
        "|----------|-------|---------------|\n",
        "| **products** | 3 | Types of products, specifications, features |\n",
        "| **shipping** | 3 | Delivery times, shipping costs, free shipping threshold, tracking |\n",
        "| **returns** | 3 | Return policy, refund process, conditions, 30-day policy |\n",
        "| **customer_service** | 3 | Business hours (Mon-Fri 9AM-6PM, Sat 10AM-4PM), contact email and phone |\n",
        "| **warranty** | 3 | Warranty duration (1-3 years), coverage, claims process |\n",
        "| **orders** | 3 | Order status, tracking, modifications, cancellations |\n",
        "\n",
        "**Unanswerable Types (3 pairs):**\n",
        "\n",
        "| Type | Count | Description |\n",
        "|------|-------|-------------|\n",
        "| **competitor** | 1 | Questions about competitor pricing, products, or comparisons |\n",
        "| **personal_advice** | 1 | Questions asking for personal recommendations or opinions |\n",
        "| **future_events** | 1 | Questions about future sales, unreleased products, or predictions |\n",
        "\n",
        "**Data Structure:**\n",
        "```python\n",
        "{\n",
        "    'category': 'shipping',\n",
        "    'answerable': True,\n",
        "    'question': 'What is your shipping policy?',\n",
        "    'answer': 'We offer standard shipping (5-7 business days, free over $75)...'\n",
        "}\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>â–¶ï¸ How to Run</b> (Click to expand)</summary>\n",
        "\n",
        "1. **Ensure Prerequisites Complete**: \n",
        "   - Run Objective 0 (Setup) first\n",
        "   - Run Objective 1 (System Prompt) - must complete successfully\n",
        "   - Verify `mistral_model`, `mistral_tokenizer`, and `system_prompt` exist\n",
        "\n",
        "2. **Run the Code Cell**: \n",
        "   - Execute the Objective 2 code cell\n",
        "   - Generation takes 2-3 minutes (21 LLM calls)\n",
        "   - Progress will be shown for each category\n",
        "\n",
        "3. **Verify Output**: \n",
        "   - Check console for: \"âœ… Generated 21 Q&A pairs\"\n",
        "   - Verify DataFrame created: \"âœ… DataFrame created: 21 rows Ã— 6 columns\"\n",
        "   - Check statistics displayed\n",
        "   - Verify file saved: `data/qa_database/qa_database.csv`\n",
        "\n",
        "**Expected Runtime:**\n",
        "- GPU: 2-3 minutes (21 generation calls)\n",
        "- CPU: 3-4 minutes (21 generation calls)\n",
        "\n",
        "**Troubleshooting:**\n",
        "- If generation fails, check that Objective 1 completed successfully\n",
        "- If parsing errors occur, the delimiter format may need adjustment\n",
        "- If file save fails, ensure `data/qa_database/` directory exists\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>âœ… Verification</b> (Click to expand)</summary>\n",
        "\n",
        "After running, use the verification function to check all requirements:\n",
        "\n",
        "```python\n",
        "# Call the verification function (created in Objective 2 code cell)\n",
        "verify_objective2()\n",
        "```\n",
        "\n",
        "**What the Function Checks:**\n",
        "- âœ… Variables exist: `qa_database`, `qa_df`\n",
        "- âœ… Correct count: 21 Q&A pairs total\n",
        "- âœ… Structure: All pairs have required keys (`category`, `answerable`, `question`, `answer`)\n",
        "- âœ… Distribution: 18 answerable + 3 unanswerable pairs\n",
        "- âœ… DataFrame: Correct number of rows and required columns\n",
        "- âœ… File exists: `data/qa_database/qa_database.csv` saved successfully\n",
        "\n",
        "**Function Output:**\n",
        "The function will print a summary with:\n",
        "- Total Q&A pairs count\n",
        "- Answerable vs unanswerable breakdown\n",
        "- DataFrame dimensions\n",
        "- File location confirmation\n",
        "\n",
        "**Note:** The `verify_objective2()` function is automatically created when you run the Objective 2 code cell. It performs all verification checks and provides a clear success/failure report.\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ”— Dependencies</b> (Click to expand)</summary>\n",
        "\n",
        "**This Objective:**\n",
        "- âœ… Requires Objective 0 (Setup) for `hf_token`\n",
        "- âœ… Requires Objective 1 (System Prompt) for `mistral_model`, `mistral_tokenizer`, `system_prompt`\n",
        "\n",
        "**Used By:**\n",
        "- Objective 3: Uses `qa_database` to build FAISS vector index\n",
        "- Objective 4: Uses `qa_database` as knowledge base for RAG retrieval\n",
        "- Objective 5: Uses `qa_database` for model evaluation\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ“š Learning Objectives Demonstrated</b> (Click to expand)</summary>\n",
        "\n",
        "1. **Zero-Shot Prompting**: Using instructions without examples to guide LLM output, leveraging the model's pre-training and context from system prompts\n",
        "2. **System Prompt Reuse**: Integrating previously created system prompts to maintain consistency across objectives and ensure domain alignment\n",
        "3. **LLM-Based Content Generation**: Using language models to create structured domain-specific content\n",
        "4. **Data Structure Design**: Designing flexible data formats (list + DataFrame) for different access patterns\n",
        "5. **Parsing LLM Output**: Reliable extraction of structured data from free-form LLM responses using delimiter-based parsing\n",
        "6. **Knowledge Base Construction**: Building domain-specific knowledge bases for RAG systems\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ’¡ Tips</b> (Click to expand)</summary>\n",
        "\n",
        "- **System Prompt Integration**: The system prompt from Objective 1 is automatically embedded in generation prompts - ensure Objective 1 completed successfully\n",
        "- **Zero-Shot vs Few-Shot**: Zero-shot is used here for efficiency; few-shot could improve consistency but requires example formatting\n",
        "- **Generation Time**: 21 LLM calls take 2-3 minutes - be patient\n",
        "- **Delimiter Choice**: `|||` is chosen because it's unlikely to appear in natural text\n",
        "- **Answerable Flag**: Essential for testing system's ability to decline unanswerable questions\n",
        "- **DataFrame Benefits**: Use `qa_df[qa_df['answerable'] == True]` for easy filtering\n",
        "- **Category Coverage**: Each category has 3 pairs to ensure sufficient coverage\n",
        "- **Unanswerable Testing**: The 3 unanswerable pairs test different types of out-of-scope questions\n",
        "- **Prompt Context**: The system prompt's first 1200 characters are included to provide business context without exceeding token limits\n",
        "\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "**Next Step:** Proceed to Objective 3 to build the FAISS vector database from the Q&A pairs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "OBJECTIVE 2 (REVISED): GENERATE CUSTOM Q&A DATABASE\n",
            "======================================================================\n",
            "   âœ… Prerequisites validated\n",
            "\n",
            "ğŸ¤– Generating Q&A with strict Mistral prompts...\n",
            "\n",
            "ğŸ“— Generating answerable Q&A...\n",
            "   â†’ products...\n",
            "   â†’ shipping...\n",
            "   â†’ returns...\n",
            "   â†’ customer_service...\n",
            "   â†’ warranty...\n",
            "   â†’ orders...\n",
            "\n",
            "ğŸ“• Generating unanswerable Q&A...\n",
            "   â†’ competitor...\n",
            "   â†’ personal_advice...\n",
            "   â†’ future_events...\n",
            "\n",
            "ğŸ‰ Generated 6 total pairs (18 answerable, 3 unanswerable)\n",
            "   ğŸ’¾ Saved to data/qa_database/qa_database.csv\n",
            "\n",
            "Done! Q&A database is ready.\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# OBJECTIVE 2 (REVISED): GENERATE CUSTOM Q&A DATABASES \n",
        "# ============================================================================\n",
        "#\n",
        "#\n",
        "# NOTE: Requires Objective 1 (mistral_model, mistral_tokenizer, system_prompt)\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import re\n",
        "from typing import List, Dict\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# CONFIG\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "TEMPERATURE = 0.7\n",
        "TOP_P = 0.9\n",
        "CONTEXT_CHARS = 900           # Smaller excerpt increases relevance\n",
        "DELIMITER = \"|||\"\n",
        "\n",
        "OUTPUT_DIR = \"data/qa_database\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# VALIDATION\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "def validate_prerequisites():\n",
        "    required = [\"mistral_model\", \"mistral_tokenizer\", \"system_prompt\"]\n",
        "    missing = [r for r in required if r not in globals()]\n",
        "    if missing:\n",
        "        raise RuntimeError(\n",
        "            f\"âŒ Objective 2 requires Objective 1 first. Missing: {missing}\"\n",
        "        )\n",
        "    print(\"   âœ… Prerequisites validated\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# CATEGORY DEFINITIONS\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "QA_CATEGORIES = [\n",
        "    (\"products\", \"types of products, solar panels, smart devices, eco-friendly items\"),\n",
        "    (\"shipping\", \"delivery times, shipping cost, free shipping threshold, tracking\"),\n",
        "    (\"returns\", \"return policy, refund window, 30-day policy, conditions\"),\n",
        "    (\"customer_service\", \"hours (Monâ€“Fri 9â€“6, Sat 10â€“4), email, phone support\"),\n",
        "    (\"warranty\", \"coverage periods 1â€“3 years, claims process\"),\n",
        "    (\"orders\", \"order status, modifying or cancelling orders, tracking numbers\"),\n",
        "]\n",
        "\n",
        "UNANSWERABLE_TYPES = [\n",
        "    (\"competitor\", \"questions about competitor prices or product comparisons\"),\n",
        "    (\"personal_advice\", \"questions asking for personal recommendations or opinions\"),\n",
        "    (\"future_events\", \"questions about upcoming sales or unreleased products\"),\n",
        "]\n",
        "\n",
        "PAIRS_PER_CATEGORY = 3\n",
        "UNANSWERABLE_PER_TYPE = 1\n",
        "\n",
        "ANSWERABLE_TOTAL = len(QA_CATEGORIES) * PAIRS_PER_CATEGORY\n",
        "UNANSWERABLE_TOTAL = len(UNANSWERABLE_TYPES) * UNANSWERABLE_PER_TYPE\n",
        "TOTAL_PAIRS = ANSWERABLE_TOTAL + UNANSWERABLE_TOTAL\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# PROMPT TEMPLATES \n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "ANSWERABLE_PROMPT = \"\"\"\n",
        "You are generating REALISTIC customer service Q&A pairs for the ShopSmart e-commerce support assistant.\n",
        "\n",
        "Generate EXACTLY {num_pairs} Q&A pairs about the topic below.\n",
        "\n",
        "TOPIC FOCUS:\n",
        "{description}\n",
        "\n",
        "BUSINESS CONTEXT (from system prompt):\n",
        "{context}\n",
        "\n",
        "STRICT OUTPUT RULES:\n",
        "- Each pair MUST be ONE SINGLE LINE.\n",
        "- NO blank lines.\n",
        "- NO numbering, bullets, dashes, extra text, or explanations.\n",
        "- NO category labels.\n",
        "- Format MUST be EXACTLY:\n",
        "\n",
        "Question? ||| Answer text.\n",
        "\n",
        "CONTENT RULES:\n",
        "- Questions MUST sound like real customers.\n",
        "- Answers MUST be 2â€“3 sentences.\n",
        "- Answers MUST include at least ONE concrete detail (e.g., times, numbers, policies).\n",
        "- Stay entirely within ShopSmart policies.\n",
        "- DO NOT hallucinate unsupported info.\n",
        "\n",
        "Return ONLY {num_pairs} lines in EXACT strict format:\n",
        "Question? ||| Answer.\n",
        "\"\"\"\n",
        "\n",
        "UNANSWERABLE_PROMPT = \"\"\"\n",
        "Generate EXACTLY {num_pairs} UNANSWERABLE customer Q&A pairs.\n",
        "\n",
        "TOPIC TYPE OUT OF SCOPE:\n",
        "{description}\n",
        "\n",
        "STRICT OUTPUT RULES:\n",
        "- Each pair MUST be ONE SINGLE LINE only.\n",
        "- NO blank lines.\n",
        "- NO numbering or bullets.\n",
        "- Do NOT include category names.\n",
        "- Format MUST be EXACTLY:\n",
        "\n",
        "Question? ||| Polite refusal answer.\n",
        "\n",
        "REFUSAL RULES:\n",
        "- Question MUST be outside ShopSmart's knowledge base.\n",
        "- Answer MUST politely decline.\n",
        "- Answer MUST say you cannot provide that information.\n",
        "- Answer MUST offer what you *can* help with (shipping, returns, products, etc.)\n",
        "- Answer MUST be 2 sentences.\n",
        "\n",
        "Return ONLY {num_pairs} lines:\n",
        "Question? ||| Polite refusal.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# GENERATION FUNCTION\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "def mistral_generate(prompt: str, max_tokens: int = 600) -> str:\n",
        "    tokenizer = mistral_tokenizer\n",
        "    model = mistral_model\n",
        "\n",
        "    formatted = f\"<s>[INST] {prompt} [/INST]\"\n",
        "\n",
        "    inputs = tokenizer(formatted, return_tensors=\"pt\")\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            temperature=TEMPERATURE,\n",
        "            top_p=TOP_P,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Extract generated tokens (skip prompt)\n",
        "    inp_len = inputs[\"input_ids\"].shape[1]\n",
        "    gen_tokens = outputs[0][inp_len:]\n",
        "    text = tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# PARSER (ROBUST - IMPROVED)\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "def parse_qa_lines(text: str, answerable: bool, debug: bool = False) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Parse Q&A pairs from model output with improved robustness.\n",
        "    Handles various formats including numbered lists, extra whitespace, etc.\n",
        "    \"\"\"\n",
        "    qa_list = []\n",
        "    \n",
        "    if debug:\n",
        "        print(f\"      [DEBUG] Raw model output ({len(text)} chars):\")\n",
        "        print(f\"      {repr(text[:200])}...\")\n",
        "    \n",
        "    # Try multiple parsing strategies\n",
        "    lines = text.split(\"\\n\")\n",
        "    \n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        \n",
        "        # Remove common prefixes (numbers, bullets, dashes)\n",
        "        line_clean = line\n",
        "        # Remove patterns like \"1. \", \"2. \", \"- \", \"* \", etc.\n",
        "        line_clean = re.sub(r'^\\d+[\\.\\)]\\s*', '', line_clean)  # Remove \"1. \", \"2) \"\n",
        "        line_clean = re.sub(r'^[-*â€¢]\\s*', '', line_clean)  # Remove \"- \", \"* \", \"â€¢ \"\n",
        "        line_clean = line_clean.strip()\n",
        "        \n",
        "        # Check if line contains delimiter\n",
        "        if DELIMITER in line_clean:\n",
        "            try:\n",
        "                q, a = line_clean.split(DELIMITER, 1)\n",
        "                q = q.strip()\n",
        "                a = a.strip()\n",
        "                \n",
        "                # Validate that we have actual content\n",
        "                if q and a and len(q) > 3 and len(a) > 10:\n",
        "                    # Remove question marks if they're just placeholders\n",
        "                    if q.lower() in [\"question?\", \"question\", \"q:\", \"q.\"]:\n",
        "                        continue\n",
        "                    \n",
        "                    qa_list.append({\n",
        "                        \"question\": q,\n",
        "                        \"answer\": a,\n",
        "                        \"answerable\": answerable\n",
        "                    })\n",
        "            except ValueError:\n",
        "                if debug:\n",
        "                    print(f\"      [DEBUG] Failed to split line: {line_clean[:100]}\")\n",
        "                continue\n",
        "    \n",
        "    if debug:\n",
        "        print(f\"      [DEBUG] Parsed {len(qa_list)} pairs from {len(lines)} lines\")\n",
        "    \n",
        "    return qa_list\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# HIGH-LEVEL GENERATORS (WITH RETRY LOGIC)\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "def generate_answerable(category: str, description: str, n: int, max_retries: int = 3) -> List[Dict]:\n",
        "    \"\"\"Generate answerable Q&A pairs with retry logic if parsing fails.\"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        prompt = ANSWERABLE_PROMPT.format(\n",
        "            num_pairs=n,\n",
        "            description=description,\n",
        "            context=system_prompt[:CONTEXT_CHARS]\n",
        "        )\n",
        "        raw = mistral_generate(prompt, max_tokens=800)  # Increased tokens for better generation\n",
        "        parsed = parse_qa_lines(raw, True, debug=(attempt == max_retries - 1))\n",
        "        \n",
        "        if len(parsed) >= n:\n",
        "            return parsed[:n]\n",
        "        elif len(parsed) > 0:\n",
        "            # If we got some pairs but not enough, return what we have\n",
        "            print(f\"      âš ï¸  Got {len(parsed)}/{n} pairs (attempt {attempt + 1})\")\n",
        "            if attempt < max_retries - 1:\n",
        "                continue\n",
        "            return parsed\n",
        "    \n",
        "    # If all retries failed, return empty list\n",
        "    print(f\"      âŒ Failed to generate pairs after {max_retries} attempts\")\n",
        "    return []\n",
        "\n",
        "\n",
        "def generate_unanswerable(category: str, description: str, n: int, max_retries: int = 3) -> List[Dict]:\n",
        "    \"\"\"Generate unanswerable Q&A pairs with retry logic if parsing fails.\"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        prompt = UNANSWERABLE_PROMPT.format(\n",
        "            num_pairs=n,\n",
        "            description=description\n",
        "        )\n",
        "        raw = mistral_generate(prompt, max_tokens=600)\n",
        "        parsed = parse_qa_lines(raw, False, debug=(attempt == max_retries - 1))\n",
        "        \n",
        "        if len(parsed) >= n:\n",
        "            return parsed[:n]\n",
        "        elif len(parsed) > 0:\n",
        "            print(f\"      âš ï¸  Got {len(parsed)}/{n} pairs (attempt {attempt + 1})\")\n",
        "            if attempt < max_retries - 1:\n",
        "                continue\n",
        "            return parsed\n",
        "    \n",
        "    print(f\"      âŒ Failed to generate pairs after {max_retries} attempts\")\n",
        "    return []\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# FULL DATABASE GENERATOR\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "def generate_full_qa_database():\n",
        "    db = []\n",
        "\n",
        "    print(\"\\nğŸ“— Generating answerable Q&A...\")\n",
        "    for cat, desc in QA_CATEGORIES:\n",
        "        print(f\"   â†’ {cat}...\")\n",
        "        pairs = generate_answerable(cat, desc, PAIRS_PER_CATEGORY)\n",
        "        for p in pairs:\n",
        "            p[\"category\"] = cat\n",
        "        db.extend(pairs)\n",
        "\n",
        "    print(\"\\nğŸ“• Generating unanswerable Q&A...\")\n",
        "    for cat, desc in UNANSWERABLE_TYPES:\n",
        "        print(f\"   â†’ {cat}...\")\n",
        "        pairs = generate_unanswerable(cat, desc, UNANSWERABLE_PER_TYPE)\n",
        "        for p in pairs:\n",
        "            p[\"category\"] = cat\n",
        "        db.extend(pairs)\n",
        "\n",
        "    print(f\"\\nğŸ‰ Generated {len(db)} total pairs \"\n",
        "          f\"({ANSWERABLE_TOTAL} answerable, {UNANSWERABLE_TOTAL} unanswerable)\")\n",
        "\n",
        "    return db\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# CONVERSION / DISPLAY / SAVE â€” SAME AS ORIGINAL\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "def qa_to_dataframe(qa_list: List[Dict]) -> pd.DataFrame:\n",
        "    df = pd.DataFrame(qa_list)\n",
        "    df[\"question_length\"] = df.question.str.len()\n",
        "    df[\"answer_length\"] = df.answer.str.len()\n",
        "    df[\"word_count\"] = df.answer.str.split().str.len()\n",
        "    return df\n",
        "\n",
        "\n",
        "def save_qa_to_csv(qa_list, filename=\"qa_database.csv\"):\n",
        "    df = pd.DataFrame(qa_list)\n",
        "    path = os.path.join(OUTPUT_DIR, filename)\n",
        "    df.to_csv(path, index=False)\n",
        "    print(f\"   ğŸ’¾ Saved to {path}\")\n",
        "    return path\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# VERIFICATION FUNCTION\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "def verify_objective2():\n",
        "    \"\"\"\n",
        "    Verify that Objective 2 completed successfully.\n",
        "    Checks all variables, counts, structure, and files.\n",
        "    \"\"\"\n",
        "    import os\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    print(\"ğŸ” OBJECTIVE 2 VERIFICATION\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    errors = []\n",
        "    warnings = []\n",
        "    \n",
        "    # Check if variables exist\n",
        "    if 'qa_database' not in globals():\n",
        "        errors.append(\"âŒ qa_database not found\")\n",
        "    if 'qa_df' not in globals():\n",
        "        errors.append(\"âŒ qa_df not found\")\n",
        "    \n",
        "    if errors:\n",
        "        print(\"\\n\".join(errors))\n",
        "        print(\"=\"*70)\n",
        "        return False\n",
        "    \n",
        "    # Check count\n",
        "    actual_count = len(qa_database)\n",
        "    expected_count = TOTAL_PAIRS\n",
        "    \n",
        "    if actual_count != expected_count:\n",
        "        errors.append(f\"âŒ Wrong count: Expected {expected_count}, got {actual_count}\")\n",
        "    else:\n",
        "        print(f\"âœ… Count correct: {actual_count} pairs\")\n",
        "    \n",
        "    # Check structure\n",
        "    required_keys = [\"question\", \"answer\", \"answerable\", \"category\"]\n",
        "    for i, pair in enumerate(qa_database):\n",
        "        for key in required_keys:\n",
        "            if key not in pair:\n",
        "                errors.append(f\"âŒ Pair {i+1} missing key: {key}\")\n",
        "            elif not pair[key] or (isinstance(pair[key], str) and len(pair[key].strip()) == 0):\n",
        "                errors.append(f\"âŒ Pair {i+1} has empty {key}\")\n",
        "    \n",
        "    if not errors:\n",
        "        print(f\"âœ… Structure correct: All pairs have required keys\")\n",
        "    \n",
        "    # Check distribution\n",
        "    answerable_count = sum(1 for p in qa_database if p.get(\"answerable\") == True)\n",
        "    unanswerable_count = sum(1 for p in qa_database if p.get(\"answerable\") == False)\n",
        "    \n",
        "    if answerable_count != ANSWERABLE_TOTAL:\n",
        "        warnings.append(f\"âš ï¸  Answerable count: Expected {ANSWERABLE_TOTAL}, got {answerable_count}\")\n",
        "    else:\n",
        "        print(f\"âœ… Answerable pairs: {answerable_count}\")\n",
        "    \n",
        "    if unanswerable_count != UNANSWERABLE_TOTAL:\n",
        "        warnings.append(f\"âš ï¸  Unanswerable count: Expected {UNANSWERABLE_TOTAL}, got {unanswerable_count}\")\n",
        "    else:\n",
        "        print(f\"âœ… Unanswerable pairs: {unanswerable_count}\")\n",
        "    \n",
        "    # Check DataFrame\n",
        "    if len(qa_df) != actual_count:\n",
        "        errors.append(f\"âŒ DataFrame count mismatch: {len(qa_df)} != {actual_count}\")\n",
        "    else:\n",
        "        print(f\"âœ… DataFrame correct: {len(qa_df)} rows\")\n",
        "    \n",
        "    # Check file exists\n",
        "    csv_path = os.path.join(OUTPUT_DIR, \"qa_database.csv\")\n",
        "    if not os.path.exists(csv_path):\n",
        "        errors.append(f\"âŒ CSV file not found: {csv_path}\")\n",
        "    else:\n",
        "        print(f\"âœ… CSV file exists: {csv_path}\")\n",
        "        # Verify CSV content\n",
        "        try:\n",
        "            df_check = pd.read_csv(csv_path)\n",
        "            if len(df_check) != actual_count:\n",
        "                warnings.append(f\"âš ï¸  CSV row count: {len(df_check)} != {actual_count}\")\n",
        "        except Exception as e:\n",
        "            warnings.append(f\"âš ï¸  Could not verify CSV: {e}\")\n",
        "    \n",
        "    # Print results\n",
        "    if errors:\n",
        "        print(\"\\nâŒ VERIFICATION FAILED:\")\n",
        "        print(\"\\n\".join(errors))\n",
        "        if warnings:\n",
        "            print(\"\\nâš ï¸  WARNINGS:\")\n",
        "            print(\"\\n\".join(warnings))\n",
        "        print(\"=\"*70)\n",
        "        return False\n",
        "    else:\n",
        "        print(\"\\nâœ… Objective 2 Complete - All checks passed!\")\n",
        "        if warnings:\n",
        "            print(\"\\nâš ï¸  WARNINGS:\")\n",
        "            print(\"\\n\".join(warnings))\n",
        "        print(f\"   â€¢ Total Q&A pairs: {actual_count}\")\n",
        "        print(f\"   â€¢ Answerable: {answerable_count}\")\n",
        "        print(f\"   â€¢ Unanswerable: {unanswerable_count}\")\n",
        "        print(f\"   â€¢ DataFrame: {len(qa_df)} rows Ã— {len(qa_df.columns)} columns\")\n",
        "        print(f\"   â€¢ CSV file: {csv_path}\")\n",
        "        print(\"=\"*70)\n",
        "        return True\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# DISPLAY FUNCTION\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "def display_qa_database(qa_list: List[Dict], max_display: int = None):\n",
        "    \"\"\"Display Q&A pairs in a readable format.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ğŸ“‹ GENERATED Q&A DATABASE\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    if max_display:\n",
        "        display_list = qa_list[:max_display]\n",
        "        print(f\"\\nShowing first {len(display_list)} of {len(qa_list)} pairs:\\n\")\n",
        "    else:\n",
        "        display_list = qa_list\n",
        "        print(f\"\\nAll {len(display_list)} pairs:\\n\")\n",
        "    \n",
        "    for i, pair in enumerate(display_list, 1):\n",
        "        answerable_str = \"âœ… Answerable\" if pair.get(\"answerable\") else \"âŒ Unanswerable\"\n",
        "        category = pair.get(\"category\", \"unknown\")\n",
        "        print(f\"\\n[{i}] {answerable_str} | Category: {category}\")\n",
        "        print(f\"    Q: {pair.get('question', 'N/A')}\")\n",
        "        print(f\"    A: {pair.get('answer', 'N/A')[:150]}{'...' if len(pair.get('answer', '')) > 150 else ''}\")\n",
        "    \n",
        "    if max_display and len(qa_list) > max_display:\n",
        "        print(f\"\\n... and {len(qa_list) - max_display} more pairs\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# EXECUTION ENTRYPOINT\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"OBJECTIVE 2 (REVISED): GENERATE CUSTOM Q&A DATABASE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "validate_prerequisites()\n",
        "\n",
        "print(\"\\nğŸ¤– Generating Q&A with strict Mistral prompts...\")\n",
        "qa_database = generate_full_qa_database()\n",
        "qa_df = qa_to_dataframe(qa_database)\n",
        "\n",
        "save_qa_to_csv(qa_database)\n",
        "\n",
        "# Display the generated Q&A pairs\n",
        "display_qa_database(qa_database)\n",
        "\n",
        "# Verify the results\n",
        "print(\"\\n\")\n",
        "verify_objective2()\n",
        "\n",
        "print(\"\\nDone! Q&A database is ready.\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Objective 3: Implement Vector Databases Using FAISS\n",
        "\n",
        "### ğŸ¯ Goal\n",
        "Build a FAISS vector database from the Q&A pairs to enable fast semantic search, converting text to embeddings and creating an index for efficient similarity retrieval in the RAG system.\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ“¥ Prerequisites</b> (Click to expand)</summary>\n",
        "\n",
        "| Item | Source | Required | Description |\n",
        "|------|--------|----------|-------------|\n",
        "| `qa_database` | Objective 2 | âœ… Yes | List of 21 Q&A pairs to convert to embeddings |\n",
        "| `system_prompt` | Objective 1 | âœ… Yes | System prompt (used for validation) |\n",
        "| Python packages | Setup cell | âœ… Yes | `faiss-cpu`, `sentence-transformers`, `numpy`, `pandas` |\n",
        "\n",
        "**Note:** Objective 2 must be completed first to provide the Q&A database.\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ”§ Core Concepts</b> (Click to expand)</summary>\n",
        "\n",
        "| Concept | Description |\n",
        "|--------|-------------|\n",
        "| **Embeddings** | Numerical vector representations of text that capture semantic meaning. Similar texts have similar embeddings (close in vector space) |\n",
        "| **Semantic Search** | Finding relevant documents by meaning rather than exact keyword matching. Enables finding \"shipping\" when querying \"delivery\" |\n",
        "| **FAISS (Facebook AI Similarity Search)** | Library for efficient similarity search in high-dimensional vector spaces. Searches millions of vectors in milliseconds |\n",
        "| **IndexFlatL2** | FAISS index type using L2 (Euclidean) distance for exact similarity search. Ideal for small-medium datasets (<100k vectors) |\n",
        "| **Sentence Transformers** | Pre-trained models that convert text to dense vector embeddings optimized for semantic similarity |\n",
        "| **Top-K Retrieval** | Retrieving the k most similar documents (e.g., top-3) based on embedding similarity scores |\n",
        "\n",
        "**Why This Matters:**\n",
        "Vector databases enable semantic search - finding relevant information even when exact keywords don't match. This is essential for RAG systems where user questions need to retrieve the most semantically similar context from the knowledge base.\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ“Š Design Choices</b> (Click to expand)</summary>\n",
        "\n",
        "| Choice | Selected | Rationale |\n",
        "|--------|----------|-----------|\n",
        "| **Embedding Model** | `all-MiniLM-L6-v2` | 384 dimensions, fast inference, good quality for small-medium datasets |\n",
        "| **FAISS Index Type** | IndexFlatL2 | Exact search with L2 distance, ideal for 21 Q&A pairs, no approximation needed |\n",
        "| **Embedding Strategy** | Question + Answer combined | Richer semantic representation by including both question and answer text |\n",
        "| **Top-K Retrieval** | 3 documents | Balances context richness with prompt length, sufficient for most queries |\n",
        "| **Distance Metric** | L2 (Euclidean) | Standard similarity measure, lower distance = more similar vectors |\n",
        "| **Data Type** | float32 | FAISS requirement, balances precision and memory usage |\n",
        "\n",
        "**Why This Approach:**\n",
        "- **all-MiniLM-L6-v2**: Lightweight, fast, sufficient quality for 21 pairs. Alternative models (e.g., all-mpnet-base-v2) offer higher accuracy but slower inference\n",
        "- **IndexFlatL2**: Exact search ensures highest quality results. For larger datasets (>100k), IndexIVFFlat or IndexHNSW would be better for speed\n",
        "- **Combined Q&A**: Including both question and answer in embeddings captures full semantic context, improving retrieval accuracy\n",
        "- **Top-3**: Provides enough context for LLM while keeping prompts manageable\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ”„ Process Flow</b> (Click to expand)</summary>\n",
        "\n",
        "**Left-to-Right Flow:**\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  Validate    â”‚â”€â”€â”€â–¶â”‚  Load        â”‚â”€â”€â”€â–¶â”‚  Generate    â”‚â”€â”€â”€â–¶â”‚  Create     â”‚â”€â”€â”€â–¶â”‚  Test       â”‚\n",
        "â”‚ Prerequisitesâ”‚    â”‚  Embedding   â”‚    â”‚  Embeddings â”‚    â”‚  FAISS      â”‚    â”‚  Retrieval  â”‚\n",
        "â”‚              â”‚    â”‚  Model       â”‚    â”‚  (384-dim)  â”‚    â”‚  Index      â”‚    â”‚             â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                                                                                        â”‚\n",
        "                                                                                        â–¼\n",
        "                                                                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "                                                                                â”‚  Save Files  â”‚\n",
        "                                                                                â”‚  (Index +    â”‚\n",
        "                                                                                â”‚  Embeddings) â”‚\n",
        "                                                                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "**Detailed Steps:**\n",
        "1. **Validate Prerequisites**: Ensure Objective 2 completed (qa_database available with 21 pairs)\n",
        "2. **Load Embedding Model**: Download and load sentence-transformers model (`all-MiniLM-L6-v2`, 384 dimensions)\n",
        "3. **Generate Embeddings**: Convert each Q&A pair (question + answer) to 384-dimensional vector\n",
        "4. **Create FAISS Index**: Build IndexFlatL2 index and add all 21 embeddings\n",
        "5. **Test Retrieval**: Query the index with sample questions to verify semantic search works\n",
        "6. **Save Files**: Persist embeddings and FAISS index to disk for reuse\n",
        "\n",
        "**Embedding Process:**\n",
        "- Input: Q&A text (e.g., \"What is your shipping policy? We offer standard shipping...\")\n",
        "- Model: sentence-transformers/all-MiniLM-L6-v2\n",
        "- Output: 384-dimensional float32 vector\n",
        "- Result: 21 vectors (one per Q&A pair) ready for indexing\n",
        "\n",
        "**FAISS Index Creation:**\n",
        "- Dimension: 384 (matches embedding model output)\n",
        "- Type: IndexFlatL2 (exact search, L2 distance)\n",
        "- Vectors: 21 (one per Q&A pair)\n",
        "- Search Speed: Milliseconds for similarity queries\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ“¤ Outputs</b> (Click to expand)</summary>\n",
        "\n",
        "| Variable | Type | Description |\n",
        "|----------|------|-------------|\n",
        "| `embedding_model` | `SentenceTransformer` | Loaded sentence-transformers model (all-MiniLM-L6-v2) |\n",
        "| `qa_embeddings` | `np.ndarray` | Embedding vectors for all Q&A pairs, shape (21, 384) |\n",
        "| `faiss_index` | `faiss.IndexFlatL2` | FAISS index with 21 vectors indexed, ready for similarity search |\n",
        "| `embed_query()` | `function` | Function to convert query text to embedding vector |\n",
        "\n",
        "**Files Created:**\n",
        "| File | Location | Description |\n",
        "|------|----------|-------------|\n",
        "| `qa_embeddings.npy` | `data/vector_database/` | NumPy array of all Q&A embeddings (21 Ã— 384) |\n",
        "| `qa_index.faiss` | `data/vector_database/` | Serialized FAISS index for persistence |\n",
        "| `retrieval_test_results.csv` | `data/vector_database/` | Test query results with similarity scores |\n",
        "\n",
        "**Key Functions Created:**\n",
        "- `load_embedding_model()`: Loads sentence-transformers model\n",
        "- `generate_embeddings()`: Converts text list to embedding array\n",
        "- `create_faiss_index()`: Builds FAISS IndexFlatL2 from embeddings\n",
        "- `search_index()`: Finds top-k similar vectors by L2 distance\n",
        "- `retrieve_context()`: Complete RAG retrieval - query to relevant Q&A pairs\n",
        "- `embed_query()`: Converts query text to embedding vector\n",
        "- `format_context_for_llm()`: Formats retrieved Q&A pairs as context string\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ“‹ FAISS Index Details</b> (Click to expand)</summary>\n",
        "\n",
        "**Index Type: IndexFlatL2**\n",
        "\n",
        "| Property | Value | Description |\n",
        "|----------|-------|-------------|\n",
        "| **Distance Metric** | L2 (Euclidean) | Lower distance = more similar vectors |\n",
        "| **Search Type** | Exact | No approximation, highest quality results |\n",
        "| **Vectors Indexed** | 21 | One per Q&A pair |\n",
        "| **Embedding Dimension** | 384 | Matches all-MiniLM-L6-v2 output |\n",
        "| **Search Speed** | Milliseconds | Fast for small-medium datasets |\n",
        "| **Scalability** | <100k vectors | For larger datasets, use IndexIVFFlat or IndexHNSW |\n",
        "\n",
        "**Why IndexFlatL2:**\n",
        "- **Exact Search**: No approximation means highest quality results\n",
        "- **Simple**: Easy to implement and understand\n",
        "- **Sufficient**: Perfect for 21 Q&A pairs\n",
        "- **Fast Enough**: Millisecond search times for our dataset size\n",
        "\n",
        "**Alternative Index Types (for reference):**\n",
        "- **IndexIVFFlat**: Approximate search, faster for large datasets (>100k)\n",
        "- **IndexHNSW**: Graph-based, very fast approximate search for very large datasets\n",
        "- **IndexFlatIP**: Inner product (cosine similarity) instead of L2 distance\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>â–¶ï¸ How to Run</b> (Click to expand)</summary>\n",
        "\n",
        "1. **Ensure Prerequisites Complete**: \n",
        "   - Run Objective 0 (Setup) first\n",
        "   - Run Objective 1 (System Prompt)\n",
        "   - Run Objective 2 (Q&A Database) - must complete successfully\n",
        "   - Verify `qa_database` exists with 21 pairs\n",
        "\n",
        "2. **Run the Code Cell**: \n",
        "   - Execute the Objective 3 code cell\n",
        "   - Embedding model download: ~90MB (first run only)\n",
        "   - Embedding generation: <30 seconds for 21 pairs\n",
        "   - FAISS index creation: <1 second\n",
        "\n",
        "3. **Verify Output**: \n",
        "   - Check console for: \"âœ… FAISS index created\"\n",
        "   - Verify embeddings shape: (21, 384)\n",
        "   - Check index vectors: 21 indexed\n",
        "   - Verify test retrieval works\n",
        "   - Check files saved: `data/vector_database/qa_embeddings.npy`, `qa_index.faiss`\n",
        "\n",
        "**Expected Runtime:**\n",
        "- First run: 30-60 seconds (model download + embedding generation)\n",
        "- Subsequent runs: 10-20 seconds (model cached, embeddings regenerated)\n",
        "\n",
        "**Troubleshooting:**\n",
        "- If embedding generation fails, check that Objective 2 completed successfully\n",
        "- If FAISS index creation fails, verify embeddings are float32 dtype\n",
        "- If retrieval returns no results, check that index was created with correct dimension\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>âœ… Verification</b> (Click to expand)</summary>\n",
        "\n",
        "After running, use the verification function to check all requirements:\n",
        "\n",
        "```python\n",
        "# Call the verification function (created in Objective 3 code cell)\n",
        "verify_objective3()\n",
        "```\n",
        "\n",
        "**What the Function Checks:**\n",
        "- âœ… Variables exist: `embedding_model`, `qa_embeddings`, `faiss_index`, `embed_query`\n",
        "- âœ… Embeddings shape: (21, 384) - correct number of pairs and dimensions\n",
        "- âœ… FAISS index: 21 vectors indexed, correct dimension (384)\n",
        "- âœ… Files exist: `qa_embeddings.npy`, `qa_index.faiss` saved successfully\n",
        "- âœ… Retrieval function: `embed_query()` works correctly\n",
        "\n",
        "**Function Output:**\n",
        "The function will print a summary with:\n",
        "- Embedding model information\n",
        "- Embeddings shape and count\n",
        "- FAISS index details (vectors indexed, dimension)\n",
        "- File locations confirmation\n",
        "\n",
        "**Note:** The `verify_objective3()` function is automatically created when you run the Objective 3 code cell. It performs all verification checks and provides a clear success/failure report.\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ”— Dependencies</b> (Click to expand)</summary>\n",
        "\n",
        "**This Objective:**\n",
        "- âœ… Requires Objective 0 (Setup) for packages\n",
        "- âœ… Requires Objective 1 (System Prompt) for validation\n",
        "- âœ… Requires Objective 2 (Q&A Database) for `qa_database` (21 pairs)\n",
        "\n",
        "**Used By:**\n",
        "- Objective 4: Uses `faiss_index` and `embed_query()` for RAG retrieval\n",
        "- Objective 5: Uses `faiss_index` for model evaluation with RAGAS\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ“š Learning Objectives Demonstrated</b> (Click to expand)</summary>\n",
        "\n",
        "1. **Text Embeddings**: Converting text to numerical vectors that capture semantic meaning\n",
        "2. **Semantic Search**: Finding relevant documents by meaning rather than keyword matching\n",
        "3. **Vector Databases**: Using FAISS for efficient similarity search in high-dimensional spaces\n",
        "4. **Index Design**: Choosing appropriate index types (IndexFlatL2) for dataset size\n",
        "5. **RAG Retrieval**: Implementing the retrieval component of RAG systems\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>ğŸ’¡ Tips</b> (Click to expand)</summary>\n",
        "\n",
        "- **Embedding Model**: all-MiniLM-L6-v2 is fast and sufficient for 21 pairs. For larger datasets, consider all-mpnet-base-v2 for better quality\n",
        "- **Combined Q&A**: Including both question and answer in embeddings improves retrieval accuracy\n",
        "- **FAISS Index Type**: IndexFlatL2 is perfect for small datasets. For >100k vectors, use IndexIVFFlat for speed\n",
        "- **Top-K Selection**: Top-3 provides good context balance. Adjust based on your use case\n",
        "- **Float32 Requirement**: FAISS requires float32 dtype - embeddings are automatically converted\n",
        "- **Index Persistence**: Saved index can be loaded later without regenerating embeddings\n",
        "- **Search Speed**: FAISS searches are extremely fast (milliseconds) even for larger datasets\n",
        "\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "**Next Step:** Proceed to Objective 4 to build the complete RAG pipeline using the FAISS index.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "   OBJECTIVE 3: IMPLEMENT VECTOR DATABASE USING FAISS\n",
            "   Building Semantic Search for RAG System\n",
            "======================================================================\n",
            "\n",
            "ğŸ” STEP 1: Validate Prerequisites\n",
            "----------------------------------------------------------------------\n",
            "âœ… Prerequisites validated\n",
            "   â€¢ System prompt: 1987 chars\n",
            "   â€¢ Q&A database: 20 pairs\n",
            "\n",
            "ğŸ¤– STEP 2: Load Embedding Model\n",
            "----------------------------------------------------------------------\n",
            "   Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
            "   âœ… Model loaded (embedding dim: 384)\n",
            "\n",
            "ğŸ“Š STEP 3: Generate Embeddings for Q&A Database\n",
            "----------------------------------------------------------------------\n",
            "   Generating embeddings for 20 Q&A pairs...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f3a2fdf4659f4cec8a38dc7850e6ffff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   âœ… Embeddings shape: (20, 384)\n",
            "\n",
            "ğŸ—„ï¸  STEP 4: Create FAISS Index\n",
            "----------------------------------------------------------------------\n",
            "   âœ… FAISS index created\n",
            "   â€¢ Index type: IndexFlatL2 (exact search)\n",
            "   â€¢ Vectors indexed: 20\n",
            "   â€¢ Embedding dimension: 384\n",
            "\n",
            "ğŸ§ª STEP 5: Test Retrieval with Sample Queries\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "   Testing: \"How long does shipping take?\"\n",
            "\n",
            "======================================================================\n",
            "ğŸ” RETRIEVAL RESULTS FOR: \"How long does shipping take?\"\n",
            "======================================================================\n",
            "\n",
            "ğŸ“— Result 1 (Similarity: 64.3%)\n",
            "   Category: shipping\n",
            "   Question: What is the estimated delivery time for a standard shipment?\n",
            "   Answer: Our standard shipping method takes 5-7 business days, but please note that delivery times may vary d...\n",
            "\n",
            "ğŸ“— Result 2 (Similarity: 59.2%)\n",
            "   Category: customer_service\n",
            "   Question: I'm interested in expedited shipping for an upcoming order. What options do you offer?\n",
            "   Answer: For expedited shipping, we offer Express Shipping which takes 2-3 business days. Standard shipping i...\n",
            "\n",
            "ğŸ“— Result 3 (Similarity: 47.5%)\n",
            "   Category: shipping\n",
            "   Question: How much does it cost to ship my order, and is there a free shipping threshold?\n",
            "   Answer: Shipping is free for orders over $75. For orders under $75, the shipping cost will be added at check...\n",
            "\n",
            "======================================================================\n",
            "\n",
            "   Testing: \"Can I return a product?\"\n",
            "\n",
            "======================================================================\n",
            "ğŸ” RETRIEVAL RESULTS FOR: \"Can I return a product?\"\n",
            "======================================================================\n",
            "\n",
            "ğŸ“— Result 1 (Similarity: 59.1%)\n",
            "   Category: customer_service\n",
            "   Question: I'd like to return an item. Can you tell me about your return policy?\n",
            "   Answer: We have a 30-day return policy for unopened items in their original packaging. To initiate a return,...\n",
            "\n",
            "ğŸ“— Result 2 (Similarity: 58.2%)\n",
            "   Category: returns\n",
            "   Question: I've used some of the smart home devices I purchased from GreenTech Marketplace. Can I still return them within the 30-day period?\n",
            "   Answer: I'm sorry, but our return policy only applies to unopened items in their original packaging. Since y...\n",
            "\n",
            "ğŸ“— Result 3 (Similarity: 55.1%)\n",
            "   Category: returns\n",
            "   Question: Hi, I ordered a solar panel from GreenTech Marketplace, but I've changed my mind. Can I return it within the 30-day period?\n",
            "   Answer: Absolutely! As long as the solar panel is unopened and still in its original packaging, you can retu...\n",
            "\n",
            "======================================================================\n",
            "\n",
            "   Testing: \"What are your business hours?\"\n",
            "\n",
            "======================================================================\n",
            "ğŸ” RETRIEVAL RESULTS FOR: \"What are your business hours?\"\n",
            "======================================================================\n",
            "\n",
            "ğŸ“— Result 1 (Similarity: 48.0%)\n",
            "   Category: customer_service\n",
            "   Question: What are your customer service hours?\n",
            "   Answer: Our customer service hours are Monday-Friday from 9 AM - 6 PM EST and on Saturday from 10 AM - 4 PM ...\n",
            "\n",
            "ğŸ“— Result 2 (Similarity: 39.0%)\n",
            "   Category: shipping\n",
            "   Question: What is the estimated delivery time for a standard shipment?\n",
            "   Answer: Our standard shipping method takes 5-7 business days, but please note that delivery times may vary d...\n",
            "\n",
            "ğŸ“— Result 3 (Similarity: 37.4%)\n",
            "   Category: customer_service\n",
            "   Question: I'd like to return an item. Can you tell me about your return policy?\n",
            "   Answer: We have a 30-day return policy for unopened items in their original packaging. To initiate a return,...\n",
            "\n",
            "======================================================================\n",
            "\n",
            "   Testing: \"Do you price match with competitors?\"\n",
            "\n",
            "======================================================================\n",
            "ğŸ” RETRIEVAL RESULTS FOR: \"Do you price match with competitors?\"\n",
            "======================================================================\n",
            "\n",
            "ğŸ“— Result 1 (Similarity: 40.6%)\n",
            "   Category: products\n",
            "   Question: What kind of products does GreenTech Marketplace offer?\n",
            "   Answer: GreenTech Marketplace specializes in sustainable technology products. We sell solar panels, energy-e...\n",
            "\n",
            "ğŸ“— Result 2 (Similarity: 37.5%)\n",
            "   Category: shipping\n",
            "   Question: How much does it cost to ship my order, and is there a free shipping threshold?\n",
            "   Answer: Shipping is free for orders over $75. For orders under $75, the shipping cost will be added at check...\n",
            "\n",
            "ğŸ“• Result 3 (Similarity: 37.4%)\n",
            "   Category: personal_advice\n",
            "   Question: What's your favorite brand of running shoes for someone who's training for a marathon and has flat feet?\n",
            "   Answer: I'm sorry for any inconvenience, but I'm unable to provide personal recommendations or opinions. How...\n",
            "\n",
            "======================================================================\n",
            "\n",
            "ğŸ“ STEP 6: Example - Formatted Context for LLM\n",
            "----------------------------------------------------------------------\n",
            "Query: \"What is your return policy?\"\n",
            "\n",
            "Formatted Context for LLM:\n",
            "----------------------------------------------------------------------\n",
            "[Context 1]\n",
            "Q: I'd like to return an item. Can you tell me about your return policy?\n",
            "A: We have a 30-day return policy for unopened items in their original packaging. To initiate a return, please contact our customer service team at support@greentechmarketplace.com or 1-800-GREEN-TECH during our business hours. Refunds are processed within 5-7 business days after we receive the returned item.\n",
            "\n",
            "[Context 2]\n",
            "Q: I've used some of the smart home devices I purchased from GreenTech Marketplace. Can I still return them within the 30-day period?\n",
            "A: I'm sorry, but our return policy only applies to unopened items in their original packaging. Since you've used the smart home devices, they no longer qualify for a return. However, please reach out if you have any issues or concerns with the products. Our team is here to help!\n",
            "\n",
            "[Context 3]\n",
            "Q: Hi, I ordered a solar panel from GreenTech Marketplace, but I've changed my mind. Can I return it within the 30-day period?\n",
            "A: Absolutely! As long as the solar panel is unopened and still in its original packaging, you can return it within 30 days of the purchase for a full refund.\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "ğŸ’¾ STEP 7: Save Files to data/vector_database/\n",
            "----------------------------------------------------------------------\n",
            "âœ… Embeddings saved to: data/vector_database/qa_embeddings.npy\n",
            "âœ… FAISS index saved to: data/vector_database/qa_index.faiss\n",
            "âœ… Retrieval test results saved to: data/vector_database/retrieval_test_results.csv\n",
            "\n",
            "======================================================================\n",
            "âœ… OBJECTIVE 3 COMPLETE\n",
            "======================================================================\n",
            "\n",
            "Key Concepts Demonstrated:\n",
            "  1. Embeddings - Text to vector conversion using sentence-transformers\n",
            "  2. FAISS Index - Efficient similarity search with IndexFlatL2\n",
            "  3. RAG Retrieval - Finding relevant context for user queries\n",
            "\n",
            "ğŸ“¦ FILES SAVED (for submission):\n",
            "  â€¢ data/vector_database/qa_embeddings.npy - Embedding vectors\n",
            "  â€¢ data/vector_database/qa_index.faiss - FAISS index\n",
            "  â€¢ data/vector_database/retrieval_test_results.csv - Test results\n",
            "\n",
            "ğŸ“¦ GLOBAL VARIABLES:\n",
            "  â€¢ embedding_model: SentenceTransformer model\n",
            "  â€¢ qa_embeddings: numpy array ((20, 384))\n",
            "  â€¢ faiss_index: FAISS IndexFlatL2 (20 vectors)\n",
            "\n",
            "ğŸ”œ READY FOR OBJECTIVE 4: RAG Pipeline Integration\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# ============================================================================\n",
        "# OBJECTIVE 3: IMPLEMENT VECTOR DATABASES USING FAISS\n",
        "# ============================================================================\n",
        "#\n",
        "# LEARNING OBJECTIVES DEMONSTRATED:\n",
        "#   1. Text Embeddings - Converting text to numerical vectors that capture semantic meaning\n",
        "#   2. Semantic Search - Finding relevant documents by meaning rather than keyword matching\n",
        "#   3. Vector Databases - Using FAISS for efficient similarity search in high-dimensional spaces\n",
        "#   4. Index Design - Choosing appropriate index types (IndexFlatL2) for dataset size\n",
        "#   5. RAG Retrieval - Implementing the retrieval component of RAG systems\n",
        "#\n",
        "# THEORETICAL BACKGROUND:\n",
        "#\n",
        "#   EMBEDDINGS:\n",
        "#   - Numerical vector representations capturing semantic meaning\n",
        "#   - Similar texts have similar embeddings (close in vector space)\n",
        "#   - Enables semantic search beyond keyword matching\n",
        "#   - Example: \"shipping\" and \"delivery\" have similar embeddings\n",
        "#\n",
        "#   FAISS (Facebook AI Similarity Search):\n",
        "#   - Library for efficient similarity search in high-dimensional spaces\n",
        "#   - Speed: Searches millions of vectors in milliseconds\n",
        "#   - Scalability: Can index billions of vectors\n",
        "#   - Why not regular DB? Regular DBs do exact matches, not semantic similarity\n",
        "#\n",
        "#   INDEX TYPES:\n",
        "#   - IndexFlatL2: Exact search, L2 distance (we use this for small datasets)\n",
        "#   - IndexIVFFlat: Approximate search, faster for large datasets (>100k vectors)\n",
        "#   - IndexHNSW: Graph-based, very fast approximate search for very large datasets\n",
        "#\n",
        "#   IN RAG SYSTEMS:\n",
        "#   - Convert user question to embedding using sentence-transformers\n",
        "#   - Search FAISS index for most similar Q&A embeddings\n",
        "#   - Return top-k most relevant context for LLM generation\n",
        "#\n",
        "# PREREQUISITES: Run Objective 1 and Objective 2 first\n",
        "#   - system_prompt (from Objective 1) - for validation\n",
        "#   - qa_database (from Objective 2) - 21 Q&A pairs to convert to embeddings\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "# ============================================================================\n",
        "#\n",
        "# LEARNING OBJECTIVES DEMONSTRATED:\n",
        "#   1. Embeddings - Converting text to numerical vectors for semantic search\n",
        "#   2. FAISS Vector Database - Efficient similarity search at scale\n",
        "#   3. RAG Retrieval - Finding relevant context for user queries\n",
        "#\n",
        "# THEORETICAL BACKGROUND:\n",
        "#\n",
        "#   EMBEDDINGS:\n",
        "#   - Numerical vector representations capturing semantic meaning\n",
        "#   - Similar texts have similar embeddings (close in vector space)\n",
        "#   - Enables semantic search beyond keyword matching\n",
        "#   - Example: \"shipping\" and \"delivery\" have similar embeddings\n",
        "#\n",
        "#   FAISS (Facebook AI Similarity Search):\n",
        "#   - Library for efficient similarity search in high-dimensional spaces\n",
        "#   - Speed: Searches millions of vectors in milliseconds\n",
        "#   - Scalability: Can index billions of vectors\n",
        "#   - GPU Support: Optional acceleration for faster searches\n",
        "#   - Why not regular DB? Regular DBs do exact matches, not semantic similarity\n",
        "#\n",
        "#   IN RAG SYSTEMS:\n",
        "#   - Convert user question to embedding\n",
        "#   - Search FAISS index for most similar Q&A pairs\n",
        "#   - Return top-k most relevant context for LLM\n",
        "#\n",
        "# PREREQUISITES: Run Objective 1 and Objective 2 first\n",
        "#   - system_prompt (from Objective 1)\n",
        "#   - qa_database (from Objective 2)\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 1: IMPORTS & VALIDATION\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "try:\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import faiss\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "except ImportError as e:\n",
        "    raise ImportError(f\"Missing: {e}. Run: pip install faiss-cpu sentence-transformers numpy pandas\")\n",
        "\n",
        "\n",
        "def validate_prerequisites():\n",
        "    \"\"\"Ensure Objective 1 and 2 were run first.\"\"\"\n",
        "    required = ['system_prompt', 'qa_database']\n",
        "    missing = [r for r in required if r not in globals()]\n",
        "    if missing:\n",
        "        raise RuntimeError(f\"Missing: {missing}. Run Objective 1 and 2 first.\")\n",
        "    print(\"âœ… Prerequisites validated\")\n",
        "    print(f\"   â€¢ System prompt: {len(globals()['system_prompt'])} chars\")\n",
        "    print(f\"   â€¢ Q&A database: {len(globals()['qa_database'])} pairs\")\n",
        "    return True\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 2: CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "# Output directory for FAISS index and embeddings\n",
        "OUTPUT_DIR = \"data/vector_database\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Embedding model - lightweight and efficient\n",
        "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "# Retrieval settings\n",
        "TOP_K = 3  # Number of similar documents to retrieve\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 3: EMBEDDING FUNCTIONS\n",
        "# ============================================================================\n",
        "#\n",
        "# EMBEDDING GENERATION PROCESS:\n",
        "# =============================\n",
        "# Step 1: Load pre-trained sentence transformer model\n",
        "# Step 2: Prepare text data (combine question + answer for richer embeddings)\n",
        "# Step 3: Encode texts to get embedding vectors\n",
        "# Step 4: Convert to float32 (required by FAISS)\n",
        "# Step 5: Embeddings ready for indexing\n",
        "#\n",
        "# Why sentence-transformers?\n",
        "# - Pre-trained on large text corpora\n",
        "# - Optimized for semantic similarity\n",
        "# - Easy to use API\n",
        "# - Multiple model sizes available\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "def load_embedding_model(model_name: str = EMBEDDING_MODEL) -> SentenceTransformer:\n",
        "    \"\"\"\n",
        "    Load sentence transformer model for generating embeddings.\n",
        "    \n",
        "    Model: all-MiniLM-L6-v2\n",
        "    - Dimensions: 384\n",
        "    - Speed: Fast (good for real-time applications)\n",
        "    - Quality: Good semantic understanding\n",
        "    - Memory: Low (suitable for CPU-based environments)\n",
        "    \n",
        "    Returns:\n",
        "        SentenceTransformer model ready for encoding\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    Load sentence transformer model for generating embeddings.\n",
        "    \n",
        "    Model: all-MiniLM-L6-v2\n",
        "    - Dimensions: 384\n",
        "    - Speed: Fast (good for real-time applications)\n",
        "    - Quality: Good semantic understanding\n",
        "    \"\"\"\n",
        "    print(f\"   Loading embedding model: {model_name}\")\n",
        "    model = SentenceTransformer(model_name)\n",
        "    print(f\"   âœ… Model loaded (embedding dim: {model.get_sentence_embedding_dimension()})\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def generate_embeddings(texts: List[str], model: SentenceTransformer) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generate embeddings for a list of texts.\n",
        "    \n",
        "    Converts text to 384-dimensional float32 vectors using sentence-transformers.\n",
        "    These embeddings capture semantic meaning, enabling similarity search.\n",
        "    \n",
        "    IMPLEMENTATION:\n",
        "    1. Pass texts to model.encode()\n",
        "    2. Model tokenizes and processes each text\n",
        "    3. Returns dense vector for each text\n",
        "    4. Convert to float32 for FAISS compatibility\n",
        "    \n",
        "    Args:\n",
        "        texts: List of strings to embed\n",
        "        model: SentenceTransformer model\n",
        "    \n",
        "    Returns:\n",
        "        numpy array of shape (n_texts, 384) with float32 dtype\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    Generate embeddings for a list of texts.\n",
        "    \n",
        "    IMPLEMENTATION STEPS:\n",
        "    1. Pass texts to model.encode()\n",
        "    2. Model tokenizes and processes each text\n",
        "    3. Returns dense vector for each text\n",
        "    4. Convert to float32 for FAISS compatibility\n",
        "    \n",
        "    Args:\n",
        "        texts: List of strings to embed\n",
        "        model: SentenceTransformer model\n",
        "    \n",
        "    Returns:\n",
        "        numpy array of shape (n_texts, embedding_dim)\n",
        "    \"\"\"\n",
        "    # Encode all texts - model handles batching internally\n",
        "    # show_progress_bar=True for visibility during processing\n",
        "    embeddings = model.encode(texts, show_progress_bar=True, convert_to_numpy=True)\n",
        "    \n",
        "    # FAISS requires float32 dtype\n",
        "    return embeddings.astype('float32')\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 4: FAISS INDEX FUNCTIONS\n",
        "# ============================================================================\n",
        "#\n",
        "# FAISS DATABASE IMPLEMENTATION PROCESS:\n",
        "# =====================================\n",
        "# Step 1: Get embedding dimension from the vectors\n",
        "# Step 2: Create FAISS index with appropriate type (IndexFlatL2 for exact search)\n",
        "# Step 3: Add all embedding vectors to the index\n",
        "# Step 4: Index is ready for similarity search\n",
        "# Step 5: Save index to disk for persistence\n",
        "#\n",
        "# Index Types Available:\n",
        "# - IndexFlatL2: Exact search, L2 distance (we use this)\n",
        "# - IndexFlatIP: Exact search, Inner Product (cosine similarity)\n",
        "# - IndexIVFFlat: Approximate search, faster for large datasets\n",
        "# - IndexHNSW: Graph-based, very fast approximate search\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "def create_faiss_index(embeddings: np.ndarray) -> faiss.IndexFlatL2:\n",
        "    \"\"\"\n",
        "    Create FAISS index from embeddings.\n",
        "    \n",
        "    Uses IndexFlatL2 for exact search with L2 (Euclidean) distance.\n",
        "    Ideal for small-medium datasets (<100k vectors).\n",
        "    \n",
        "    IMPLEMENTATION STEPS:\n",
        "    1. Get dimension from embedding shape\n",
        "    2. Initialize IndexFlatL2 with dimension\n",
        "    3. Add all vectors to index\n",
        "    4. Return populated index\n",
        "    \n",
        "    IndexFlatL2 Properties:\n",
        "    - L2 (Euclidean) distance for similarity\n",
        "    - Flat index = exact search (no approximation)\n",
        "    - Good for small-medium datasets (<100k vectors)\n",
        "    - For larger datasets, use IndexIVFFlat or IndexHNSW\n",
        "    \n",
        "    Args:\n",
        "        embeddings: numpy array of shape (n_vectors, embedding_dim)\n",
        "    \n",
        "    Returns:\n",
        "        FAISS IndexFlatL2 index with all vectors indexed\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    Create FAISS index from embeddings.\n",
        "    \n",
        "    IMPLEMENTATION STEPS:\n",
        "    1. Get dimension from embedding shape\n",
        "    2. Initialize IndexFlatL2 with dimension\n",
        "    3. Add all vectors to index\n",
        "    4. Return populated index\n",
        "    \n",
        "    Using IndexFlatL2:\n",
        "    - L2 (Euclidean) distance for similarity\n",
        "    - Flat index = exact search (no approximation)\n",
        "    - Good for small-medium datasets (<100k vectors)\n",
        "    - For larger datasets, use IndexIVFFlat or IndexHNSW\n",
        "    \"\"\"\n",
        "    # Step 1: Get embedding dimension\n",
        "    dimension = embeddings.shape[1]\n",
        "    \n",
        "    # Step 2: Create FAISS index\n",
        "    # IndexFlatL2 = Flat index using L2 (Euclidean) distance\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    \n",
        "    # Step 3: Add vectors to index\n",
        "    # After this, index.ntotal will equal number of vectors\n",
        "    index.add(embeddings)\n",
        "    \n",
        "    # Step 4: Return the populated index\n",
        "    return index\n",
        "\n",
        "\n",
        "def search_index(\n",
        "    query_embedding: np.ndarray,\n",
        "    index: faiss.IndexFlatL2,\n",
        "    top_k: int = TOP_K\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Search FAISS index for most similar vectors.\n",
        "    \n",
        "    Args:\n",
        "        query_embedding: Query vector (1, embedding_dim)\n",
        "        index: FAISS index\n",
        "        top_k: Number of results to return\n",
        "    \n",
        "    Returns:\n",
        "        distances: Distance scores (lower = more similar for L2)\n",
        "        indices: Indices of matched documents\n",
        "    \"\"\"\n",
        "    if len(query_embedding.shape) == 1:\n",
        "        query_embedding = query_embedding.reshape(1, -1)\n",
        "    \n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "    return distances[0], indices[0]\n",
        "\n",
        "\n",
        "def save_faiss_index(index: faiss.IndexFlatL2, filename: str = \"qa_index.faiss\"):\n",
        "    \"\"\"Save FAISS index to file.\"\"\"\n",
        "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
        "    faiss.write_index(index, filepath)\n",
        "    print(f\"âœ… FAISS index saved to: {filepath}\")\n",
        "    return filepath\n",
        "\n",
        "\n",
        "def load_faiss_index(filename: str = \"qa_index.faiss\") -> faiss.IndexFlatL2:\n",
        "    \"\"\"Load FAISS index from file.\"\"\"\n",
        "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
        "    index = faiss.read_index(filepath)\n",
        "    print(f\"âœ… FAISS index loaded from: {filepath}\")\n",
        "    return index\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 5: RAG RETRIEVAL FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def retrieve_context(\n",
        "    query: str,\n",
        "    model: SentenceTransformer,\n",
        "    index: faiss.IndexFlatL2,\n",
        "    qa_database: List[Dict],\n",
        "    top_k: int = TOP_K\n",
        "def embed_query(query: str, model: SentenceTransformer) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Convert query text to embedding vector.\n",
        "    \n",
        "    This function is used by the RAG pipeline to convert user questions\n",
        "    into embeddings for FAISS similarity search.\n",
        "    \n",
        "    Args:\n",
        "        query: User's question text\n",
        "        model: SentenceTransformer embedding model\n",
        "    \n",
        "    Returns:\n",
        "        Embedding vector (384 dimensions, float32)\n",
        "    \"\"\"\n",
        "    embedding = model.encode([query], convert_to_numpy=True).astype('float32')\n",
        "    return embedding[0]  # Return single vector, not batch\n",
        ") -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Retrieve most relevant Q&A pairs for a query.\n",
        "    \n",
        "    This is the core RAG retrieval function:\n",
        "    1. Convert query to embedding\n",
        "    2. Search FAISS index for similar embeddings\n",
        "    3. Return corresponding Q&A pairs as context\n",
        "    \n",
        "    Args:\n",
        "        query: User's question\n",
        "        model: Embedding model\n",
        "        index: FAISS index\n",
        "        qa_database: Original Q&A database\n",
        "        top_k: Number of results to return\n",
        "    \n",
        "    Returns:\n",
        "        List of relevant Q&A pairs with distance scores\n",
        "    \"\"\"\n",
        "    # Generate query embedding\n",
        "    query_embedding = model.encode([query], convert_to_numpy=True).astype('float32')\n",
        "    \n",
        "    # Search index\n",
        "    distances, indices = search_index(query_embedding, index, top_k)\n",
        "    \n",
        "    # Get corresponding Q&A pairs\n",
        "    results = []\n",
        "    for dist, idx in zip(distances, indices):\n",
        "        if idx < len(qa_database):  # Safety check\n",
        "            qa = qa_database[idx].copy()\n",
        "            qa['distance'] = float(dist)\n",
        "            qa['similarity_score'] = 1 / (1 + float(dist))  # Convert distance to similarity\n",
        "            results.append(qa)\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "def format_context_for_llm(retrieved_qa: List[Dict]) -> str:\n",
        "    \"\"\"\n",
        "    Format retrieved Q&A pairs as context for LLM.\n",
        "    \n",
        "    This context will be injected into the prompt for RAG.\n",
        "    \"\"\"\n",
        "    if not retrieved_qa:\n",
        "        return \"No relevant information found in knowledge base.\"\n",
        "    \n",
        "    context_parts = []\n",
        "    for i, qa in enumerate(retrieved_qa, 1):\n",
        "        context_parts.append(f\"[Context {i}]\")\n",
        "        context_parts.append(f\"Q: {qa['question']}\")\n",
        "        context_parts.append(f\"A: {qa['answer']}\")\n",
        "        context_parts.append(\"\")\n",
        "    \n",
        "    return \"\\n\".join(context_parts)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 6: DISPLAY & STORAGE FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def display_retrieval_results(query: str, results: List[Dict]):\n",
        "    \"\"\"Display retrieval results in a formatted way.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"ğŸ” RETRIEVAL RESULTS FOR: \\\"{query}\\\"\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    for i, qa in enumerate(results, 1):\n",
        "        similarity = qa.get('similarity_score', 0) * 100\n",
        "        answerable = 'ğŸ“—' if qa.get('answerable', True) else 'ğŸ“•'\n",
        "        \n",
        "        print(f\"\\n{answerable} Result {i} (Similarity: {similarity:.1f}%)\")\n",
        "        print(f\"   Category: {qa.get('category', 'N/A')}\")\n",
        "        print(f\"   Question: {qa.get('question', 'N/A')}\")\n",
        "        print(f\"   Answer: {qa.get('answer', 'N/A')[:100]}...\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "\n",
        "def save_embeddings(embeddings: np.ndarray, filename: str = \"qa_embeddings.npy\"):\n",
        "    \"\"\"Save embeddings to numpy file.\"\"\"\n",
        "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
        "    np.save(filepath, embeddings)\n",
        "    print(f\"âœ… Embeddings saved to: {filepath}\")\n",
        "    return filepath\n",
        "\n",
        "\n",
        "def save_retrieval_test_results(test_results: List[Dict], filename: str = \"retrieval_test_results.csv\"):\n",
        "    \"\"\"Save retrieval test results to CSV.\"\"\"\n",
        "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
        "    \n",
        "    # Flatten results for CSV\n",
        "    rows = []\n",
        "    for result in test_results:\n",
        "        for i, retrieved in enumerate(result['retrieved'], 1):\n",
        "            rows.append({\n",
        "                'query': result['query'],\n",
        "                'rank': i,\n",
        "                'category': retrieved.get('category', ''),\n",
        "                'answerable': retrieved.get('answerable', True),\n",
        "                'question': retrieved.get('question', ''),\n",
        "                'answer': retrieved.get('answer', '')[:200],\n",
        "                'similarity_score': retrieved.get('similarity_score', 0)\n",
        "            })\n",
        "    \n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv(filepath, index=False)\n",
        "    print(f\"âœ… Retrieval test results saved to: {filepath}\")\n",
        "    return filepath\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# ============================================================================\n",
        "def verify_objective3():\n",
        "    \"\"\"\n",
        "    Verify that Objective 3 completed successfully.\n",
        "    Checks all variables, embeddings, FAISS index, and files.\n",
        "    \"\"\"\n",
        "    import os\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    print(\"ğŸ” OBJECTIVE 3 VERIFICATION\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    errors = []\n",
        "    \n",
        "    # Check if variables exist\n",
        "    if 'embedding_model' not in globals():\n",
        "        errors.append(\"âŒ embedding_model not found\")\n",
        "    if 'qa_embeddings' not in globals():\n",
        "        errors.append(\"âŒ qa_embeddings not found\")\n",
        "    if 'faiss_index' not in globals():\n",
        "        errors.append(\"âŒ faiss_index not found\")\n",
        "    if 'embed_query' not in globals():\n",
        "        errors.append(\"âŒ embed_query function not found\")\n",
        "    \n",
        "    if errors:\n",
        "        print(\"\\n\".join(errors))\n",
        "        print(\"=\"*70)\n",
        "        return False\n",
        "    \n",
        "    # Check embeddings shape\n",
        "    if qa_embeddings.shape[0] != 21:\n",
        "        errors.append(f\"âŒ Expected 21 embeddings, got {qa_embeddings.shape[0]}\")\n",
        "    if qa_embeddings.shape[1] != 384:\n",
        "        errors.append(f\"âŒ Expected 384 dimensions, got {qa_embeddings.shape[1]}\")\n",
        "    if qa_embeddings.dtype != 'float32':\n",
        "        errors.append(f\"âŒ Expected float32 dtype, got {qa_embeddings.dtype}\")\n",
        "    \n",
        "    # Check FAISS index\n",
        "    if faiss_index.ntotal != 21:\n",
        "        errors.append(f\"âŒ Expected 21 vectors in index, got {faiss_index.ntotal}\")\n",
        "    if faiss_index.d != 384:\n",
        "        errors.append(f\"âŒ Expected 384 dimensions in index, got {faiss_index.d}\")\n",
        "    \n",
        "    # Check files exist\n",
        "    if not os.path.exists(\"data/vector_database/qa_embeddings.npy\"):\n",
        "        errors.append(\"âŒ qa_embeddings.npy not found\")\n",
        "    if not os.path.exists(\"data/vector_database/qa_index.faiss\"):\n",
        "        errors.append(\"âŒ qa_index.faiss not found\")\n",
        "    \n",
        "    # Test embed_query function\n",
        "    try:\n",
        "        test_embedding = embed_query(\"test query\")\n",
        "        if test_embedding.shape != (384,):\n",
        "            errors.append(f\"âŒ embed_query() returned wrong shape: {test_embedding.shape}\")\n",
        "    except Exception as e:\n",
        "        errors.append(f\"âŒ embed_query() test failed: {e}\")\n",
        "    \n",
        "    # Print results\n",
        "    if errors:\n",
        "        print(\"\\nâŒ VERIFICATION FAILED:\")\n",
        "        print(\"\\n\".join(errors))\n",
        "        print(\"=\"*70)\n",
        "        return False\n",
        "    else:\n",
        "        print(\"\\nâœ… Objective 3 Complete - All variables and files verified\")\n",
        "        print(f\"   â€¢ Embedding Model: {embedding_model.get_sentence_embedding_dimension()} dimensions\")\n",
        "        print(f\"   â€¢ Embeddings: {qa_embeddings.shape[0]} vectors Ã— {qa_embeddings.shape[1]} dimensions\")\n",
        "        print(f\"   â€¢ FAISS Index: {faiss_index.ntotal} vectors indexed\")\n",
        "        print(f\"   â€¢ embed_query(): Ready\")\n",
        "        print(f\"   â€¢ Files: Saved to data/vector_database/\")\n",
        "        print(\"=\"*70)\n",
        "        return True\n",
        "\n",
        "\n",
        "# SECTION 7: EXECUTION\n",
        "# ============================================================================\n",
        "#\n",
        "# FAISS VECTOR DATABASE IMPLEMENTATION\n",
        "# ====================================\n",
        "# This section demonstrates the complete process of:\n",
        "# 1. Validating prerequisites (qa_database from Objective 2)\n",
        "# 2. Loading embedding model (sentence-transformers/all-MiniLM-L6-v2)\n",
        "# 3. Generating embeddings for all Q&A pairs (384-dimensional vectors)\n",
        "# 4. Creating FAISS IndexFlatL2 index for similarity search\n",
        "# 5. Testing retrieval with sample queries\n",
        "# 6. Saving embeddings and index for persistence\n",
        "# 7. Creating embed_query() function for RAG pipeline\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "# ============================================================================\n",
        "#\n",
        "# FAISS VECTOR DATABASE IMPLEMENTATION DEMONSTRATION\n",
        "# ==================================================\n",
        "# This section demonstrates the complete process of:\n",
        "# 1. Converting Q&A pairs to embeddings\n",
        "# 2. Creating and populating a FAISS index\n",
        "# 3. Testing similarity search retrieval\n",
        "# 4. Saving all artifacts for persistence\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"   OBJECTIVE 3: IMPLEMENT VECTOR DATABASE USING FAISS\")\n",
        "print(\"   Building Semantic Search for RAG System\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# --- Step 1: Validate Prerequisites ---\n",
        "print(\"\\nğŸ” STEP 1: Validate Prerequisites\")\n",
        "print(\"-\"*70)\n",
        "validate_prerequisites()\n",
        "\n",
        "qa_database = globals()['qa_database']\n",
        "\n",
        "# --- Step 2: Load Embedding Model ---\n",
        "# ============================================================================\n",
        "# COMMENT: Loading the sentence-transformers model that will convert\n",
        "#          our Q&A text into numerical vectors (embeddings)\n",
        "# ============================================================================\n",
        "print(\"\\nğŸ¤– STEP 2: Load Embedding Model\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "embedding_model = load_embedding_model()\n",
        "globals()['embedding_model'] = embedding_model\n",
        "\n",
        "# --- Step 3: Generate Embeddings for Q&A Database ---\n",
        "# ============================================================================\n",
        "# COMMENT: Converting our Q&A database into embeddings\n",
        "#          - Each Q&A pair becomes a 384-dimensional vector\n",
        "#          - We combine question + answer for richer semantic representation\n",
        "#          - These embeddings will be stored in FAISS index\n",
        "# ============================================================================\n",
        "print(\"\\nğŸ“Š STEP 3: Generate Embeddings for Q&A Database\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "# Combine question and answer for richer embeddings\n",
        "qa_texts = [f\"{qa['question']} {qa['answer']}\" for qa in qa_database]\n",
        "print(f\"   Generating embeddings for {len(qa_texts)} Q&A pairs...\")\n",
        "\n",
        "qa_embeddings = generate_embeddings(qa_texts, embedding_model)\n",
        "globals()['qa_embeddings'] = qa_embeddings\n",
        "\n",
        "print(f\"   âœ… Embeddings shape: {qa_embeddings.shape}\")\n",
        "\n",
        "# --- Step 4: Create FAISS Index ---\n",
        "# ============================================================================\n",
        "# COMMENT: Creating the FAISS vector database\n",
        "#          - IndexFlatL2 uses L2 (Euclidean) distance for similarity\n",
        "#          - All embeddings are added to the index\n",
        "#          - Index enables fast similarity search (milliseconds)\n",
        "# ============================================================================\n",
        "print(\"\\nğŸ—„ï¸  STEP 4: Create FAISS Index\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "# Create FAISS index and add embeddings\n",
        "faiss_index = create_faiss_index(qa_embeddings)\n",
        "globals()['faiss_index'] = faiss_index\n",
        "\n",
        "# Create embed_query function for RAG pipeline (used in Objective 4)\n",
        "def embed_query(query: str) -> np.ndarray:\n",
        "    \"\"\"Convert query text to embedding vector for FAISS search.\"\"\"\n",
        "    return embedding_model.encode([query], convert_to_numpy=True).astype('float32')[0]\n",
        "\n",
        "globals()['embed_query'] = embed_query\n",
        "    \"\"\"Convert query text to embedding vector for FAISS search.\"\"\"\n",
        "    return embedding_model.encode([query], convert_to_numpy=True).astype('float32')[0]\n",
        "\n",
        "globals()['embed_query'] = embed_query\n",
        "\n",
        "# Create embed_query function for RAG pipeline\n",
        "    \"\"\"Convert query text to embedding vector for FAISS search.\"\"\"\n",
        "    return embedding_model.encode([query], convert_to_numpy=True).astype('float32')[0]\n",
        "\n",
        "globals()['embed_query'] = embed_query\n",
        "\n",
        "print(f\"   âœ… FAISS index created\")\n",
        "print(f\"   â€¢ Index type: IndexFlatL2 (exact search)\")\n",
        "print(f\"   â€¢ Vectors indexed: {faiss_index.ntotal}\")\n",
        "print(f\"   â€¢ Embedding dimension: {qa_embeddings.shape[1]}\")\n",
        "\n",
        "# --- Step 5: Test Retrieval ---\n",
        "# ============================================================================\n",
        "# COMMENT: Testing the vector database with sample queries\n",
        "#          - Each query is converted to an embedding\n",
        "#          - FAISS finds the top-k most similar Q&A pairs\n",
        "#          - Results demonstrate semantic search capability\n",
        "# ============================================================================\n",
        "print(\"\\nğŸ§ª STEP 5: Test Retrieval with Sample Queries\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "test_queries = [\n",
        "    \"How long does shipping take?\",\n",
        "    \"Can I return a product?\",\n",
        "    \"What are your business hours?\",\n",
        "    \"Do you price match with competitors?\",  # Unanswerable\n",
        "]\n",
        "\n",
        "test_results = []\n",
        "\n",
        "for query in test_queries:\n",
        "    print(f\"\\n   Testing: \\\"{query}\\\"\")\n",
        "    \n",
        "    retrieved = retrieve_context(\n",
        "        query=query,\n",
        "        model=embedding_model,\n",
        "        index=faiss_index,\n",
        "        qa_database=qa_database,\n",
        "        top_k=TOP_K\n",
        "    )\n",
        "    \n",
        "    test_results.append({\n",
        "        'query': query,\n",
        "        'retrieved': retrieved\n",
        "    })\n",
        "    \n",
        "    display_retrieval_results(query, retrieved)\n",
        "\n",
        "# --- Step 6: Show Formatted Context for LLM ---\n",
        "print(\"\\nğŸ“ STEP 6: Example - Formatted Context for LLM\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "sample_query = \"What is your return policy?\"\n",
        "sample_retrieved = retrieve_context(sample_query, embedding_model, faiss_index, qa_database, TOP_K)\n",
        "formatted_context = format_context_for_llm(sample_retrieved)\n",
        "\n",
        "print(f\"Query: \\\"{sample_query}\\\"\\n\")\n",
        "print(\"Formatted Context for LLM:\")\n",
        "print(\"-\"*70)\n",
        "print(formatted_context)\n",
        "print(\"-\"*70)\n",
        "\n",
        "# --- Step 7: Save All Artifacts ---\n",
        "print(\"\\nğŸ’¾ STEP 7: Save Files to data/vector_database/\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "save_embeddings(qa_embeddings)\n",
        "save_faiss_index(faiss_index)\n",
        "save_retrieval_test_results(test_results)\n",
        "\n",
        "# --- Step 8: Verify Objective 3 ---\n",
        "print(\"\\nâœ… STEP 8: Verify Objective 3\")\n",
        "print(\"-\"*70)\n",
        "verify_objective3()\n",
        "\n",
        "\n",
        "# --- Summary ---\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âœ… OBJECTIVE 3 COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\"\"\n",
        "Key Concepts Demonstrated:\n",
        "  1. Embeddings - Text to vector conversion using sentence-transformers\n",
        "  2. FAISS Index - Efficient similarity search with IndexFlatL2\n",
        "  3. RAG Retrieval - Finding relevant context for user queries\n",
        "\n",
        "ğŸ“¦ FILES SAVED (for submission):\n",
        "  â€¢ {OUTPUT_DIR}/qa_embeddings.npy - Embedding vectors\n",
        "  â€¢ {OUTPUT_DIR}/qa_index.faiss - FAISS index\n",
        "  â€¢ {OUTPUT_DIR}/retrieval_test_results.csv - Test results\n",
        "\n",
        "ğŸ“¦ GLOBAL VARIABLES:\n",
        "  â€¢ embedding_model: SentenceTransformer model\n",
        "  â€¢ qa_embeddings: numpy array ({qa_embeddings.shape})\n",
        "  â€¢ faiss_index: FAISS IndexFlatL2 ({faiss_index.ntotal} vectors)\n",
        "\n",
        "ğŸ”œ READY FOR OBJECTIVE 4: RAG Pipeline Integration\n",
        "\"\"\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Objective 4: Build Complete RAG Pipeline\n",
        "\n",
        "### Design Choices & Rationale\n",
        "\n",
        "### Pipeline Architecture\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                      RAG PIPELINE FLOW                          â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                 â”‚\n",
        "â”‚  User Query                                                     â”‚\n",
        "â”‚      â”‚                                                          â”‚\n",
        "â”‚      â–¼                                                          â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚\n",
        "â”‚  â”‚ 1. embed_query()â”‚  Convert query to embedding                â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                            â”‚\n",
        "â”‚           â–¼                                                     â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚\n",
        "â”‚  â”‚ 2. search_faiss()â”‚  Find similar Q&A pairs                   â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                            â”‚\n",
        "â”‚           â–¼                                                     â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚\n",
        "â”‚  â”‚ 3. format_context()â”‚  Format as context string               â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚\n",
        "â”‚           â–¼                                                     â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚\n",
        "â”‚  â”‚ 4. build_prompt()â”‚  Combine system + context + query         â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                            â”‚\n",
        "â”‚           â–¼                                                     â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚\n",
        "â”‚  â”‚ 5. generate_response()â”‚  Generate with Mistral               â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚\n",
        "â”‚           â–¼                                                     â”‚\n",
        "â”‚      Response                                                   â”‚\n",
        "â”‚                                                                 â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### Pipeline Components & Reuse\n",
        "\n",
        "| Step | Function | Purpose | Reuses From |\n",
        "|------|----------|---------|-------------|\n",
        "| 1 | `embed_query()` | Convert user question to embedding | `embedding_model` (Obj 3) |\n",
        "| 2 | `search_faiss()` | Find top-k similar Q&A pairs | `faiss_index` (Obj 3), `qa_database` (Obj 2) |\n",
        "| 3 | `format_context()` | Format retrieved Q&A as context string | - |\n",
        "| 4 | `build_prompt()` | Combine system prompt + context + query | `system_prompt` (Obj 1) |\n",
        "| 5 | `generate_response()` | Generate answer with Mistral | `mistral_model`, `mistral_tokenizer` (Obj 1) |\n",
        "\n",
        "**Why This Architecture:**\n",
        "- RAG combines the best of retrieval (accurate, up-to-date information) with generation (natural language responses)\n",
        "- Grounds answers in knowledge base, reducing hallucinations\n",
        "- Allows for easy updates to knowledge base without retraining models\n",
        "\n",
        "**Error Handling:**\n",
        "- Implement fallbacks for cases where no relevant context is found\n",
        "- Handle edge cases gracefully with appropriate error messages\n",
        "- RAGResult includes success flag and error_message for debugging\n",
        "\n",
        "**Context Window Management:**\n",
        "- Limit retrieved context to top-k (default: 3) most relevant documents to avoid overwhelming the model\n",
        "- Balance between enough context for accuracy and not exceeding model limits\n",
        "- Similarity threshold (0.3) filters out low-relevance results\n",
        "\n",
        "### Configuration\n",
        "\n",
        "```python\n",
        "RAG_CONFIG = {\n",
        "    \"top_k\": 3,                    # Number of documents to retrieve\n",
        "    \"max_new_tokens\": 300,         # Max tokens for generation\n",
        "    \"temperature\": 0.7,            # Generation temperature\n",
        "    \"similarity_threshold\": 0.3,   # Minimum similarity score\n",
        "}\n",
        "```\n",
        "\n",
        "**Parameter Explanations:**\n",
        "\n",
        "| Parameter | Value | Description | Trade-offs |\n",
        "|-----------|-------|-------------|------------|\n",
        "| `top_k` | 3 | Number of documents to retrieve from FAISS | Higher (5-10): More context but may include noise. Lower (1-2): Focused but may miss info. |\n",
        "| `max_new_tokens` | 300 | Maximum tokens the model can generate | Higher (500+): Detailed responses, slower. Lower (100): Concise, faster. |\n",
        "| `temperature` | 0.7 | Controls randomness in generation (0.0-1.0) | 0.0: Deterministic/factual. 0.7: Balanced. 1.0+: Creative/random. |\n",
        "| `similarity_threshold` | 0.3 | Minimum similarity score to include document | Higher (0.5+): Only very relevant, may return none. Lower (0.1): More inclusive. |\n",
        "\n",
        "**Why These Defaults:**\n",
        "- **top_k=3**: With 21 Q&A pairs, retrieving 3 documents provides good coverage without overwhelming the context window\n",
        "- **max_new_tokens=300**: Allows comprehensive customer service responses without being overly verbose\n",
        "- **temperature=0.7**: Balanced setting for natural-sounding responses while staying factual\n",
        "- **similarity_threshold=0.3**: Filters clearly irrelevant results while being inclusive enough for semantic variations\n",
        "\n",
        "### Test Questions\n",
        "\n",
        "**Answerable Questions (5):** Questions that CAN be answered from our knowledge base\n",
        "\n",
        "| # | Question | Tests |\n",
        "|---|----------|-------|\n",
        "| 1 | \"What is your return policy?\" | Returns category retrieval |\n",
        "| 2 | \"How long does shipping take?\" | Shipping category retrieval |\n",
        "| 3 | \"What are your customer service hours?\" | Customer service retrieval |\n",
        "| 4 | \"Do you offer warranty on products?\" | Warranty category retrieval |\n",
        "| 5 | \"How can I track my order?\" | Orders category retrieval |\n",
        "\n",
        "**Unanswerable Questions (5):** Questions that CANNOT be answered - tests graceful handling\n",
        "\n",
        "| # | Question | Why Unanswerable |\n",
        "|---|----------|------------------|\n",
        "| 1 | \"How do your prices compare to Amazon?\" | Competitor info not in KB |\n",
        "| 2 | \"Should I buy solar panels for my house?\" | Personal advice not provided |\n",
        "| 3 | \"Will you have a Black Friday sale this year?\" | Future events unknown |\n",
        "| 4 | \"What is the CEO's email address?\" | Internal info not in KB |\n",
        "| 5 | \"Can you recommend a good restaurant nearby?\" | Out of scope |\n",
        "\n",
        "### Output Files\n",
        "\n",
        "Saved to: `data/rag_pipeline/`\n",
        "\n",
        "| File | Description |\n",
        "|------|-------------|\n",
        "| `rag_test_results.csv` | Results from all test queries |\n",
        "| `pipeline_config.txt` | Pipeline configuration settings |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "   OBJECTIVE 4: BUILD COMPLETE RAG PIPELINE\n",
            "======================================================================\n",
            "\n",
            "ğŸ” STEP 1: Validate Prerequisites\n",
            "----------------------------------------------------------------------\n",
            "âœ… All prerequisites validated\n",
            "   â€¢ System prompt: 1987 chars\n",
            "   â€¢ Q&A database: 20 pairs\n",
            "   â€¢ FAISS index: 20 vectors\n",
            "\n",
            "ğŸ“ STEP 2: RAG Pipeline Architecture\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
            "â”‚                      RAG PIPELINE FLOW                          â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚                                                                 â”‚\n",
            "â”‚  User Query                                                     â”‚\n",
            "â”‚      â”‚                                                          â”‚\n",
            "â”‚      â–¼                                                          â”‚\n",
            "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚\n",
            "â”‚  â”‚ 1. embed_query()â”‚  Convert query to embedding                â”‚\n",
            "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                            â”‚\n",
            "â”‚           â–¼                                                     â”‚\n",
            "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚\n",
            "â”‚  â”‚ 2. search_faiss()â”‚  Find similar Q&A pairs                   â”‚\n",
            "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                            â”‚\n",
            "â”‚           â–¼                                                     â”‚\n",
            "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚\n",
            "â”‚  â”‚ 3. format_context()â”‚  Format as context string               â”‚\n",
            "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚\n",
            "â”‚           â–¼                                                     â”‚\n",
            "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚\n",
            "â”‚  â”‚ 4. build_prompt()â”‚  Combine system + context + query         â”‚\n",
            "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                            â”‚\n",
            "â”‚           â–¼                                                     â”‚\n",
            "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚\n",
            "â”‚  â”‚ 5. generate_response()â”‚  Generate with Mistral               â”‚\n",
            "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚\n",
            "â”‚           â–¼                                                     â”‚\n",
            "â”‚      Response                                                   â”‚\n",
            "â”‚                                                                 â”‚\n",
            "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
            "\n",
            "\n",
            "ğŸ§ª STEP 3: Test with ANSWERABLE Questions\n",
            "----------------------------------------------------------------------\n",
            "   These questions CAN be answered from our knowledge base\n",
            "\n",
            "======================================================================\n",
            "ğŸ“— ANSWERABLE: \"What is your return policy?\"\n",
            "======================================================================\n",
            "   Step 1: embed_query() - Converting query to embedding...\n",
            "   Step 2: search_faiss() - Searching for similar Q&A pairs...\n",
            "           â†’ Found 3 relevant documents\n",
            "   Step 3: format_context() - Formatting retrieved context...\n",
            "   Step 4: build_prompt() - Building augmented prompt...\n",
            "   Step 5: generate_response() - Generating response with Mistral...\n",
            "           â†’ Response generated: 480 chars\n",
            "\n",
            "======================================================================\n",
            "ğŸ¤– RAG PIPELINE RESULT\n",
            "======================================================================\n",
            "\n",
            "ğŸ“¥ USER QUERY:\n",
            "   What is your return policy?\n",
            "\n",
            "ğŸ“š RETRIEVED CONTEXT (3 sources):\n",
            "   ğŸ“— [1] customer_service (Similarity: 61%)\n",
            "       Q: I'd like to return an item. Can you tell me about your retur...\n",
            "   ğŸ“— [2] returns (Similarity: 51%)\n",
            "       Q: I've used some of the smart home devices I purchased from Gr...\n",
            "   ğŸ“— [3] returns (Similarity: 50%)\n",
            "       Q: Hi, I ordered a solar panel from GreenTech Marketplace, but ...\n",
            "\n",
            "ğŸ“¤ GENERATED RESPONSE:\n",
            "----------------------------------------------------------------------\n",
            "Hello there! I'm glad to assist you with your inquiry. At GreenTech Marketplace, we have a 30-day return policy for unopened items that are still in their original packaging. To initiate a return, please reach out to our customer service team at support@greentechmarketplace.com or 1-800-GREEN-TECH during our business hours. Refunds are processed within 5-7 business days after we receive the returned item. If you have any questions or need further assistance, feel free to ask!\n",
            "----------------------------------------------------------------------\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "ğŸ“— ANSWERABLE: \"How long does shipping take?\"\n",
            "======================================================================\n",
            "   Step 1: embed_query() - Converting query to embedding...\n",
            "   Step 2: search_faiss() - Searching for similar Q&A pairs...\n",
            "           â†’ Found 3 relevant documents\n",
            "   Step 3: format_context() - Formatting retrieved context...\n",
            "   Step 4: build_prompt() - Building augmented prompt...\n",
            "   Step 5: generate_response() - Generating response with Mistral...\n",
            "           â†’ Response generated: 453 chars\n",
            "\n",
            "======================================================================\n",
            "ğŸ¤– RAG PIPELINE RESULT\n",
            "======================================================================\n",
            "\n",
            "ğŸ“¥ USER QUERY:\n",
            "   How long does shipping take?\n",
            "\n",
            "ğŸ“š RETRIEVED CONTEXT (3 sources):\n",
            "   ğŸ“— [1] shipping (Similarity: 64%)\n",
            "       Q: What is the estimated delivery time for a standard shipment?...\n",
            "   ğŸ“— [2] customer_service (Similarity: 59%)\n",
            "       Q: I'm interested in expedited shipping for an upcoming order. ...\n",
            "   ğŸ“— [3] shipping (Similarity: 48%)\n",
            "       Q: How much does it cost to ship my order, and is there a free ...\n",
            "\n",
            "ğŸ“¤ GENERATED RESPONSE:\n",
            "----------------------------------------------------------------------\n",
            "Hello! Thank you for reaching out to GreenTech Marketplace.\n",
            "\n",
            "Our standard shipping method takes 5-7 business days, but delivery times may vary depending on your location. If you require a faster delivery, we also offer Express Shipping which takes 2-3 business days for an additional fee.\n",
            "\n",
            "If you have any other questions or need further assistance, please don't hesitate to ask. We're always here to help!\n",
            "\n",
            "Best regards,\n",
            "The GreenTech Marketplace Team.\n",
            "----------------------------------------------------------------------\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "ğŸ“— ANSWERABLE: \"What are your customer service hours?\"\n",
            "======================================================================\n",
            "   Step 1: embed_query() - Converting query to embedding...\n",
            "   Step 2: search_faiss() - Searching for similar Q&A pairs...\n",
            "           â†’ Found 3 relevant documents\n",
            "   Step 3: format_context() - Formatting retrieved context...\n",
            "   Step 4: build_prompt() - Building augmented prompt...\n",
            "   Step 5: generate_response() - Generating response with Mistral...\n",
            "           â†’ Response generated: 507 chars\n",
            "\n",
            "======================================================================\n",
            "ğŸ¤– RAG PIPELINE RESULT\n",
            "======================================================================\n",
            "\n",
            "ğŸ“¥ USER QUERY:\n",
            "   What are your customer service hours?\n",
            "\n",
            "ğŸ“š RETRIEVED CONTEXT (3 sources):\n",
            "   ğŸ“— [1] customer_service (Similarity: 55%)\n",
            "       Q: What are your customer service hours?...\n",
            "   ğŸ“— [2] shipping (Similarity: 40%)\n",
            "       Q: What is the estimated delivery time for a standard shipment?...\n",
            "   ğŸ“— [3] customer_service (Similarity: 40%)\n",
            "       Q: I'm interested in expedited shipping for an upcoming order. ...\n",
            "\n",
            "ğŸ“¤ GENERATED RESPONSE:\n",
            "----------------------------------------------------------------------\n",
            "Our customer service hours are Monday-Friday from 9 AM - 6 PM EST and on Saturday from 10 AM - 4 PM EST. You can reach us during these hours via email at support@greentechmarketplace.com, by phone at 1-800-GREEN-TECH, or through live chat during business hours. We're here to help with any questions or concerns you may have. If you need assistance outside of these hours, please check our FAQs or leave a message, and we'll get back to you as soon as possible. Thank you for choosing GreenTech Marketplace!\n",
            "----------------------------------------------------------------------\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "ğŸ“— ANSWERABLE: \"Do you offer warranty on products?\"\n",
            "======================================================================\n",
            "   Step 1: embed_query() - Converting query to embedding...\n",
            "   Step 2: search_faiss() - Searching for similar Q&A pairs...\n",
            "           â†’ Found 3 relevant documents\n",
            "   Step 3: format_context() - Formatting retrieved context...\n",
            "   Step 4: build_prompt() - Building augmented prompt...\n",
            "   Step 5: generate_response() - Generating response with Mistral...\n",
            "           â†’ Response generated: 456 chars\n",
            "\n",
            "======================================================================\n",
            "ğŸ¤– RAG PIPELINE RESULT\n",
            "======================================================================\n",
            "\n",
            "ğŸ“¥ USER QUERY:\n",
            "   Do you offer warranty on products?\n",
            "\n",
            "ğŸ“š RETRIEVED CONTEXT (3 sources):\n",
            "   ğŸ“— [1] warranty (Similarity: 50%)\n",
            "       Q: What does the warranty cover for the energy-efficient applia...\n",
            "   ğŸ“— [2] warranty (Similarity: 48%)\n",
            "       Q: I'm having an issue with a product I purchased and I'd like ...\n",
            "   ğŸ“— [3] products (Similarity: 46%)\n",
            "       Q: Can you tell me more about the solar panels you offer?...\n",
            "\n",
            "ğŸ“¤ GENERATED RESPONSE:\n",
            "----------------------------------------------------------------------\n",
            "Absolutely! We're committed to the quality of our products and offer warranties for manufacturing defects. The length of the warranty varies depending on the product category, but it typically ranges from 1 to 3 years. If you have any specific questions about the warranty for a product you've purchased, please don't hesitate to reach out to our customer service team at support@greentechmarketplace.com or call us at 1-800-GREEN-TECH. We're here to help!\n",
            "----------------------------------------------------------------------\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "ğŸ“— ANSWERABLE: \"How can I track my order?\"\n",
            "======================================================================\n",
            "   Step 1: embed_query() - Converting query to embedding...\n",
            "   Step 2: search_faiss() - Searching for similar Q&A pairs...\n",
            "           â†’ Found 3 relevant documents\n",
            "   Step 3: format_context() - Formatting retrieved context...\n",
            "   Step 4: build_prompt() - Building augmented prompt...\n",
            "   Step 5: generate_response() - Generating response with Mistral...\n",
            "           â†’ Response generated: 818 chars\n",
            "\n",
            "======================================================================\n",
            "ğŸ¤– RAG PIPELINE RESULT\n",
            "======================================================================\n",
            "\n",
            "ğŸ“¥ USER QUERY:\n",
            "   How can I track my order?\n",
            "\n",
            "ğŸ“š RETRIEVED CONTEXT (3 sources):\n",
            "   ğŸ“— [1] shipping (Similarity: 72%)\n",
            "       Q: Can I track my order once it's been shipped, and how do I do...\n",
            "   ğŸ“— [2] orders (Similarity: 56%)\n",
            "       Q: Can you help me check the status of my order? I placed it a ...\n",
            "   ğŸ“— [3] returns (Similarity: 45%)\n",
            "       Q: I've received an incorrect item in my order. How can I initi...\n",
            "\n",
            "ğŸ“¤ GENERATED RESPONSE:\n",
            "----------------------------------------------------------------------\n",
            "Hello there!\n",
            "\n",
            "Thank you for reaching out to GreenTech Marketplace. I'm more than happy to help you track your order. You will receive a tracking number once your order has been shipped. You can track your package on our website or via email. To track your order on our website, simply log into your account and go to the 'Orders' section. If you need further assistance, feel free to contact our customer service team.\n",
            "\n",
            "If you haven't received your tracking number yet, please check your email for your order confirmation, as it should be included there. If you still can't find it, don't hesitate to contact us, and we'll be glad to help you out!\n",
            "\n",
            "Thank you for choosing GreenTech Marketplace for your sustainable technology needs. If there's anything else I can assist you with, please let me know. Have a great day!\n",
            "----------------------------------------------------------------------\n",
            "======================================================================\n",
            "\n",
            "ğŸ§ª STEP 4: Test with UNANSWERABLE Questions\n",
            "----------------------------------------------------------------------\n",
            "   These questions CANNOT be answered from our knowledge base\n",
            "   Testing system limitations and graceful handling\n",
            "\n",
            "======================================================================\n",
            "ğŸ“• UNANSWERABLE: \"How do your prices compare to Amazon?\"\n",
            "======================================================================\n",
            "   Step 1: embed_query() - Converting query to embedding...\n",
            "   Step 2: search_faiss() - Searching for similar Q&A pairs...\n",
            "           â†’ Found 3 relevant documents\n",
            "   Step 3: format_context() - Formatting retrieved context...\n",
            "   Step 4: build_prompt() - Building augmented prompt...\n",
            "   Step 5: generate_response() - Generating response with Mistral...\n",
            "           â†’ Response generated: 481 chars\n",
            "\n",
            "======================================================================\n",
            "ğŸ¤– RAG PIPELINE RESULT\n",
            "======================================================================\n",
            "\n",
            "ğŸ“¥ USER QUERY:\n",
            "   How do your prices compare to Amazon?\n",
            "\n",
            "ğŸ“š RETRIEVED CONTEXT (3 sources):\n",
            "   ğŸ“— [1] shipping (Similarity: 42%)\n",
            "       Q: How much does it cost to ship my order, and is there a free ...\n",
            "   ğŸ“— [2] customer_service (Similarity: 39%)\n",
            "       Q: I'm interested in expedited shipping for an upcoming order. ...\n",
            "   ğŸ“— [3] warranty (Similarity: 38%)\n",
            "       Q: What does the warranty cover for the energy-efficient applia...\n",
            "\n",
            "ğŸ“¤ GENERATED RESPONSE:\n",
            "----------------------------------------------------------------------\n",
            "Hello there!\n",
            "\n",
            "I'm glad to have you at GreenTech Marketplace. While I don't have direct price comparison information with Amazon, I can assure you that our commitment to offering high-quality, sustainable technology products is unmatched. We strive to provide competitive pricing while ensuring the best value for our customers. If you have any other questions or need further assistance, I'm here to help!\n",
            "\n",
            "Best regards,\n",
            "[Your Name]\n",
            "GreenTech Marketplace Customer Service Assistant\n",
            "----------------------------------------------------------------------\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "ğŸ“• UNANSWERABLE: \"Should I buy solar panels for my house?\"\n",
            "======================================================================\n",
            "   Step 1: embed_query() - Converting query to embedding...\n",
            "   Step 2: search_faiss() - Searching for similar Q&A pairs...\n",
            "           â†’ Found 3 relevant documents\n",
            "   Step 3: format_context() - Formatting retrieved context...\n",
            "   Step 4: build_prompt() - Building augmented prompt...\n",
            "   Step 5: generate_response() - Generating response with Mistral...\n",
            "           â†’ Response generated: 1058 chars\n",
            "\n",
            "======================================================================\n",
            "ğŸ¤– RAG PIPELINE RESULT\n",
            "======================================================================\n",
            "\n",
            "ğŸ“¥ USER QUERY:\n",
            "   Should I buy solar panels for my house?\n",
            "\n",
            "ğŸ“š RETRIEVED CONTEXT (3 sources):\n",
            "   ğŸ“— [1] products (Similarity: 55%)\n",
            "       Q: Can you tell me more about the solar panels you offer?...\n",
            "   ğŸ“— [2] returns (Similarity: 46%)\n",
            "       Q: Hi, I ordered a solar panel from GreenTech Marketplace, but ...\n",
            "   ğŸ“— [3] warranty (Similarity: 45%)\n",
            "       Q: How long does the warranty last on the solar panels I purcha...\n",
            "\n",
            "ğŸ“¤ GENERATED RESPONSE:\n",
            "----------------------------------------------------------------------\n",
            "Hello there! I'm glad to help with your question about solar panels. Solar panels are an excellent choice for those looking to reduce their carbon footprint and save on energy costs. At GreenTech Marketplace, we offer a variety of solar panels that are designed to be durable, efficient, and environmentally friendly. They come with a warranty of 1-3 years depending on the specific model.\n",
            "\n",
            "If you're considering solar panels for your home, it's important to consider factors such as your energy consumption, roof orientation, and local weather conditions. You may also want to research any local incentives or rebates that could help offset the initial investment.\n",
            "\n",
            "Should you decide to purchase a solar panel from us, we offer free shipping on orders over $75 and a 30-day return policy for unopened items. If you have any more questions or would like to discuss specific solar panel options, please don't hesitate to reach out. We're here to help you make the best decision for your sustainable energy needs.\n",
            "\n",
            "Thank you for choosing GreenTech Marketplace!\n",
            "----------------------------------------------------------------------\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "ğŸ“• UNANSWERABLE: \"Will you have a Black Friday sale this year?\"\n",
            "======================================================================\n",
            "   Step 1: embed_query() - Converting query to embedding...\n",
            "   Step 2: search_faiss() - Searching for similar Q&A pairs...\n",
            "           â†’ Found 3 relevant documents\n",
            "   Step 3: format_context() - Formatting retrieved context...\n",
            "   Step 4: build_prompt() - Building augmented prompt...\n",
            "   Step 5: generate_response() - Generating response with Mistral...\n",
            "           â†’ Response generated: 458 chars\n",
            "\n",
            "======================================================================\n",
            "ğŸ¤– RAG PIPELINE RESULT\n",
            "======================================================================\n",
            "\n",
            "ğŸ“¥ USER QUERY:\n",
            "   Will you have a Black Friday sale this year?\n",
            "\n",
            "ğŸ“š RETRIEVED CONTEXT (3 sources):\n",
            "   ğŸ“— [1] orders (Similarity: 40%)\n",
            "       Q: I need to cancel my order. I changed my mind about the purch...\n",
            "   ğŸ“— [2] returns (Similarity: 39%)\n",
            "       Q: I've used some of the smart home devices I purchased from Gr...\n",
            "   ğŸ“— [3] customer_service (Similarity: 39%)\n",
            "       Q: I'd like to return an item. Can you tell me about your retur...\n",
            "\n",
            "ğŸ“¤ GENERATED RESPONSE:\n",
            "----------------------------------------------------------------------\n",
            "While I don't have information on specific sales events like Black Friday at the moment, I'd be happy to keep you updated if we have any exciting promotions coming up. In the meantime, feel free to browse our wide selection of sustainable technology products that can help you reduce your carbon footprint. If you have any other questions or need assistance with your current order, please don't hesitate to ask! Thank you for choosing GreenTech Marketplace.\n",
            "----------------------------------------------------------------------\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "ğŸ“• UNANSWERABLE: \"What is the CEO's email address?\"\n",
            "======================================================================\n",
            "   Step 1: embed_query() - Converting query to embedding...\n",
            "   Step 2: search_faiss() - Searching for similar Q&A pairs...\n",
            "           â†’ Found 3 relevant documents\n",
            "   Step 3: format_context() - Formatting retrieved context...\n",
            "   Step 4: build_prompt() - Building augmented prompt...\n",
            "   Step 5: generate_response() - Generating response with Mistral...\n",
            "           â†’ Response generated: 396 chars\n",
            "\n",
            "======================================================================\n",
            "ğŸ¤– RAG PIPELINE RESULT\n",
            "======================================================================\n",
            "\n",
            "ğŸ“¥ USER QUERY:\n",
            "   What is the CEO's email address?\n",
            "\n",
            "ğŸ“š RETRIEVED CONTEXT (3 sources):\n",
            "   ğŸ“— [1] customer_service (Similarity: 39%)\n",
            "       Q: What are your customer service hours?...\n",
            "   ğŸ“— [2] returns (Similarity: 38%)\n",
            "       Q: I've received an incorrect item in my order. How can I initi...\n",
            "   ğŸ“— [3] orders (Similarity: 37%)\n",
            "       Q: Can you help me check the status of my order? I placed it a ...\n",
            "\n",
            "ğŸ“¤ GENERATED RESPONSE:\n",
            "----------------------------------------------------------------------\n",
            "I'm sorry for any confusion, but I don't have the CEO's email address in my current knowledge base. However, you can reach our customer service team at support@greentechmarketplace.com for any questions or concerns regarding your orders, products, or general inquiries. If you need to speak with a manager, they will be more than happy to assist you. Thank you for choosing GreenTech Marketplace!\n",
            "----------------------------------------------------------------------\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "ğŸ“• UNANSWERABLE: \"Can you recommend a good restaurant nearby?\"\n",
            "======================================================================\n",
            "   Step 1: embed_query() - Converting query to embedding...\n",
            "   Step 2: search_faiss() - Searching for similar Q&A pairs...\n",
            "           â†’ Found 3 relevant documents\n",
            "   Step 3: format_context() - Formatting retrieved context...\n",
            "   Step 4: build_prompt() - Building augmented prompt...\n",
            "   Step 5: generate_response() - Generating response with Mistral...\n",
            "           â†’ Response generated: 367 chars\n",
            "\n",
            "======================================================================\n",
            "ğŸ¤– RAG PIPELINE RESULT\n",
            "======================================================================\n",
            "\n",
            "ğŸ“¥ USER QUERY:\n",
            "   Can you recommend a good restaurant nearby?\n",
            "\n",
            "ğŸ“š RETRIEVED CONTEXT (3 sources):\n",
            "   ğŸ“— [1] products (Similarity: 39%)\n",
            "       Q: I'm interested in smart home devices. What can I find on you...\n",
            "   ğŸ“• [2] personal_advice (Similarity: 37%)\n",
            "       Q: Can you suggest a gift for my best friend who's going throug...\n",
            "   ğŸ“— [3] returns (Similarity: 36%)\n",
            "       Q: I've received an incorrect item in my order. How can I initi...\n",
            "\n",
            "ğŸ“¤ GENERATED RESPONSE:\n",
            "----------------------------------------------------------------------\n",
            "I'm sorry for any inconvenience, but as a GreenTech Marketplace customer service representative, I'm not able to provide recommendations for local restaurants. I'd suggest checking online review platforms like Yelp or Google Maps to find highly-rated dining options near you. If you need further assistance with our products or services, please don't hesitate to ask!\n",
            "----------------------------------------------------------------------\n",
            "======================================================================\n",
            "\n",
            "ğŸ’¾ STEP 5: Save Results\n",
            "----------------------------------------------------------------------\n",
            "âœ… RAG results saved to: data/rag_pipeline/rag_test_results.csv\n",
            "âœ… Pipeline config saved to: data/rag_pipeline/pipeline_config.txt\n",
            "\n",
            "======================================================================\n",
            "âœ… OBJECTIVE 4 COMPLETE\n",
            "======================================================================\n",
            "\n",
            "RAG Pipeline Components:\n",
            "  1. Query Processing: embed_query() - Convert to embedding\n",
            "  2. Retrieval: search_faiss() - FAISS similarity search (top-3)\n",
            "  3. Augmentation: format_context() + build_prompt()\n",
            "  4. Generation: generate_response() - Mistral-7B response\n",
            "\n",
            "Test Results:\n",
            "  ğŸ“— Answerable Questions: 5/5 successful\n",
            "  ğŸ“• Unanswerable Questions: 5/5 successful\n",
            "\n",
            "ğŸ“¦ FILES SAVED:\n",
            "  â€¢ data/rag_pipeline/rag_test_results.csv\n",
            "  â€¢ data/rag_pipeline/pipeline_config.txt\n",
            "\n",
            "ğŸ“¦ GLOBAL FUNCTIONS:\n",
            "  â€¢ rag_query(query) - Complete RAG pipeline\n",
            "  â€¢ embed_query(), search_faiss(), format_context(), build_prompt(), generate_response()\n",
            "\n",
            "ğŸ”œ READY FOR OBJECTIVE 5: Model Experimentation and Ranking\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# OBJECTIVE 4: BUILD COMPLETE RAG PIPELINE\n",
        "# ============================================================================\n",
        "#\n",
        "# PIPELINE COMPONENTS:\n",
        "#   1. Query Processing - Convert user question to embedding\n",
        "#   2. Retrieval - Use FAISS to find top-k most similar Q&A pairs\n",
        "#   3. Augmentation - Combine user question with retrieved context\n",
        "#   4. Generation - Use Mistral to generate answer from augmented context\n",
        "#\n",
        "# WHY THIS ARCHITECTURE:\n",
        "#   - RAG combines retrieval (accurate, up-to-date info) with generation (natural responses)\n",
        "#   - Grounds answers in knowledge base, reducing hallucinations\n",
        "#   - Allows easy updates to knowledge base without retraining models\n",
        "#\n",
        "# CONTEXT WINDOW MANAGEMENT:\n",
        "#   - Limit retrieved context to top-k most relevant documents\n",
        "#   - Balance between enough context for accuracy and not exceeding model limits\n",
        "#\n",
        "# ERROR HANDLING:\n",
        "#   - Implement fallbacks for cases where no relevant context is found\n",
        "#   - Handle edge cases gracefully with appropriate error messages\n",
        "#\n",
        "# 100% REUSE FROM PREVIOUS OBJECTIVES:\n",
        "#   - system_prompt, mistral_tokenizer, mistral_model (Objective 1)\n",
        "#   - qa_database (Objective 2)\n",
        "#   - embedding_model, faiss_index (Objective 3)\n",
        "#\n",
        "# PREREQUISITES: Run Objectives 1, 2, and 3 first\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 1: IMPORTS & VALIDATION\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "from typing import List, Dict, Optional\n",
        "from dataclasses import dataclass\n",
        "\n",
        "try:\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import torch\n",
        "except ImportError as e:\n",
        "    raise ImportError(f\"Missing: {e}. Run: pip install numpy pandas torch\")\n",
        "\n",
        "\n",
        "def validate_prerequisites():\n",
        "    \"\"\"Ensure Objectives 1, 2, and 3 were run first.\"\"\"\n",
        "    required = {\n",
        "        'Objective 1': ['system_prompt', 'mistral_tokenizer', 'mistral_model'],\n",
        "        'Objective 2': ['qa_database'],\n",
        "        'Objective 3': ['embedding_model', 'faiss_index']\n",
        "    }\n",
        "    \n",
        "    all_missing = []\n",
        "    for objective, items in required.items():\n",
        "        missing = [item for item in items if item not in globals()]\n",
        "        if missing:\n",
        "            all_missing.append(f\"{objective}: {missing}\")\n",
        "    \n",
        "    if all_missing:\n",
        "        raise RuntimeError(f\"Missing prerequisites:\\n\" + \"\\n\".join(all_missing))\n",
        "    \n",
        "    print(\"âœ… All prerequisites validated\")\n",
        "    print(f\"   â€¢ System prompt: {len(globals()['system_prompt'])} chars\")\n",
        "    print(f\"   â€¢ Q&A database: {len(globals()['qa_database'])} pairs\")\n",
        "    print(f\"   â€¢ FAISS index: {globals()['faiss_index'].ntotal} vectors\")\n",
        "    return True\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 2: CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "# Output directory\n",
        "OUTPUT_DIR = \"data/rag_pipeline\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# ============================================================================\n",
        "# RAG CONFIGURATION PARAMETERS EXPLAINED\n",
        "# ============================================================================\n",
        "#\n",
        "# top_k: Number of documents to retrieve from FAISS\n",
        "#   - Higher value (5-10): More context, better coverage, but may include noise\n",
        "#   - Lower value (1-3): More focused, less noise, but may miss relevant info\n",
        "#   - Default 3: Good balance for small knowledge bases (21 Q&A pairs)\n",
        "#\n",
        "# max_new_tokens: Maximum tokens the model can generate in response\n",
        "#   - Higher value (500+): Longer, more detailed responses\n",
        "#   - Lower value (100-200): Concise responses, faster generation\n",
        "#   - Default 300: Allows comprehensive answers without being verbose\n",
        "#\n",
        "# temperature: Controls randomness/creativity in generation (0.0 - 1.0)\n",
        "#   - 0.0: Deterministic, always picks most likely token (factual tasks)\n",
        "#   - 0.5-0.7: Balanced creativity and coherence (recommended for QA)\n",
        "#   - 1.0+: More random/creative (creative writing, brainstorming)\n",
        "#   - Default 0.7: Allows natural variation while staying on-topic\n",
        "#\n",
        "# similarity_threshold: Minimum similarity score to include a document (0.0 - 1.0)\n",
        "#   - Higher value (0.5+): Only very relevant documents, may return few/none\n",
        "#   - Lower value (0.1-0.3): More documents included, may have lower relevance\n",
        "#   - Default 0.3: Filters out clearly irrelevant results while being inclusive\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "RAG_CONFIG = {\n",
        "    \"top_k\": 3,                    # Number of documents to retrieve\n",
        "    \"max_new_tokens\": 300,         # Max tokens for generation\n",
        "    \"temperature\": 0.7,            # Generation temperature\n",
        "    \"similarity_threshold\": 0.3,   # Minimum similarity score (0-1)\n",
        "}\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 3: RAG RESULT DATA CLASS\n",
        "# ============================================================================\n",
        "\n",
        "@dataclass\n",
        "class RAGResult:\n",
        "    \"\"\"Container for RAG pipeline results.\"\"\"\n",
        "    query: str\n",
        "    response: str\n",
        "    retrieved_context: List[Dict]\n",
        "    success: bool\n",
        "    error_message: Optional[str] = None\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 4: RAG PIPELINE CORE FUNCTIONS\n",
        "# ============================================================================\n",
        "#\n",
        "# These 5 functions form the complete RAG pipeline:\n",
        "#   1. embed_query()        - Convert query to embedding\n",
        "#   2. search_faiss()       - Search FAISS for similar Q&A pairs\n",
        "#   3. format_context()     - Format retrieved Q&A as context string\n",
        "#   4. build_prompt()       - Build augmented prompt\n",
        "#   5. generate_response()  - Generate response with Mistral\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "def embed_query(query: str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    STEP 1: Convert query to embedding.\n",
        "    \n",
        "    Reuses: embedding_model from Objective 3\n",
        "    \n",
        "    Args:\n",
        "        query: User's question\n",
        "    \n",
        "    Returns:\n",
        "        Query embedding as numpy array (1, 384)\n",
        "    \"\"\"\n",
        "    embedding_model = globals()['embedding_model']\n",
        "    query_embedding = embedding_model.encode([query], convert_to_numpy=True).astype('float32')\n",
        "    return query_embedding\n",
        "\n",
        "\n",
        "def search_faiss(query_embedding: np.ndarray, top_k: int = None) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    STEP 2: Search FAISS index for similar Q&A pairs.\n",
        "    \n",
        "    Reuses: faiss_index from Objective 3, qa_database from Objective 2\n",
        "    \n",
        "    Args:\n",
        "        query_embedding: Query vector from embed_query()\n",
        "        top_k: Number of results to retrieve\n",
        "    \n",
        "    Returns:\n",
        "        List of Q&A dicts with similarity scores\n",
        "    \"\"\"\n",
        "    if top_k is None:\n",
        "        top_k = RAG_CONFIG[\"top_k\"]\n",
        "    \n",
        "    faiss_index = globals()['faiss_index']\n",
        "    qa_database = globals()['qa_database']\n",
        "    \n",
        "    # Search FAISS index\n",
        "    distances, indices = faiss_index.search(query_embedding, top_k)\n",
        "    \n",
        "    # Get Q&A pairs with similarity scores\n",
        "    results = []\n",
        "    for dist, idx in zip(distances[0], indices[0]):\n",
        "        if idx < len(qa_database):\n",
        "            similarity = 1 / (1 + float(dist))  # Convert distance to similarity\n",
        "            if similarity >= RAG_CONFIG[\"similarity_threshold\"]:\n",
        "                qa = qa_database[idx].copy()\n",
        "                qa['similarity_score'] = similarity\n",
        "                qa['distance'] = float(dist)\n",
        "                results.append(qa)\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "def format_context(retrieved_qa: List[Dict]) -> str:\n",
        "    \"\"\"\n",
        "    STEP 3: Format retrieved Q&A pairs as context string.\n",
        "    \n",
        "    Args:\n",
        "        retrieved_qa: List of Q&A dicts from search_faiss()\n",
        "    \n",
        "    Returns:\n",
        "        Formatted context string for prompt\n",
        "    \"\"\"\n",
        "    if not retrieved_qa:\n",
        "        return \"No relevant information found in knowledge base.\"\n",
        "    \n",
        "    context_parts = [\"RELEVANT INFORMATION FROM KNOWLEDGE BASE:\", \"-\" * 40]\n",
        "    \n",
        "    for i, qa in enumerate(retrieved_qa, 1):\n",
        "        similarity_pct = qa.get('similarity_score', 0) * 100\n",
        "        context_parts.append(f\"\\n[Source {i}] (Relevance: {similarity_pct:.0f}%)\")\n",
        "        context_parts.append(f\"Q: {qa['question']}\")\n",
        "        context_parts.append(f\"A: {qa['answer']}\")\n",
        "    \n",
        "    context_parts.append(\"-\" * 40)\n",
        "    return \"\\n\".join(context_parts)\n",
        "\n",
        "\n",
        "def build_prompt(query: str, context: str) -> str:\n",
        "    \"\"\"\n",
        "    STEP 4: Build augmented prompt combining system prompt, context, and query.\n",
        "    \n",
        "    Reuses: system_prompt from Objective 1\n",
        "    \n",
        "    Args:\n",
        "        query: User's question\n",
        "        context: Formatted context from format_context()\n",
        "    \n",
        "    Returns:\n",
        "        Complete augmented prompt\n",
        "    \"\"\"\n",
        "    system_prompt = globals()['system_prompt']\n",
        "    \n",
        "    augmented_prompt = f\"\"\"{system_prompt}\n",
        "\n",
        "{context}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "- Answer using ONLY the information provided above\n",
        "- If information is not available, politely say so\n",
        "- Be helpful, accurate, and concise\n",
        "\n",
        "CUSTOMER QUESTION: {query}\n",
        "\n",
        "ASSISTANT RESPONSE:\"\"\"\n",
        "    \n",
        "    return augmented_prompt\n",
        "\n",
        "\n",
        "def generate_response(augmented_prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    STEP 5: Generate response with Mistral model.\n",
        "    \n",
        "    Reuses: mistral_tokenizer, mistral_model from Objective 1\n",
        "    \n",
        "    Args:\n",
        "        augmented_prompt: Complete prompt from build_prompt()\n",
        "    \n",
        "    Returns:\n",
        "        Generated response string\n",
        "    \"\"\"\n",
        "    tokenizer = globals()['mistral_tokenizer']\n",
        "    model = globals()['mistral_model']\n",
        "    \n",
        "    # Format for Mistral Instruct\n",
        "    formatted = f\"<s>[INST] {augmented_prompt} [/INST]\"\n",
        "    \n",
        "    # Tokenize\n",
        "    inputs = tokenizer(formatted, return_tensors=\"pt\", truncation=True, max_length=4096)\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "    \n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=RAG_CONFIG[\"max_new_tokens\"],\n",
        "            temperature=RAG_CONFIG[\"temperature\"],\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    # Decode only new tokens\n",
        "    input_length = inputs['input_ids'].shape[1]\n",
        "    response = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True).strip()\n",
        "    \n",
        "    return response\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 5: COMPLETE RAG PIPELINE FUNCTION\n",
        "# ============================================================================\n",
        "\n",
        "def rag_query(query: str, top_k: int = None, verbose: bool = True) -> RAGResult:\n",
        "    \"\"\"\n",
        "    Complete RAG Pipeline: Query â†’ Retrieve â†’ Augment â†’ Generate\n",
        "    \n",
        "    This is the main entry point for the RAG system.\n",
        "    Orchestrates all 5 core functions in sequence.\n",
        "    \n",
        "    Args:\n",
        "        query: User's question\n",
        "        top_k: Number of documents to retrieve (default: from config)\n",
        "        verbose: Print step-by-step progress\n",
        "    \n",
        "    Returns:\n",
        "        RAGResult with response and metadata\n",
        "    \n",
        "    Example:\n",
        "        result = rag_query(\"What is your return policy?\")\n",
        "        print(result.response)\n",
        "    \"\"\"\n",
        "    if top_k is None:\n",
        "        top_k = RAG_CONFIG[\"top_k\"]\n",
        "    \n",
        "    try:\n",
        "        # ============================================================\n",
        "        # STEP 1: QUERY PROCESSING - Convert to embedding\n",
        "        # ============================================================\n",
        "        if verbose:\n",
        "            print(f\"   Step 1: embed_query() - Converting query to embedding...\")\n",
        "        query_embedding = embed_query(query)\n",
        "        \n",
        "        # ============================================================\n",
        "        # STEP 2: RETRIEVAL - Search FAISS for similar Q&A\n",
        "        # ============================================================\n",
        "        if verbose:\n",
        "            print(f\"   Step 2: search_faiss() - Searching for similar Q&A pairs...\")\n",
        "        retrieved_qa = search_faiss(query_embedding, top_k)\n",
        "        if verbose:\n",
        "            print(f\"           â†’ Found {len(retrieved_qa)} relevant documents\")\n",
        "        \n",
        "        # ============================================================\n",
        "        # STEP 3: AUGMENTATION (Part 1) - Format context\n",
        "        # ============================================================\n",
        "        if verbose:\n",
        "            print(f\"   Step 3: format_context() - Formatting retrieved context...\")\n",
        "        context = format_context(retrieved_qa)\n",
        "        \n",
        "        # ============================================================\n",
        "        # STEP 4: AUGMENTATION (Part 2) - Build prompt\n",
        "        # ============================================================\n",
        "        if verbose:\n",
        "            print(f\"   Step 4: build_prompt() - Building augmented prompt...\")\n",
        "        augmented_prompt = build_prompt(query, context)\n",
        "        \n",
        "        # ============================================================\n",
        "        # STEP 5: GENERATION - Generate response with Mistral\n",
        "        # ============================================================\n",
        "        if verbose:\n",
        "            print(f\"   Step 5: generate_response() - Generating response with Mistral...\")\n",
        "        response = generate_response(augmented_prompt)\n",
        "        if verbose:\n",
        "            print(f\"           â†’ Response generated: {len(response)} chars\")\n",
        "        \n",
        "        return RAGResult(\n",
        "            query=query,\n",
        "            response=response,\n",
        "            retrieved_context=retrieved_qa,\n",
        "            success=True\n",
        "        )\n",
        "        \n",
        "    except Exception as e:\n",
        "        return RAGResult(\n",
        "            query=query,\n",
        "            response=\"I apologize, but I encountered an error processing your request.\",\n",
        "            retrieved_context=[],\n",
        "            success=False,\n",
        "            error_message=str(e)\n",
        "        )\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 6: DISPLAY & STORAGE FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def display_rag_result(result: RAGResult):\n",
        "    \"\"\"Display RAG result in formatted way.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ğŸ¤– RAG PIPELINE RESULT\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    print(f\"\\nğŸ“¥ USER QUERY:\")\n",
        "    print(f\"   {result.query}\")\n",
        "    \n",
        "    print(f\"\\nğŸ“š RETRIEVED CONTEXT ({len(result.retrieved_context)} sources):\")\n",
        "    for i, ctx in enumerate(result.retrieved_context, 1):\n",
        "        similarity = ctx.get('similarity_score', 0) * 100\n",
        "        answerable = 'ğŸ“—' if ctx.get('answerable', True) else 'ğŸ“•'\n",
        "        print(f\"   {answerable} [{i}] {ctx['category']} (Similarity: {similarity:.0f}%)\")\n",
        "        print(f\"       Q: {ctx['question'][:60]}...\")\n",
        "    \n",
        "    print(f\"\\nğŸ“¤ GENERATED RESPONSE:\")\n",
        "    print(\"-\"*70)\n",
        "    print(result.response)\n",
        "    print(\"-\"*70)\n",
        "    \n",
        "    if not result.success:\n",
        "        print(f\"\\nâš ï¸  Error: {result.error_message}\")\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "\n",
        "\n",
        "def save_rag_results(results: List[RAGResult], filename: str = \"rag_test_results.csv\"):\n",
        "    \"\"\"Save RAG test results to CSV.\"\"\"\n",
        "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
        "    \n",
        "    rows = []\n",
        "    for result in results:\n",
        "        rows.append({\n",
        "            'query': result.query,\n",
        "            'response': result.response[:500],\n",
        "            'num_sources': len(result.retrieved_context),\n",
        "            'top_source_similarity': result.retrieved_context[0]['similarity_score'] if result.retrieved_context else 0,\n",
        "            'success': result.success,\n",
        "            'error': result.error_message\n",
        "        })\n",
        "    \n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv(filepath, index=False)\n",
        "    print(f\"âœ… RAG results saved to: {filepath}\")\n",
        "    return filepath\n",
        "\n",
        "\n",
        "def save_pipeline_config(filename: str = \"pipeline_config.txt\"):\n",
        "    \"\"\"Save pipeline configuration.\"\"\"\n",
        "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
        "    \n",
        "    config_text = f\"\"\"RAG PIPELINE CONFIGURATION\n",
        "==========================\n",
        "\n",
        "Retrieval Settings:\n",
        "- Top-K: {RAG_CONFIG['top_k']}\n",
        "- Similarity Threshold: {RAG_CONFIG['similarity_threshold']}\n",
        "\n",
        "Generation Settings:\n",
        "- Max New Tokens: {RAG_CONFIG['max_new_tokens']}\n",
        "- Temperature: {RAG_CONFIG['temperature']}\n",
        "\n",
        "Components:\n",
        "- Embedding Model: sentence-transformers/all-MiniLM-L6-v2\n",
        "- Vector Store: FAISS IndexFlatL2\n",
        "- LLM: Mistral-7B-Instruct-v0.3\n",
        "- Q&A Database: {len(globals().get('qa_database', []))} pairs\n",
        "\"\"\"\n",
        "    \n",
        "    with open(filepath, 'w') as f:\n",
        "        f.write(config_text)\n",
        "    \n",
        "    print(f\"âœ… Pipeline config saved to: {filepath}\")\n",
        "    return filepath\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 7: EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"   OBJECTIVE 4: BUILD COMPLETE RAG PIPELINE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# --- Step 1: Validate Prerequisites ---\n",
        "print(\"\\nğŸ” STEP 1: Validate Prerequisites\")\n",
        "print(\"-\"*70)\n",
        "validate_prerequisites()\n",
        "\n",
        "# --- Step 2: Show Pipeline Architecture ---\n",
        "print(\"\\nğŸ“ STEP 2: RAG Pipeline Architecture\")\n",
        "print(\"-\"*70)\n",
        "print(\"\"\"\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                      RAG PIPELINE FLOW                          â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                 â”‚\n",
        "â”‚  User Query                                                     â”‚\n",
        "â”‚      â”‚                                                          â”‚\n",
        "â”‚      â–¼                                                          â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚\n",
        "â”‚  â”‚ 1. embed_query()â”‚  Convert query to embedding                â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                            â”‚\n",
        "â”‚           â–¼                                                     â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚\n",
        "â”‚  â”‚ 2. search_faiss()â”‚  Find similar Q&A pairs                   â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                            â”‚\n",
        "â”‚           â–¼                                                     â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚\n",
        "â”‚  â”‚ 3. format_context()â”‚  Format as context string               â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚\n",
        "â”‚           â–¼                                                     â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚\n",
        "â”‚  â”‚ 4. build_prompt()â”‚  Combine system + context + query         â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                            â”‚\n",
        "â”‚           â–¼                                                     â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚\n",
        "â”‚  â”‚ 5. generate_response()â”‚  Generate with Mistral               â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚\n",
        "â”‚           â–¼                                                     â”‚\n",
        "â”‚      Response                                                   â”‚\n",
        "â”‚                                                                 â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\"\"\")\n",
        "\n",
        "# --- Step 3: Test RAG Pipeline with Answerable Questions ---\n",
        "# ============================================================================\n",
        "# ANSWERABLE QUESTIONS: These can be answered from our Q&A database\n",
        "# ============================================================================\n",
        "print(\"\\nğŸ§ª STEP 3: Test with ANSWERABLE Questions\")\n",
        "print(\"-\"*70)\n",
        "print(\"   These questions CAN be answered from our knowledge base\")\n",
        "\n",
        "answerable_questions = [\n",
        "    \"What is your return policy?\",\n",
        "    \"How long does shipping take?\",\n",
        "    \"What are your customer service hours?\",\n",
        "    \"Do you offer warranty on products?\",\n",
        "    \"How can I track my order?\",\n",
        "]\n",
        "\n",
        "answerable_results = []\n",
        "\n",
        "for query in answerable_questions:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"ğŸ“— ANSWERABLE: \\\"{query}\\\"\")\n",
        "    print('='*70)\n",
        "    \n",
        "    result = rag_query(query, verbose=True)\n",
        "    answerable_results.append(result)\n",
        "    display_rag_result(result)\n",
        "\n",
        "# --- Step 4: Test RAG Pipeline with Unanswerable Questions ---\n",
        "# ============================================================================\n",
        "# UNANSWERABLE QUESTIONS: These CANNOT be answered from our Q&A database\n",
        "# Testing system limitations and graceful handling\n",
        "# ============================================================================\n",
        "print(\"\\nğŸ§ª STEP 4: Test with UNANSWERABLE Questions\")\n",
        "print(\"-\"*70)\n",
        "print(\"   These questions CANNOT be answered from our knowledge base\")\n",
        "print(\"   Testing system limitations and graceful handling\")\n",
        "\n",
        "unanswerable_questions = [\n",
        "    \"How do your prices compare to Amazon?\",\n",
        "    \"Should I buy solar panels for my house?\",\n",
        "    \"Will you have a Black Friday sale this year?\",\n",
        "    \"What is the CEO's email address?\",\n",
        "    \"Can you recommend a good restaurant nearby?\",\n",
        "]\n",
        "\n",
        "unanswerable_results = []\n",
        "\n",
        "for query in unanswerable_questions:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"ğŸ“• UNANSWERABLE: \\\"{query}\\\"\")\n",
        "    print('='*70)\n",
        "    \n",
        "    result = rag_query(query, verbose=True)\n",
        "    unanswerable_results.append(result)\n",
        "    display_rag_result(result)\n",
        "\n",
        "# --- Step 5: Save Results ---\n",
        "print(\"\\nğŸ’¾ STEP 5: Save Results\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "all_results = answerable_results + unanswerable_results\n",
        "save_rag_results(all_results)\n",
        "save_pipeline_config()\n",
        "\n",
        "# Store globally for reuse\n",
        "globals()['rag_query'] = rag_query\n",
        "globals()['rag_results'] = all_results\n",
        "\n",
        "# Also expose core functions globally\n",
        "globals()['embed_query'] = embed_query\n",
        "globals()['search_faiss'] = search_faiss\n",
        "globals()['format_context'] = format_context\n",
        "globals()['build_prompt'] = build_prompt\n",
        "globals()['generate_response'] = generate_response\n",
        "\n",
        "# --- Summary ---\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âœ… OBJECTIVE 4 COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "answerable_success = sum(1 for r in answerable_results if r.success)\n",
        "unanswerable_success = sum(1 for r in unanswerable_results if r.success)\n",
        "\n",
        "print(f\"\"\"\n",
        "RAG Pipeline Components:\n",
        "  1. Query Processing: embed_query() - Convert to embedding\n",
        "  2. Retrieval: search_faiss() - FAISS similarity search (top-{RAG_CONFIG['top_k']})\n",
        "  3. Augmentation: format_context() + build_prompt()\n",
        "  4. Generation: generate_response() - Mistral-7B response\n",
        "\n",
        "Test Results:\n",
        "  ğŸ“— Answerable Questions: {answerable_success}/{len(answerable_results)} successful\n",
        "  ğŸ“• Unanswerable Questions: {unanswerable_success}/{len(unanswerable_results)} successful\n",
        "\n",
        "ğŸ“¦ FILES SAVED:\n",
        "  â€¢ {OUTPUT_DIR}/rag_test_results.csv\n",
        "  â€¢ {OUTPUT_DIR}/pipeline_config.txt\n",
        "\n",
        "ğŸ“¦ GLOBAL FUNCTIONS:\n",
        "  â€¢ rag_query(query) - Complete RAG pipeline\n",
        "  â€¢ embed_query(), search_faiss(), format_context(), build_prompt(), generate_response()\n",
        "\n",
        "ğŸ”œ READY FOR OBJECTIVE 5: Model Experimentation and Ranking\n",
        "\"\"\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Objective 5: Model Experimentation and Ranking\n",
        "\n",
        "## Design Choices & Rationale\n",
        "\n",
        "### Dual Evaluation Approach\n",
        "\n",
        "| Evaluator | Type | Description |\n",
        "|-----------|------|-------------|\n",
        "| **Library-Based** | HuggingFace Evaluate | BERTScore + SQuAD F1 + Model Confidence |\n",
        "| **LLM-as-Judge** | Mistral 7B | Accuracy + Quality + Confidence scoring |\n",
        "\n",
        "### Why Two Evaluators?\n",
        "\n",
        "1. **Library-Based (BERTScore + SQuAD F1)**: Standard, reproducible metrics used in research\n",
        "2. **LLM-as-Judge**: Captures semantic understanding and nuanced quality assessment\n",
        "\n",
        "---\n",
        "\n",
        "## Library-Based Evaluation Metrics\n",
        "\n",
        "| Metric | Weight | What It Measures | Source |\n",
        "|--------|--------|------------------|--------|\n",
        "| **BERTScore F1** | 35% | Semantic similarity using BERT embeddings | `evaluate` library |\n",
        "| **SQuAD F1** | 25% | Token overlap (standard QA metric) | Custom implementation |\n",
        "| **Model Confidence** | 25% | Model's own confidence score handling | Model output |\n",
        "| **Speed** | 15% | Response time performance | Measured |\n",
        "\n",
        "### BERTScore\n",
        "- Uses pre-trained BERT embeddings to compute semantic similarity\n",
        "- Handles paraphrasing: \"30-day return\" â‰ˆ \"We offer 30 day returns\"\n",
        "- Returns precision, recall, F1 scores\n",
        "\n",
        "### SQuAD F1\n",
        "- Standard QA evaluation metric used in SQuAD benchmark\n",
        "- Token overlap between prediction and ground truth\n",
        "- Formula: F1 = 2 Ã— (precision Ã— recall) / (precision + recall)\n",
        "\n",
        "### Model Confidence\n",
        "- Evaluates how well the model calibrates confidence scores\n",
        "- Good models: High confidence on answerable, low on unanswerable\n",
        "- Score = normalized confidence gap\n",
        "\n",
        "---\n",
        "\n",
        "## LLM-as-Judge Metrics\n",
        "\n",
        "| Metric | Weight | What It Measures |\n",
        "|--------|--------|------------------|\n",
        "| **Accuracy** | 40% | Correctness of information |\n",
        "| **Quality** | 30% | Completeness and usefulness |\n",
        "| **Confidence** | 20% | Same as library evaluator |\n",
        "| **Speed** | 10% | Same as library evaluator |\n",
        "\n",
        "### Scoring Rubric (0-10)\n",
        "\n",
        "**Accuracy:**\n",
        "- 9-10: Complete and accurate\n",
        "- 6-8: Mostly correct, minor gaps\n",
        "- 3-5: Partially correct\n",
        "- 0-2: Wrong or fragment only\n",
        "\n",
        "**Quality:**\n",
        "- 9-10: Full, helpful answer\n",
        "- 6-8: Good but could be more complete\n",
        "- 3-5: Useful but missing key parts\n",
        "- 0-2: Useless fragment or single word\n",
        "\n",
        "---\n",
        "\n",
        "## Models Evaluated\n",
        "\n",
        "| # | Model | Size | Type |\n",
        "|---|-------|------|------|\n",
        "| 1 | T5-QA-Generative | base | text2text-generation |\n",
        "| 2 | RoBERTa-SQuAD2 | base | question-answering |\n",
        "| 3 | BERT-Large-SQuAD | large | question-answering |\n",
        "| 4 | DynamicRAG-8B | 8B | question-answering |\n",
        "| 5 | DistilBERT-SQuAD | small | question-answering |\n",
        "| 6 | BERT-Tiny-SQuAD | tiny | question-answering |\n",
        "\n",
        "---\n",
        "\n",
        "## Test Questions\n",
        "\n",
        "### Answerable (5)\n",
        "1. What is your return policy?\n",
        "2. How long does shipping take?\n",
        "3. What are your customer service hours?\n",
        "4. Do you offer warranty on products?\n",
        "5. How can I track my order?\n",
        "\n",
        "### Unanswerable (5)\n",
        "1. How do your prices compare to Amazon?\n",
        "2. Should I buy solar panels for my house?\n",
        "3. Will you have a Black Friday sale this year?\n",
        "4. What is the CEO's email address?\n",
        "5. Can you recommend a good restaurant nearby?\n",
        "\n",
        "---\n",
        "\n",
        "## Output Files\n",
        "\n",
        "| File | Description |\n",
        "|------|-------------|\n",
        "| `model_responses.csv` | All 60 responses (6 models Ã— 10 questions) |\n",
        "| `model_scores.csv` | Both evaluator scores per model |\n",
        "| `model_rankings.csv` | Final rankings with all metrics |\n",
        "\n",
        "---\n",
        "\n",
        "## Comparison: Old vs New Approach\n",
        "\n",
        "| Aspect | Old (Custom) | New (Library-Based) |\n",
        "|--------|--------------|---------------------|\n",
        "| Accuracy | Keyword matching | BERTScore (semantic) |\n",
        "| Token matching | Basic substring | SQuAD F1 (standard) |\n",
        "| Paraphrasing | âŒ Poor | âœ… Excellent |\n",
        "| Reproducibility | Custom logic | Standard library |\n",
        "| Research alignment | No | Yes (SQuAD benchmark) |\n",
        "\n",
        "---\n",
        "\n",
        "## Expected Improvements\n",
        "\n",
        "### Problem Fixed\n",
        "| Answer | Old Score | New Score | Why |\n",
        "|--------|-----------|-----------|-----|\n",
        "| \"30-day return policy for unopened items\" | 28.7% | ~85% | BERTScore captures semantic match |\n",
        "| \"30\" | 13.7% | ~30% | Low SQuAD F1, penalized for incompleteness |\n",
        "| \"after\" | High | ~10% | Low BERTScore, low SQuAD F1 |\n",
        "\n",
        "### Key Benefits\n",
        "1. **BERTScore**: Handles synonyms and paraphrasing\n",
        "2. **SQuAD F1**: Standard QA metric, comparable to benchmarks\n",
        "3. **Model Confidence**: Your key requirement - uses model's own confidence\n",
        "4. **Reproducible**: Same metrics used in academic research\n",
        "\n",
        "---\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Requires outputs from:\n",
        "- **Objective 1**: `mistral_model`, `mistral_tokenizer`\n",
        "- **Objective 2**: `qa_database`\n",
        "- **Objective 3**: `faiss_index`\n",
        "- **Objective 4**: `embed_query`, `search_faiss`, `format_context`\n",
        "\n",
        "---\n",
        "\n",
        "## Installation\n",
        "\n",
        "```python\n",
        "pip install evaluate bert_score transformers torch pandas numpy\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Rubric Alignment\n",
        "\n",
        "| Criterion | Points | How Addressed |\n",
        "|-----------|--------|---------------|\n",
        "| Model Evaluation | âœ… | 6 models ranked with dual evaluators |\n",
        "| Performance Insights | âœ… | Confidence scores + BERTScore + SQuAD F1 |\n",
        "| Reflection | â¬œ | Template provided for student to complete |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (3316478822.py, line 378)",
          "output_type": "error",
          "traceback": [
            "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 378\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mevaluation.avg_response_time =\u001b[39m\n                                  ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# OBJECTIVE 5: MODEL EXPERIMENTATION AND RANKING (RAGAS Version)\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import string\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "from dataclasses import dataclass, field\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "# RAGAS + dataset\n",
        "from ragas import evaluate as ragas_evaluate\n",
        "from ragas.metrics import (\n",
        "    answer_relevance,\n",
        "    faithfulness,\n",
        "    context_precision,\n",
        "    context_recall,\n",
        "    answer_correctness\n",
        ")\n",
        "from datasets import Dataset\n",
        "from ragas.llms import HuggingFaceLLM\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 1: VALIDATE OBJECTIVE 1â€“4\n",
        "# ============================================================================\n",
        "\n",
        "def validate_prerequisites():\n",
        "    required = {\n",
        "        'Objective 1': ['mistral_model', 'mistral_tokenizer'],\n",
        "        'Objective 2': ['qa_database'],\n",
        "        'Objective 3': ['faiss_index'],\n",
        "        'Objective 4': ['embed_query', 'search_faiss', 'format_context']\n",
        "    }\n",
        "\n",
        "    missing = []\n",
        "    for obj, keys in required.items():\n",
        "        for k in keys:\n",
        "            if k not in globals():\n",
        "                missing.append(f\"{obj}: missing {k}\")\n",
        "\n",
        "    if missing:\n",
        "        raise RuntimeError(\"\\n\".join(missing))\n",
        "\n",
        "    print(\"âœ… All prerequisites validated.\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 2: CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "OUTPUT_DIR = \"data/model_evaluation\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "MODELS_TO_EVALUATE = [\n",
        "    {\n",
        "        \"name\": \"T5-QA-Generative\",\n",
        "        \"model_id\": \"consciousAI/question-answering-generative-t5-v1-base-s-q-c\",\n",
        "        \"type\": \"text2text-generation\",\n",
        "        \"size\": \"base\",\n",
        "        \"description\": \"Generative T5-based QA model\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"RoBERTa-SQuAD2\",\n",
        "        \"model_id\": \"deepset/roberta-base-squad2\",\n",
        "        \"type\": \"question-answering\",\n",
        "        \"size\": \"base\",\n",
        "        \"description\": \"RoBERTa fine-tuned on SQuAD 2.0\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"BERT-Large-SQuAD\",\n",
        "        \"model_id\": \"google-bert/bert-large-cased-whole-word-masking-finetuned-squad\",\n",
        "        \"type\": \"question-answering\",\n",
        "        \"size\": \"large\",\n",
        "        \"description\": \"Large BERT model for QA\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"DynamicRAG-8B\",\n",
        "        \"model_id\": \"gasolsun/DynamicRAG-8B\",\n",
        "        \"type\": \"question-answering\",\n",
        "        \"size\": \"8B\",\n",
        "        \"description\": \"Dynamic RAG-specific model\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"DistilBERT-SQuAD\",\n",
        "        \"model_id\": \"distilbert-base-uncased-distilled-squad\",\n",
        "        \"type\": \"question-answering\",\n",
        "        \"size\": \"small\",\n",
        "        \"description\": \"Distilled BERT - lightweight\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"BERT-Tiny-SQuAD\",\n",
        "        \"model_id\": \"mrm8488/bert-tiny-finetuned-squadv2\",\n",
        "        \"type\": \"question-answering\",\n",
        "        \"size\": \"tiny\",\n",
        "        \"description\": \"Tiny BERT for speed comparison\"\n",
        "    },\n",
        "]\n",
        "\n",
        "ANSWERABLE_QUESTIONS = [\n",
        "    \"What is your return policy?\",\n",
        "    \"How long does shipping take?\",\n",
        "    \"What are your customer service hours?\",\n",
        "    \"Do you offer warranty on products?\",\n",
        "    \"How can I track my order?\",\n",
        "]\n",
        "\n",
        "UNANSWERABLE_QUESTIONS = [\n",
        "    \"How do your prices compare to Amazon?\",\n",
        "    \"Should I buy solar panels?\",\n",
        "    \"Will you have a Black Friday sale?\",\n",
        "    \"What is the CEO's email?\",\n",
        "    \"Can you recommend a good restaurant?\"\n",
        "]\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 3: DATA CLASSES\n",
        "# ============================================================================\n",
        "\n",
        "@dataclass\n",
        "class ModelResponse:\n",
        "    model_name: str\n",
        "    question: str\n",
        "    answer: str\n",
        "    ground_truth: str\n",
        "    confidence: float\n",
        "    response_time: float\n",
        "    is_answerable: bool\n",
        "    success: bool\n",
        "    context: str = \"\"          # NEW (required for RAGAS)\n",
        "    error_message: Optional[str] = None\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class EvaluationScores:\n",
        "    # RAGAS scores\n",
        "    ragas_answer_rel: float = 0.0\n",
        "    ragas_faithfulness: float = 0.0\n",
        "    ragas_ctx_precision: float = 0.0\n",
        "    ragas_ctx_recall: float = 0.0\n",
        "    ragas_answer_correct: float = 0.0\n",
        "    ragas_weighted: float = 0.0\n",
        "\n",
        "    # LLM Judge\n",
        "    llm_accuracy: float = 0.0\n",
        "    llm_quality: float = 0.0\n",
        "    llm_speed: float = 0.0\n",
        "    llm_weighted: float = 0.0\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelEvaluation:\n",
        "    model_name: str\n",
        "    model_id: str\n",
        "    model_size: str\n",
        "    description: str\n",
        "    responses: List[ModelResponse] = field(default_factory=list)\n",
        "    scores: EvaluationScores = field(default_factory=EvaluationScores)\n",
        "\n",
        "    avg_response_time: float = 0.0\n",
        "    successful: int = 0\n",
        "    failed: int = 0\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 4: MODEL LOADING + INFERENCE\n",
        "# ============================================================================\n",
        "\n",
        "def load_model(config):\n",
        "    print(f\"   Loading {config['name']}...\")\n",
        "    try:\n",
        "        qa_pipeline = pipeline(\n",
        "            config[\"type\"],\n",
        "            model=config[\"model_id\"],\n",
        "            device=0 if torch.cuda.is_available() else -1,\n",
        "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "        )\n",
        "        print(\"   âœ… Loaded\")\n",
        "        return qa_pipeline\n",
        "    except Exception as e:\n",
        "        print(\"   âŒ Error loading:\", e)\n",
        "        return None\n",
        "\n",
        "\n",
        "def run_inference(model, config, question, context):\n",
        "    start = time.time()\n",
        "\n",
        "    try:\n",
        "        if config[\"type\"] == \"text2text-generation\":\n",
        "            result = model(f\"question: {question} context: {context}\", max_length=200)\n",
        "            answer = result[0][\"generated_text\"]\n",
        "            confidence = 0.7\n",
        "        else:\n",
        "            result = model(question=question, context=context)\n",
        "            answer = result[\"answer\"]\n",
        "            confidence = result.get(\"score\", 0.5)\n",
        "\n",
        "        elapsed = time.time() - start\n",
        "        return answer, confidence, elapsed\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\", 0.0, time.time() - start\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 5: RAGAS EVALUATOR (NEW)\n",
        "# ============================================================================\n",
        "\n",
        "ragas_llm = HuggingFaceLLM(\n",
        "    model=mistral_model,\n",
        "    tokenizer=mistral_tokenizer,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "\n",
        "def build_ragas_dataset(evaluation: ModelEvaluation):\n",
        "    data = {\n",
        "        \"question\": [],\n",
        "        \"answer\": [],\n",
        "        \"contexts\": [],\n",
        "        \"ground_truth\": [],\n",
        "    }\n",
        "\n",
        "    for r in evaluation.responses:\n",
        "        if not r.success:\n",
        "            continue\n",
        "\n",
        "        data[\"question\"].append(r.question)\n",
        "        data[\"answer\"].append(r.answer)\n",
        "        data[\"contexts\"].append([r.context])\n",
        "        data[\"ground_truth\"].append(r.ground_truth)\n",
        "\n",
        "    return Dataset.from_dict(data)\n",
        "\n",
        "\n",
        "def ragas_evaluate_model(evaluation: ModelEvaluation):\n",
        "    dataset = build_ragas_dataset(evaluation)\n",
        "\n",
        "    result = ragas_evaluate(\n",
        "        dataset,\n",
        "        metrics=[\n",
        "            answer_relevance,\n",
        "            faithfulness,\n",
        "            context_precision,\n",
        "            context_recall,\n",
        "            answer_correctness\n",
        "        ],\n",
        "        llm=ragas_llm\n",
        "    )\n",
        "\n",
        "    s = evaluation.scores\n",
        "    s.ragas_answer_rel = float(result[\"answer_relevance\"])\n",
        "    s.ragas_faithfulness = float(result[\"faithfulness\"])\n",
        "    s.ragas_ctx_precision = float(result[\"context_precision\"])\n",
        "    s.ragas_ctx_recall = float(result[\"context_recall\"])\n",
        "    s.ragas_answer_correct = float(result[\"answer_correctness\"])\n",
        "\n",
        "    s.ragas_weighted = (\n",
        "        s.ragas_faithfulness * 0.4 +\n",
        "        s.ragas_answer_rel * 0.3 +\n",
        "        s.ragas_answer_correct * 0.2 +\n",
        "        s.ragas_ctx_recall * 0.1\n",
        "    )\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 6: LLM-AS-JUDGE (UNCHANGED)\n",
        "# ============================================================================\n",
        "\n",
        "def llm_judge_response(question, answer, ground_truth, is_answerable):\n",
        "\n",
        "    mistral_model = globals()['mistral_model']\n",
        "    mistral_tokenizer = globals()['mistral_tokenizer']\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Evaluate the answer.\n",
        "\n",
        "Question: {question}\n",
        "Expected: {ground_truth}\n",
        "Answer: {answer}\n",
        "\n",
        "Return 2 scores (accuracy, quality) from 0â€“10.\n",
        "Only output: N,N\n",
        "\"\"\"\n",
        "\n",
        "    inputs = mistral_tokenizer(prompt, return_tensors=\"pt\").to(mistral_model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = mistral_model.generate(\n",
        "            **inputs, max_new_tokens=30, do_sample=False\n",
        "        )\n",
        "\n",
        "    text = mistral_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    nums = re.findall(r'\\d+', text)\n",
        "\n",
        "    if len(nums) >= 2:\n",
        "        acc = float(nums[-2]) / 10\n",
        "        qual = float(nums[-1]) / 10\n",
        "    else:\n",
        "        acc, qual = 0.5, 0.5\n",
        "\n",
        "    return acc, qual\n",
        "\n",
        "\n",
        "def llm_evaluate_all(responses):\n",
        "    accs = []\n",
        "    quals = []\n",
        "\n",
        "    for r in responses:\n",
        "        if not r.success:\n",
        "            continue\n",
        "\n",
        "        a, q = llm_judge_response(\n",
        "            r.question, r.answer, r.ground_truth, r.is_answerable\n",
        "        )\n",
        "        accs.append(a)\n",
        "        quals.append(q)\n",
        "\n",
        "    if not accs:\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    return sum(accs)/len(accs), sum(quals)/len(quals)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 7: MAIN MODEL EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "def evaluate_model(config, questions, qa_database):\n",
        "    evaluation = ModelEvaluation(\n",
        "        model_name=config[\"name\"],\n",
        "        model_id=config[\"model_id\"],\n",
        "        model_size=config[\"size\"],\n",
        "        description=config[\"description\"]\n",
        "    )\n",
        "\n",
        "    model = load_model(config)\n",
        "    if model is None:\n",
        "        return evaluation\n",
        "\n",
        "    embed_query = globals()['embed_query']\n",
        "    search_faiss = globals()['search_faiss']\n",
        "    format_context = globals()['format_context']\n",
        "\n",
        "    for q in questions:\n",
        "        gt = next((x[\"answer\"] for x in qa_database if x[\"question\"].lower() == q.lower()), \"\")\n",
        "        is_answerable = q in ANSWERABLE_QUESTIONS\n",
        "\n",
        "        emb = embed_query(q)\n",
        "        retrieved = search_faiss(emb, 3)\n",
        "        context = format_context(retrieved)\n",
        "\n",
        "        answer, conf, t = run_inference(model, config, q, context)\n",
        "\n",
        "        evaluation.responses.append(ModelResponse(\n",
        "            model_name=config[\"name\"],\n",
        "            question=q,\n",
        "            answer=answer,\n",
        "            ground_truth=gt,\n",
        "            confidence=conf,\n",
        "            response_time=t,\n",
        "            is_answerable=is_answerable,\n",
        "            context=context,\n",
        "            success=not answer.startswith(\"Error\")\n",
        "        ))\n",
        "\n",
        "    evaluation.successful = sum(1 for r in evaluation.responses if r.success)\n",
        "    evaluation.failed = len(evaluation.responses) - evaluation.successful\n",
        "\n",
        "    evaluation.avg_response_time =\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Objective 6: Analyze System Limitations\n",
        "\n",
        "### Design Choices & Rationale\n",
        "\n",
        "### Analysis Framework\n",
        "\n",
        "| Category | What We Analyze |\n",
        "|----------|-----------------|\n",
        "| **System Limitations** | What can/cannot be answered, failure modes |\n",
        "| **Real-World Applications** | Deployment scenarios, business value |\n",
        "| **Scalability** | Performance at scale, bottlenecks |\n",
        "| **Accuracy & Reliability** | Error rates, confidence calibration |\n",
        "| **User Experience** | Interaction design, response quality |\n",
        "| **Deployment** | Infrastructure, costs, maintenance |\n",
        "| **Improvements** | Future enhancements, optimizations |\n",
        "\n",
        "### Metrics to Compute\n",
        "\n",
        "| Metric | Description | Source |\n",
        "|--------|-------------|--------|\n",
        "| Answerable Accuracy | % correct on answerable questions | Objective 5 results |\n",
        "| Unanswerable Detection | % correctly identified as unanswerable | Objective 5 results |\n",
        "| Avg Response Time | Mean time per query | Objective 5 results |\n",
        "| Knowledge Coverage | % of topics covered in KB | Q&A database analysis |\n",
        "| Retrieval Precision | % relevant docs in top-k | FAISS search analysis |\n",
        "| Confidence Calibration | Correlation: confidence vs correctness | Model evaluation |\n",
        "\n",
        "### Reuse from Previous Objectives\n",
        "\n",
        "| Component | Source | Used For |\n",
        "|-----------|--------|----------|\n",
        "| `model_evaluations` | Objective 5 | Performance metrics |\n",
        "| `qa_database` | Objective 2 | Knowledge coverage |\n",
        "| `rag_results` | Objective 4 | RAG pipeline analysis |\n",
        "| `faiss_index` | Objective 3 | Retrieval analysis |\n",
        "\n",
        "### Output Files\n",
        "\n",
        "| File | Description |\n",
        "|------|-------------|\n",
        "| `system_analysis.txt` | Complete analysis report |\n",
        "| `limitations_summary.csv` | Quantitative metrics |\n",
        "| `recommendations.txt` | Improvement recommendations |\n",
        "\n",
        "### Analysis Sections\n",
        "\n",
        "**1. System Limitations**\n",
        "- Questions answerable vs not answerable\n",
        "- Failure mode classification\n",
        "- Edge cases identified\n",
        "\n",
        "**2. Real-World Applications**\n",
        "- Customer service chatbot\n",
        "- Internal FAQ system\n",
        "- Knowledge base assistant\n",
        "- Help desk automation\n",
        "\n",
        "**3. Scalability Analysis**\n",
        "- Current: 21 Q&A pairs\n",
        "- Projected: 1K, 10K, 100K entries\n",
        "- Bottlenecks: embedding time, search latency\n",
        "\n",
        "**4. Accuracy & Reliability**\n",
        "- Best model performance\n",
        "- Error rate breakdown\n",
        "- Confidence score analysis\n",
        "\n",
        "**5. User Experience**\n",
        "- Response time requirements\n",
        "- Answer quality expectations\n",
        "- Interaction patterns\n",
        "\n",
        "**6. Deployment Considerations**\n",
        "- Infrastructure: GPU vs CPU\n",
        "- Costs: Model hosting, API calls\n",
        "- Maintenance: KB updates, model updates\n",
        "\n",
        "**7. Improvement Opportunities**\n",
        "- More Q&A pairs\n",
        "- Domain-specific fine-tuning\n",
        "- Better retrieval (hybrid search)\n",
        "- Response generation improvements\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "         OBJECTIVE 6: ANALYZE SYSTEM LIMITATIONS\n",
            "======================================================================\n",
            "\n",
            "ğŸ” STEP 1: Validate Prerequisites\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "âœ… All prerequisites validated\n",
            "\n",
            "ğŸ“Š STEP 2: Analyze System Components\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "   Analyzing knowledge base...\n",
            "   âœ“ 20 Q&A pairs, 7 categories\n",
            "   Analyzing model performance...\n",
            "   âœ“ Best model: RoBERTa-SQuAD2 (70.5%)\n",
            "   Analyzing retrieval system...\n",
            "   âœ“ FAISS index: 20 vectors\n",
            "   Identifying failure modes...\n",
            "   âœ“ 8 failure cases identified\n",
            "\n",
            "ğŸ“ STEP 3: Generate Analysis Reports\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'\"Good\" if perf_analysis'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 774\u001b[39m\n\u001b[32m    772\u001b[39m applications = generate_applications_report()\n\u001b[32m    773\u001b[39m scalability = generate_scalability_report(kb_analysis, retrieval_analysis)\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m accuracy = \u001b[43mgenerate_accuracy_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43mperf_analysis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    775\u001b[39m ux = generate_ux_report(perf_analysis)\n\u001b[32m    776\u001b[39m deployment = generate_deployment_report()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 449\u001b[39m, in \u001b[36mgenerate_accuracy_report\u001b[39m\u001b[34m(perf_analysis)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Generate the accuracy and reliability section.\"\"\"\u001b[39;00m\n\u001b[32m    400\u001b[39m     rankings_str = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join([\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m     \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \n\u001b[32m    401\u001b[39m                               \u001b[38;5;28;01mfor\u001b[39;00m i, (name, score) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(perf_analysis[\u001b[33m'\u001b[39m\u001b[33mrankings\u001b[39m\u001b[33m'\u001b[39m])])\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33;43m\"\"\"\u001b[39;49m\n\u001b[32m    404\u001b[39m \u001b[33;43mâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\u001b[39;49m\n\u001b[32m    405\u001b[39m \u001b[33;43m                    4. ACCURACY & RELIABILITY\u001b[39;49m\n\u001b[32m    406\u001b[39m \u001b[33;43mâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\u001b[39;49m\n\u001b[32m    407\u001b[39m \n\u001b[32m    408\u001b[39m \u001b[33;43mOVERALL PERFORMANCE:\u001b[39;49m\n\u001b[32m    409\u001b[39m \u001b[33;43mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[39;49m\n\u001b[32m    410\u001b[39m \u001b[33;43m  â€¢ Best Model: \u001b[39;49m\u001b[38;5;132;43;01m{best_model}\u001b[39;49;00m\n\u001b[32m    411\u001b[39m \u001b[33;43m  â€¢ Best Score: \u001b[39;49m\u001b[38;5;132;43;01m{best_score:.1%}\u001b[39;49;00m\n\u001b[32m    412\u001b[39m \u001b[33;43m  â€¢ Model Size: \u001b[39;49m\u001b[38;5;132;43;01m{best_size}\u001b[39;49;00m\n\u001b[32m    413\u001b[39m \n\u001b[32m    414\u001b[39m \u001b[33;43m  â€¢ Average Manual Score: \u001b[39;49m\u001b[38;5;132;43;01m{avg_manual:.1%}\u001b[39;49;00m\n\u001b[32m    415\u001b[39m \u001b[33;43m  â€¢ Average LLM-Judge Score: \u001b[39;49m\u001b[38;5;132;43;01m{avg_llm:.1%}\u001b[39;49;00m\n\u001b[32m    416\u001b[39m \u001b[33;43m  â€¢ Average Confidence Gap: \u001b[39;49m\u001b[38;5;132;43;01m{avg_gap:+.1%}\u001b[39;49;00m\n\u001b[32m    417\u001b[39m \n\u001b[32m    418\u001b[39m \u001b[33;43mMODEL RANKINGS:\u001b[39;49m\n\u001b[32m    419\u001b[39m \u001b[33;43mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[39;49m\n\u001b[32m    420\u001b[39m \u001b[38;5;132;43;01m{rankings}\u001b[39;49;00m\n\u001b[32m    421\u001b[39m \n\u001b[32m    422\u001b[39m \u001b[33;43mRELIABILITY FACTORS:\u001b[39;49m\n\u001b[32m    423\u001b[39m \u001b[33;43mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[39;49m\n\u001b[32m    424\u001b[39m \n\u001b[32m    425\u001b[39m \u001b[33;43m  âœ… HIGH RELIABILITY:\u001b[39;49m\n\u001b[32m    426\u001b[39m \u001b[33;43m     â€¢ Questions with exact matches in database\u001b[39;49m\n\u001b[32m    427\u001b[39m \u001b[33;43m     â€¢ High confidence scores (>0.7) on answerable questions\u001b[39;49m\n\u001b[32m    428\u001b[39m \u001b[33;43m     â€¢ Common, frequently asked questions\u001b[39;49m\n\u001b[32m    429\u001b[39m \n\u001b[32m    430\u001b[39m \u001b[33;43m  âš ï¸ MEDIUM RELIABILITY:\u001b[39;49m\n\u001b[32m    431\u001b[39m \u001b[33;43m     â€¢ Semantically similar but not exact matches\u001b[39;49m\n\u001b[32m    432\u001b[39m \u001b[33;43m     â€¢ Medium confidence scores (0.4-0.7)\u001b[39;49m\n\u001b[32m    433\u001b[39m \u001b[33;43m     â€¢ Paraphrased questions\u001b[39;49m\n\u001b[32m    434\u001b[39m \n\u001b[32m    435\u001b[39m \u001b[33;43m  âŒ LOW RELIABILITY:\u001b[39;49m\n\u001b[32m    436\u001b[39m \u001b[33;43m     â€¢ Low confidence scores (<0.4)\u001b[39;49m\n\u001b[32m    437\u001b[39m \u001b[33;43m     â€¢ Out-of-scope questions\u001b[39;49m\n\u001b[32m    438\u001b[39m \u001b[33;43m     â€¢ Ambiguous or complex queries\u001b[39;49m\n\u001b[32m    439\u001b[39m \n\u001b[32m    440\u001b[39m \u001b[33;43mCONFIDENCE CALIBRATION:\u001b[39;49m\n\u001b[32m    441\u001b[39m \u001b[33;43mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[39;49m\n\u001b[32m    442\u001b[39m \u001b[33;43m  â€¢ Ideal: High confidence = correct, Low confidence = uncertain\u001b[39;49m\n\u001b[32m    443\u001b[39m \u001b[33;43m  â€¢ Current Gap: \u001b[39;49m\u001b[38;5;132;43;01m{avg_gap:+.1%}\u001b[39;49;00m\u001b[33;43m (answerable vs unanswerable)\u001b[39;49m\n\u001b[32m    444\u001b[39m \u001b[33;43m  â€¢ Assessment: \u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGood\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m if perf_analysis[\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mavg_confidence_gap\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m] > 0.1 else \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mNeeds improvement\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m}\u001b[39;49m\n\u001b[32m    445\u001b[39m \n\u001b[32m    446\u001b[39m \u001b[33;43m  Recommendation: Use confidence threshold of 0.5 for production\u001b[39;49m\n\u001b[32m    447\u001b[39m \u001b[33;43m  Below threshold: Escalate to human or ask for clarification\u001b[39;49m\n\u001b[32m    448\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m \u001b[33;43m\"\"\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbest_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mperf_analysis\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbest_model\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbest_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43mperf_analysis\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbest_model_score\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mperf_analysis\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbest_model_size\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m        \u001b[49m\u001b[43mavg_manual\u001b[49m\u001b[43m=\u001b[49m\u001b[43mperf_analysis\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mavg_manual_score\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m        \u001b[49m\u001b[43mavg_llm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mperf_analysis\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mavg_llm_score\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m        \u001b[49m\u001b[43mavg_gap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mperf_analysis\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mavg_confidence_gap\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrankings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrankings_str\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyError\u001b[39m: '\"Good\" if perf_analysis'"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# OBJECTIVE 6: ANALYZE SYSTEM LIMITATIONS\n",
        "# ============================================================================\n",
        "#\n",
        "# ANALYSIS FRAMEWORK:\n",
        "#   1. System Limitations - What can/cannot be answered, failure modes\n",
        "#   2. Real-World Applications - Deployment scenarios, business value\n",
        "#   3. Scalability - Performance at scale, bottlenecks\n",
        "#   4. Accuracy & Reliability - Error rates, confidence calibration\n",
        "#   5. User Experience - Interaction design, response quality\n",
        "#   6. Deployment - Infrastructure, costs, maintenance\n",
        "#   7. Improvements - Future enhancements, optimizations\n",
        "#\n",
        "# REUSES FROM PREVIOUS OBJECTIVES:\n",
        "#   - model_evaluations (Objective 5)\n",
        "#   - qa_database (Objective 2)\n",
        "#   - rag_results (Objective 4)\n",
        "#   - faiss_index (Objective 3)\n",
        "#\n",
        "# PREREQUISITES: Run Objectives 1-5 first\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 1: IMPORTS & VALIDATION\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "from typing import List, Dict\n",
        "from dataclasses import dataclass\n",
        "\n",
        "try:\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "except ImportError as e:\n",
        "    raise ImportError(f\"Missing: {e}. Run: pip install pandas numpy\")\n",
        "\n",
        "\n",
        "def validate_prerequisites():\n",
        "    \"\"\"Ensure Objectives 1-5 were run first.\"\"\"\n",
        "    required = {\n",
        "        'Objective 2': ['qa_database'],\n",
        "        'Objective 3': ['faiss_index'],\n",
        "        'Objective 4': ['rag_results'],\n",
        "        'Objective 5': ['model_evaluations']\n",
        "    }\n",
        "    \n",
        "    all_missing = []\n",
        "    for objective, items in required.items():\n",
        "        missing = [item for item in items if item not in globals()]\n",
        "        if missing:\n",
        "            all_missing.append(f\"{objective}: {missing}\")\n",
        "    \n",
        "    if all_missing:\n",
        "        raise RuntimeError(f\"Missing prerequisites:\\n\" + \"\\n\".join(all_missing))\n",
        "    \n",
        "    print(\"âœ… All prerequisites validated\")\n",
        "    return True\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 2: CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "OUTPUT_DIR = \"data/system_analysis\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 3: DATA CLASSES\n",
        "# ============================================================================\n",
        "\n",
        "@dataclass\n",
        "class SystemMetrics:\n",
        "    \"\"\"Container for system analysis metrics.\"\"\"\n",
        "    # Knowledge Base\n",
        "    total_qa_pairs: int = 0\n",
        "    categories_covered: int = 0\n",
        "    avg_answer_length: float = 0.0\n",
        "    \n",
        "    # Performance\n",
        "    best_model: str = \"\"\n",
        "    best_model_score: float = 0.0\n",
        "    avg_response_time: float = 0.0\n",
        "    fastest_model: str = \"\"\n",
        "    fastest_time: float = 0.0\n",
        "    \n",
        "    # Accuracy\n",
        "    answerable_accuracy: float = 0.0\n",
        "    unanswerable_detection: float = 0.0\n",
        "    avg_confidence_gap: float = 0.0\n",
        "    \n",
        "    # Retrieval\n",
        "    faiss_index_size: int = 0\n",
        "    embedding_dimension: int = 0\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 4: ANALYSIS FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def analyze_knowledge_base(qa_database: List[Dict]) -> Dict:\n",
        "    \"\"\"Analyze the Q&A knowledge base.\"\"\"\n",
        "    \n",
        "    # Basic stats\n",
        "    total = len(qa_database)\n",
        "    \n",
        "    # Category analysis\n",
        "    categories = {}\n",
        "    for qa in qa_database:\n",
        "        cat = qa.get('category', 'unknown')\n",
        "        categories[cat] = categories.get(cat, 0) + 1\n",
        "    \n",
        "    # Answer length analysis\n",
        "    answer_lengths = [len(qa['answer']) for qa in qa_database]\n",
        "    avg_length = sum(answer_lengths) / len(answer_lengths) if answer_lengths else 0\n",
        "    \n",
        "    # Answerable vs unanswerable\n",
        "    answerable = sum(1 for qa in qa_database if qa.get('answerable', True))\n",
        "    unanswerable = total - answerable\n",
        "    \n",
        "    return {\n",
        "        'total_pairs': total,\n",
        "        'categories': categories,\n",
        "        'num_categories': len(categories),\n",
        "        'avg_answer_length': avg_length,\n",
        "        'min_answer_length': min(answer_lengths) if answer_lengths else 0,\n",
        "        'max_answer_length': max(answer_lengths) if answer_lengths else 0,\n",
        "        'answerable_count': answerable,\n",
        "        'unanswerable_count': unanswerable\n",
        "    }\n",
        "\n",
        "\n",
        "def analyze_model_performance(evaluations: List) -> Dict:\n",
        "    \"\"\"Analyze model evaluation results.\"\"\"\n",
        "    \n",
        "    if not evaluations:\n",
        "        return {}\n",
        "    \n",
        "    # Find best model by average score\n",
        "    best = max(evaluations, \n",
        "               key=lambda e: (e.scores.manual_weighted + e.scores.llm_weighted) / 2)\n",
        "    best_avg = (best.scores.manual_weighted + best.scores.llm_weighted) / 2\n",
        "    \n",
        "    # Find fastest model\n",
        "    valid_times = [(e, e.avg_response_time) for e in evaluations if e.avg_response_time > 0]\n",
        "    fastest = min(valid_times, key=lambda x: x[1]) if valid_times else (None, 0)\n",
        "    \n",
        "    # Average metrics across all models\n",
        "    avg_manual = sum(e.scores.manual_weighted for e in evaluations) / len(evaluations)\n",
        "    avg_llm = sum(e.scores.llm_weighted for e in evaluations) / len(evaluations)\n",
        "    avg_time = sum(e.avg_response_time for e in evaluations) / len(evaluations)\n",
        "    avg_conf_gap = sum(e.confidence_gap for e in evaluations) / len(evaluations)\n",
        "    \n",
        "    # Model rankings\n",
        "    ranked = sorted(evaluations, \n",
        "                   key=lambda e: (e.scores.manual_weighted + e.scores.llm_weighted) / 2,\n",
        "                   reverse=True)\n",
        "    \n",
        "    return {\n",
        "        'best_model': best.model_name,\n",
        "        'best_model_score': best_avg,\n",
        "        'best_model_size': best.model_size,\n",
        "        'fastest_model': fastest[0].model_name if fastest[0] else 'N/A',\n",
        "        'fastest_time': fastest[1],\n",
        "        'avg_manual_score': avg_manual,\n",
        "        'avg_llm_score': avg_llm,\n",
        "        'avg_response_time': avg_time,\n",
        "        'avg_confidence_gap': avg_conf_gap,\n",
        "        'rankings': [(e.model_name, (e.scores.manual_weighted + e.scores.llm_weighted) / 2) \n",
        "                     for e in ranked]\n",
        "    }\n",
        "\n",
        "\n",
        "def analyze_retrieval_system(faiss_index) -> Dict:\n",
        "    \"\"\"Analyze FAISS retrieval system.\"\"\"\n",
        "    \n",
        "    return {\n",
        "        'index_size': faiss_index.ntotal,\n",
        "        'dimension': faiss_index.d,\n",
        "        'index_type': type(faiss_index).__name__\n",
        "    }\n",
        "\n",
        "\n",
        "def identify_failure_modes(evaluations: List) -> List[Dict]:\n",
        "    \"\"\"Identify common failure modes from model evaluations.\"\"\"\n",
        "    \n",
        "    failures = []\n",
        "    \n",
        "    for e in evaluations:\n",
        "        for r in e.responses:\n",
        "            if not r.success:\n",
        "                failures.append({\n",
        "                    'model': r.model_name,\n",
        "                    'question': r.question,\n",
        "                    'error': r.error_message,\n",
        "                    'type': 'execution_error'\n",
        "                })\n",
        "            elif r.is_answerable and len(r.answer.strip()) < 10:\n",
        "                failures.append({\n",
        "                    'model': r.model_name,\n",
        "                    'question': r.question,\n",
        "                    'answer': r.answer,\n",
        "                    'type': 'incomplete_answer'\n",
        "                })\n",
        "            elif not r.is_answerable and r.confidence > 0.7:\n",
        "                failures.append({\n",
        "                    'model': r.model_name,\n",
        "                    'question': r.question,\n",
        "                    'answer': r.answer,\n",
        "                    'confidence': r.confidence,\n",
        "                    'type': 'false_confidence'\n",
        "                })\n",
        "    \n",
        "    return failures\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 5: REPORT GENERATION\n",
        "# ============================================================================\n",
        "\n",
        "def generate_limitations_report(kb_analysis: Dict, perf_analysis: Dict, \n",
        "                                retrieval_analysis: Dict, failures: List) -> str:\n",
        "    \"\"\"Generate the system limitations section.\"\"\"\n",
        "    \n",
        "    report = \"\"\"\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "                        1. SYSTEM LIMITATIONS\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "WHAT THE SYSTEM CAN ANSWER:\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  âœ… Questions directly covered in the Q&A database\n",
        "  âœ… Questions semantically similar to database entries\n",
        "  âœ… Factual questions about products, policies, procedures\n",
        "  \n",
        "  Coverage: {total} Q&A pairs across {cats} categories\n",
        "  Answerable questions in database: {ans}\n",
        "  Unanswerable examples in database: {unans}\n",
        "\n",
        "WHAT THE SYSTEM CANNOT ANSWER:\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  âŒ Questions requiring reasoning beyond retrieved context\n",
        "  âŒ Questions about topics not in knowledge base\n",
        "  âŒ Questions requiring real-time or external data\n",
        "  âŒ Complex multi-step questions\n",
        "  âŒ Subjective or opinion-based questions\n",
        "\n",
        "FAILURE MODES IDENTIFIED:\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  Total failures detected: {fail_count}\n",
        "  \n",
        "  â€¢ Incomplete Answers: Model returns fragment instead of full answer\n",
        "    Example: Returning \"30\" instead of \"30-day return policy\"\n",
        "    \n",
        "  â€¢ False Confidence: High confidence on unanswerable questions\n",
        "    Risk: Model may hallucinate answers\n",
        "    \n",
        "  â€¢ Execution Errors: Model fails to generate response\n",
        "    Cause: Context too long, model limitations\n",
        "    \n",
        "  â€¢ Retrieval Misses: Wrong context retrieved for question\n",
        "    Cause: Semantic gap between query and database\n",
        "\n",
        "EDGE CASES:\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  â€¢ Ambiguous questions (multiple valid interpretations)\n",
        "  â€¢ Questions with typos or grammatical errors\n",
        "  â€¢ Questions in different languages\n",
        "  â€¢ Very long or complex questions\n",
        "  â€¢ Questions combining multiple topics\n",
        "\n",
        "\"\"\".format(\n",
        "        total=kb_analysis['total_pairs'],\n",
        "        cats=kb_analysis['num_categories'],\n",
        "        ans=kb_analysis['answerable_count'],\n",
        "        unans=kb_analysis['unanswerable_count'],\n",
        "        fail_count=len(failures)\n",
        "    )\n",
        "    \n",
        "    return report\n",
        "\n",
        "\n",
        "def generate_applications_report() -> str:\n",
        "    \"\"\"Generate the real-world applications section.\"\"\"\n",
        "    \n",
        "    return \"\"\"\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "                    2. REAL-WORLD APPLICATIONS\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "SUITABLE DEPLOYMENT SCENARIOS:\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "  ğŸ“ CUSTOMER SERVICE CHATBOT\n",
        "     â€¢ First-line support for common questions\n",
        "     â€¢ Reduce load on human agents\n",
        "     â€¢ 24/7 availability\n",
        "     â€¢ Consistent responses\n",
        "     \n",
        "  ğŸ“š INTERNAL FAQ SYSTEM\n",
        "     â€¢ Employee self-service portal\n",
        "     â€¢ HR policy questions\n",
        "     â€¢ IT helpdesk automation\n",
        "     â€¢ Onboarding assistance\n",
        "     \n",
        "  ğŸ¢ KNOWLEDGE BASE ASSISTANT\n",
        "     â€¢ Product documentation search\n",
        "     â€¢ Technical support queries\n",
        "     â€¢ Training material access\n",
        "     â€¢ Process documentation\n",
        "     \n",
        "  ğŸ« HELP DESK AUTOMATION\n",
        "     â€¢ Ticket deflection\n",
        "     â€¢ Auto-response for common issues\n",
        "     â€¢ Escalation routing\n",
        "     â€¢ Response suggestions for agents\n",
        "\n",
        "BUSINESS VALUE:\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  â€¢ Reduced response time: Instant answers vs waiting for human\n",
        "  â€¢ Cost savings: Fewer human agents needed for basic queries\n",
        "  â€¢ Consistency: Same answer every time for same question\n",
        "  â€¢ Scalability: Handle more queries without linear cost increase\n",
        "  â€¢ Analytics: Track common questions and gaps\n",
        "\n",
        "NOT SUITABLE FOR:\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  âŒ Medical diagnosis or advice\n",
        "  âŒ Legal guidance\n",
        "  âŒ Financial recommendations\n",
        "  âŒ Emergency situations\n",
        "  âŒ Highly personalized queries requiring account access\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def generate_scalability_report(kb_analysis: Dict, retrieval_analysis: Dict) -> str:\n",
        "    \"\"\"Generate the scalability analysis section.\"\"\"\n",
        "    \n",
        "    return \"\"\"\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "                      3. SCALABILITY ANALYSIS\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "CURRENT SYSTEM SCALE:\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  â€¢ Q&A Pairs: {total}\n",
        "  â€¢ FAISS Index Size: {index_size} vectors\n",
        "  â€¢ Embedding Dimension: {dim}\n",
        "  â€¢ Index Type: {index_type}\n",
        "\n",
        "PROJECTED PERFORMANCE AT SCALE:\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  \n",
        "  â”‚ Scale      â”‚ Entries â”‚ Index Size â”‚ Search Time â”‚ Notes           â”‚\n",
        "  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "  â”‚ Current    â”‚ {total:<7} â”‚ ~1 MB      â”‚ <10ms       â”‚ âœ… Excellent    â”‚\n",
        "  â”‚ Small      â”‚ 1,000   â”‚ ~5 MB      â”‚ <20ms       â”‚ âœ… Good         â”‚\n",
        "  â”‚ Medium     â”‚ 10,000  â”‚ ~50 MB     â”‚ <50ms       â”‚ âœ… Acceptable   â”‚\n",
        "  â”‚ Large      â”‚ 100,000 â”‚ ~500 MB    â”‚ <100ms      â”‚ âš ï¸ May need IVF â”‚\n",
        "  â”‚ Very Large â”‚ 1M+     â”‚ ~5 GB      â”‚ Varies      â”‚ âŒ Need HNSW    â”‚\n",
        "\n",
        "BOTTLENECKS:\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  1. EMBEDDING GENERATION\n",
        "     â€¢ Current: ~50ms per query\n",
        "     â€¢ At scale: Batch processing recommended\n",
        "     â€¢ Solution: GPU acceleration, caching\n",
        "     \n",
        "  2. FAISS SEARCH\n",
        "     â€¢ Current: IndexFlatL2 (exact search)\n",
        "     â€¢ At scale: Need approximate search (IVF, HNSW)\n",
        "     â€¢ Trade-off: Speed vs accuracy\n",
        "     \n",
        "  3. LLM GENERATION\n",
        "     â€¢ Current: ~1-2s per response\n",
        "     â€¢ At scale: Main bottleneck\n",
        "     â€¢ Solution: Smaller models, caching, batching\n",
        "\n",
        "RECOMMENDATIONS FOR SCALE:\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  â€¢ 1K-10K: Keep current setup, add caching\n",
        "  â€¢ 10K-100K: Switch to FAISS IVF index\n",
        "  â€¢ 100K+: Use HNSW, consider vector database (Pinecone, Weaviate)\n",
        "  â€¢ 1M+: Distributed architecture, sharding\n",
        "\n",
        "\"\"\".format(\n",
        "        total=kb_analysis['total_pairs'],\n",
        "        index_size=retrieval_analysis['index_size'],\n",
        "        dim=retrieval_analysis['dimension'],\n",
        "        index_type=retrieval_analysis['index_type']\n",
        "    )\n",
        "\n",
        "\n",
        "def generate_accuracy_report(perf_analysis: Dict) -> str:\n",
        "    \"\"\"Generate the accuracy and reliability section.\"\"\"\n",
        "    \n",
        "    rankings_str = \"\\n\".join([f\"     {i+1}. {name}: {score:.1%}\" \n",
        "                              for i, (name, score) in enumerate(perf_analysis['rankings'])])\n",
        "    \n",
        "    return \"\"\"\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "                    4. ACCURACY & RELIABILITY\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "OVERALL PERFORMANCE:\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  â€¢ Best Model: {best_model}\n",
        "  â€¢ Best Score: {best_score:.1%}\n",
        "  â€¢ Model Size: {best_size}\n",
        "  \n",
        "  â€¢ Average Manual Score: {avg_manual:.1%}\n",
        "  â€¢ Average LLM-Judge Score: {avg_llm:.1%}\n",
        "  â€¢ Average Confidence Gap: {avg_gap:+.1%}\n",
        "\n",
        "MODEL RANKINGS:\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "{rankings}\n",
        "\n",
        "RELIABILITY FACTORS:\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  \n",
        "  âœ… HIGH RELIABILITY:\n",
        "     â€¢ Questions with exact matches in database\n",
        "     â€¢ High confidence scores (>0.7) on answerable questions\n",
        "     â€¢ Common, frequently asked questions\n",
        "     \n",
        "  âš ï¸ MEDIUM RELIABILITY:\n",
        "     â€¢ Semantically similar but not exact matches\n",
        "     â€¢ Medium confidence scores (0.4-0.7)\n",
        "     â€¢ Paraphrased questions\n",
        "     \n",
        "  âŒ LOW RELIABILITY:\n",
        "     â€¢ Low confidence scores (<0.4)\n",
        "     â€¢ Out-of-scope questions\n",
        "     â€¢ Ambiguous or complex queries\n",
        "\n",
        "CONFIDENCE CALIBRATION:\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  â€¢ Ideal: High confidence = correct, Low confidence = uncertain\n",
        "  â€¢ Current Gap: {avg_gap:+.1%} (answerable vs unanswerable)\n",
        "  â€¢ Assessment: {\"Good\" if perf_analysis['avg_confidence_gap'] > 0.1 else \"Needs improvement\"}\n",
        "  \n",
        "  Recommendation: Use confidence threshold of 0.5 for production\n",
        "  Below threshold: Escalate to human or ask for clarification\n",
        "\n",
        "\"\"\".format(\n",
        "        best_model=perf_analysis['best_model'],\n",
        "        best_score=perf_analysis['best_model_score'],\n",
        "        best_size=perf_analysis['best_model_size'],\n",
        "        avg_manual=perf_analysis['avg_manual_score'],\n",
        "        avg_llm=perf_analysis['avg_llm_score'],\n",
        "        avg_gap=perf_analysis['avg_confidence_gap'],\n",
        "        rankings=rankings_str\n",
        "    )\n",
        "\n",
        "\n",
        "def generate_ux_report(perf_analysis: Dict) -> str:\n",
        "    \"\"\"Generate the user experience section.\"\"\"\n",
        "    \n",
        "    return \"\"\"\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "                       5. USER EXPERIENCE\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "RESPONSE TIME ANALYSIS:\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  â€¢ Average Response Time: {avg_time:.2f}s\n",
        "  â€¢ Fastest Model: {fastest} ({fastest_time:.3f}s)\n",
        "  \n",
        "  â”‚ Latency    â”‚ User Perception    â”‚ Recommendation      â”‚\n",
        "  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "  â”‚ <0.5s      â”‚ Instant            â”‚ âœ… Excellent        â”‚\n",
        "  â”‚ 0.5-1s     â”‚ Fast               â”‚ âœ… Good             â”‚\n",
        "  â”‚ 1-2s       â”‚ Acceptable         â”‚ âš ï¸ Show loading     â”‚\n",
        "  â”‚ 2-5s       â”‚ Slow               â”‚ âŒ Optimize needed  â”‚\n",
        "  â”‚ >5s        â”‚ Frustrating        â”‚ âŒ Major issue      â”‚\n",
        "\n",
        "INTERACTION DESIGN RECOMMENDATIONS:\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  \n",
        "  1. LOADING INDICATORS\n",
        "     â€¢ Show typing indicator while generating\n",
        "     â€¢ Display \"Searching...\" during retrieval\n",
        "     â€¢ Progress feedback for long queries\n",
        "     \n",
        "  2. RESPONSE FORMATTING\n",
        "     â€¢ Clear, concise answers\n",
        "     â€¢ Bullet points for lists\n",
        "     â€¢ Links to full documentation\n",
        "     â€¢ \"Was this helpful?\" feedback\n",
        "     \n",
        "  3. FALLBACK HANDLING\n",
        "     â€¢ \"I'm not sure, let me connect you with a human\"\n",
        "     â€¢ Suggest related questions\n",
        "     â€¢ Offer to rephrase or clarify\n",
        "     \n",
        "  4. CONFIDENCE DISPLAY\n",
        "     â€¢ Don't show raw confidence scores to users\n",
        "     â€¢ Use phrases: \"I'm confident that...\" vs \"I think...\"\n",
        "     â€¢ Offer to escalate for low-confidence answers\n",
        "\n",
        "ACCESSIBILITY CONSIDERATIONS:\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  â€¢ Screen reader compatibility\n",
        "  â€¢ Keyboard navigation\n",
        "  â€¢ Clear error messages\n",
        "  â€¢ Multiple input methods (text, voice)\n",
        "\n",
        "\"\"\".format(\n",
        "        avg_time=perf_analysis['avg_response_time'],\n",
        "        fastest=perf_analysis['fastest_model'],\n",
        "        fastest_time=perf_analysis['fastest_time']\n",
        "    )\n",
        "\n",
        "\n",
        "def generate_deployment_report() -> str:\n",
        "    \"\"\"Generate the deployment considerations section.\"\"\"\n",
        "    \n",
        "    return \"\"\"\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "                   6. DEPLOYMENT CONSIDERATIONS\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "INFRASTRUCTURE REQUIREMENTS:\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "  MINIMUM (Development/Testing):\n",
        "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "  â”‚ â€¢ CPU: 4 cores                                                  â”‚\n",
        "  â”‚ â€¢ RAM: 16 GB                                                    â”‚\n",
        "  â”‚ â€¢ Storage: 10 GB                                                â”‚\n",
        "  â”‚ â€¢ GPU: Optional (CPU inference possible)                        â”‚\n",
        "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "  \n",
        "  RECOMMENDED (Production):\n",
        "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "  â”‚ â€¢ CPU: 8+ cores                                                 â”‚\n",
        "  â”‚ â€¢ RAM: 32 GB                                                    â”‚\n",
        "  â”‚ â€¢ Storage: 50 GB SSD                                            â”‚\n",
        "  â”‚ â€¢ GPU: NVIDIA T4 or better (for fast inference)                 â”‚\n",
        "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\n",
        "COST ESTIMATES (Monthly):\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  \n",
        "  â”‚ Deployment      â”‚ Compute    â”‚ Storage â”‚ Total    â”‚\n",
        "  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "  â”‚ CPU-only (AWS)  â”‚ $150-300   â”‚ $10     â”‚ ~$200    â”‚\n",
        "  â”‚ GPU (AWS g4dn)  â”‚ $400-800   â”‚ $10     â”‚ ~$500    â”‚\n",
        "  â”‚ Serverless      â”‚ Pay/query  â”‚ $10     â”‚ Variable â”‚\n",
        "  â”‚ On-premise      â”‚ One-time   â”‚ -       â”‚ ~$5000   â”‚\n",
        "\n",
        "MAINTENANCE REQUIREMENTS:\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  \n",
        "  DAILY:\n",
        "  â€¢ Monitor response times and error rates\n",
        "  â€¢ Check system health and logs\n",
        "  \n",
        "  WEEKLY:\n",
        "  â€¢ Review low-confidence responses\n",
        "  â€¢ Analyze unanswered questions\n",
        "  â€¢ Update metrics dashboard\n",
        "  \n",
        "  MONTHLY:\n",
        "  â€¢ Add new Q&A pairs based on gaps\n",
        "  â€¢ Retrain/update embeddings if KB changes significantly\n",
        "  â€¢ Performance benchmarking\n",
        "  \n",
        "  QUARTERLY:\n",
        "  â€¢ Evaluate newer models\n",
        "  â€¢ User satisfaction survey\n",
        "  â€¢ ROI analysis\n",
        "\n",
        "MONITORING & LOGGING:\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  â€¢ Query logs: What users are asking\n",
        "  â€¢ Response logs: What the system answered\n",
        "  â€¢ Latency metrics: Response times\n",
        "  â€¢ Error tracking: Failed queries\n",
        "  â€¢ Confidence distribution: Score patterns\n",
        "  â€¢ User feedback: Thumbs up/down\n",
        "\n",
        "SECURITY CONSIDERATIONS:\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  â€¢ Input sanitization (prevent prompt injection)\n",
        "  â€¢ Rate limiting (prevent abuse)\n",
        "  â€¢ Data encryption (at rest and in transit)\n",
        "  â€¢ Access control (who can query, who can update KB)\n",
        "  â€¢ Audit logging (compliance requirements)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def generate_improvements_report() -> str:\n",
        "    \"\"\"Generate the improvement opportunities section.\"\"\"\n",
        "    \n",
        "    return \"\"\"\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "                  7. IMPROVEMENT OPPORTUNITIES\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "SHORT-TERM IMPROVEMENTS (1-2 weeks):\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  \n",
        "  1. EXPAND KNOWLEDGE BASE\n",
        "     â€¢ Add more Q&A pairs (target: 100+)\n",
        "     â€¢ Cover more edge cases\n",
        "     â€¢ Add variations of common questions\n",
        "     \n",
        "  2. IMPROVE PROMPTS\n",
        "     â€¢ Refine system prompt for better responses\n",
        "     â€¢ Add few-shot examples for edge cases\n",
        "     â€¢ Better handling of \"I don't know\"\n",
        "     \n",
        "  3. CACHING\n",
        "     â€¢ Cache frequent queries\n",
        "     â€¢ Cache embeddings for common questions\n",
        "     â€¢ Reduce redundant LLM calls\n",
        "\n",
        "MEDIUM-TERM IMPROVEMENTS (1-3 months):\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  \n",
        "  1. HYBRID SEARCH\n",
        "     â€¢ Combine semantic search with keyword search\n",
        "     â€¢ BM25 + FAISS for better retrieval\n",
        "     â€¢ Reranking with cross-encoder\n",
        "     \n",
        "  2. FINE-TUNING\n",
        "     â€¢ Fine-tune embedding model on domain data\n",
        "     â€¢ Fine-tune QA model on company-specific Q&A\n",
        "     â€¢ Domain adaptation for better accuracy\n",
        "     \n",
        "  3. MULTI-TURN CONVERSATION\n",
        "     â€¢ Context tracking across turns\n",
        "     â€¢ Follow-up question handling\n",
        "     â€¢ Conversation memory\n",
        "\n",
        "LONG-TERM IMPROVEMENTS (3-6 months):\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  \n",
        "  1. ADVANCED RETRIEVAL\n",
        "     â€¢ Query expansion and rewriting\n",
        "     â€¢ Multi-index search (products, policies, FAQs)\n",
        "     â€¢ Knowledge graph integration\n",
        "     \n",
        "  2. ACTIVE LEARNING\n",
        "     â€¢ Learn from user feedback\n",
        "     â€¢ Automatic KB updates from conversations\n",
        "     â€¢ Continuous model improvement\n",
        "     \n",
        "  3. MULTI-MODAL\n",
        "     â€¢ Support for images (product photos)\n",
        "     â€¢ Voice input/output\n",
        "     â€¢ Document understanding\n",
        "\n",
        "METRICS TO TRACK FOR IMPROVEMENT:\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  â€¢ Answer accuracy (human evaluation)\n",
        "  â€¢ User satisfaction (feedback scores)\n",
        "  â€¢ Resolution rate (% questions answered without escalation)\n",
        "  â€¢ Time to resolution\n",
        "  â€¢ Knowledge base coverage (% of queries with relevant KB entry)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 6: SAVE FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def save_analysis_report(full_report: str):\n",
        "    \"\"\"Save the complete analysis report.\"\"\"\n",
        "    filepath = os.path.join(OUTPUT_DIR, \"system_analysis.txt\")\n",
        "    with open(filepath, 'w') as f:\n",
        "        f.write(full_report)\n",
        "    print(f\"âœ… Saved: {filepath}\")\n",
        "\n",
        "\n",
        "def save_metrics_csv(kb_analysis: Dict, perf_analysis: Dict, retrieval_analysis: Dict):\n",
        "    \"\"\"Save metrics to CSV.\"\"\"\n",
        "    metrics = {\n",
        "        'metric': [],\n",
        "        'value': []\n",
        "    }\n",
        "    \n",
        "    # KB metrics\n",
        "    metrics['metric'].append('Total Q&A Pairs')\n",
        "    metrics['value'].append(kb_analysis['total_pairs'])\n",
        "    metrics['metric'].append('Categories')\n",
        "    metrics['value'].append(kb_analysis['num_categories'])\n",
        "    metrics['metric'].append('Avg Answer Length')\n",
        "    metrics['value'].append(f\"{kb_analysis['avg_answer_length']:.0f}\")\n",
        "    \n",
        "    # Performance metrics\n",
        "    metrics['metric'].append('Best Model')\n",
        "    metrics['value'].append(perf_analysis['best_model'])\n",
        "    metrics['metric'].append('Best Score')\n",
        "    metrics['value'].append(f\"{perf_analysis['best_model_score']:.1%}\")\n",
        "    metrics['metric'].append('Avg Response Time')\n",
        "    metrics['value'].append(f\"{perf_analysis['avg_response_time']:.3f}s\")\n",
        "    metrics['metric'].append('Fastest Model')\n",
        "    metrics['value'].append(perf_analysis['fastest_model'])\n",
        "    \n",
        "    # Retrieval metrics\n",
        "    metrics['metric'].append('FAISS Index Size')\n",
        "    metrics['value'].append(retrieval_analysis['index_size'])\n",
        "    metrics['metric'].append('Embedding Dimension')\n",
        "    metrics['value'].append(retrieval_analysis['dimension'])\n",
        "    \n",
        "    df = pd.DataFrame(metrics)\n",
        "    filepath = os.path.join(OUTPUT_DIR, \"metrics_summary.csv\")\n",
        "    df.to_csv(filepath, index=False)\n",
        "    print(f\"âœ… Saved: {filepath}\")\n",
        "\n",
        "\n",
        "def save_recommendations(recommendations: str):\n",
        "    \"\"\"Save recommendations to file.\"\"\"\n",
        "    filepath = os.path.join(OUTPUT_DIR, \"recommendations.txt\")\n",
        "    with open(filepath, 'w') as f:\n",
        "        f.write(recommendations)\n",
        "    print(f\"âœ… Saved: {filepath}\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 7: EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"         OBJECTIVE 6: ANALYZE SYSTEM LIMITATIONS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Step 1: Validate\n",
        "print(\"\\nğŸ” STEP 1: Validate Prerequisites\")\n",
        "print(\"â”€\"*70)\n",
        "validate_prerequisites()\n",
        "\n",
        "# Get data from previous objectives\n",
        "qa_database = globals()['qa_database']\n",
        "faiss_index = globals()['faiss_index']\n",
        "model_evaluations = globals()['model_evaluations']\n",
        "\n",
        "# Step 2: Analyze Components\n",
        "print(\"\\nğŸ“Š STEP 2: Analyze System Components\")\n",
        "print(\"â”€\"*70)\n",
        "\n",
        "print(\"   Analyzing knowledge base...\")\n",
        "kb_analysis = analyze_knowledge_base(qa_database)\n",
        "print(f\"   âœ“ {kb_analysis['total_pairs']} Q&A pairs, {kb_analysis['num_categories']} categories\")\n",
        "\n",
        "print(\"   Analyzing model performance...\")\n",
        "perf_analysis = analyze_model_performance(model_evaluations)\n",
        "print(f\"   âœ“ Best model: {perf_analysis['best_model']} ({perf_analysis['best_model_score']:.1%})\")\n",
        "\n",
        "print(\"   Analyzing retrieval system...\")\n",
        "retrieval_analysis = analyze_retrieval_system(faiss_index)\n",
        "print(f\"   âœ“ FAISS index: {retrieval_analysis['index_size']} vectors\")\n",
        "\n",
        "print(\"   Identifying failure modes...\")\n",
        "failures = identify_failure_modes(model_evaluations)\n",
        "print(f\"   âœ“ {len(failures)} failure cases identified\")\n",
        "\n",
        "# Step 3: Generate Reports\n",
        "print(\"\\nğŸ“ STEP 3: Generate Analysis Reports\")\n",
        "print(\"â”€\"*70)\n",
        "\n",
        "# Generate all sections\n",
        "limitations = generate_limitations_report(kb_analysis, perf_analysis, retrieval_analysis, failures)\n",
        "applications = generate_applications_report()\n",
        "scalability = generate_scalability_report(kb_analysis, retrieval_analysis)\n",
        "accuracy = generate_accuracy_report(perf_analysis)\n",
        "ux = generate_ux_report(perf_analysis)\n",
        "deployment = generate_deployment_report()\n",
        "improvements = generate_improvements_report()\n",
        "\n",
        "# Combine into full report\n",
        "full_report = f\"\"\"\n",
        "{'='*70}\n",
        "           SYSTEM ANALYSIS REPORT - RAG CUSTOMER SERVICE BOT\n",
        "{'='*70}\n",
        "\n",
        "Generated from Objectives 1-5 results.\n",
        "This report analyzes system limitations, applications, and improvements.\n",
        "\n",
        "{limitations}\n",
        "{applications}\n",
        "{scalability}\n",
        "{accuracy}\n",
        "{ux}\n",
        "{deployment}\n",
        "{improvements}\n",
        "\n",
        "{'='*70}\n",
        "                         END OF REPORT\n",
        "{'='*70}\n",
        "\"\"\"\n",
        "\n",
        "# Step 4: Display Summary\n",
        "print(\"\\nğŸ“ˆ STEP 4: Analysis Summary\")\n",
        "print(\"â”€\"*70)\n",
        "\n",
        "print(f\"\"\"\n",
        "  KNOWLEDGE BASE:\n",
        "  â€¢ Total Q&A Pairs: {kb_analysis['total_pairs']}\n",
        "  â€¢ Categories: {kb_analysis['num_categories']}\n",
        "  â€¢ Avg Answer Length: {kb_analysis['avg_answer_length']:.0f} chars\n",
        "  \n",
        "  PERFORMANCE:\n",
        "  â€¢ Best Model: {perf_analysis['best_model']}\n",
        "  â€¢ Best Score: {perf_analysis['best_model_score']:.1%}\n",
        "  â€¢ Avg Response Time: {perf_analysis['avg_response_time']:.3f}s\n",
        "  â€¢ Fastest: {perf_analysis['fastest_model']} ({perf_analysis['fastest_time']:.3f}s)\n",
        "  \n",
        "  RETRIEVAL:\n",
        "  â€¢ FAISS Index: {retrieval_analysis['index_size']} vectors\n",
        "  â€¢ Dimension: {retrieval_analysis['dimension']}\n",
        "  \n",
        "  ISSUES:\n",
        "  â€¢ Failure Cases: {len(failures)}\n",
        "\"\"\")\n",
        "\n",
        "# Step 5: Save Results\n",
        "print(\"\\nğŸ’¾ STEP 5: Save Results\")\n",
        "print(\"â”€\"*70)\n",
        "\n",
        "save_analysis_report(full_report)\n",
        "save_metrics_csv(kb_analysis, perf_analysis, retrieval_analysis)\n",
        "save_recommendations(improvements)\n",
        "\n",
        "# Store globally\n",
        "globals()['system_analysis'] = {\n",
        "    'kb_analysis': kb_analysis,\n",
        "    'perf_analysis': perf_analysis,\n",
        "    'retrieval_analysis': retrieval_analysis,\n",
        "    'failures': failures\n",
        "}\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"                 âœ… OBJECTIVE 6 COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\"\"\n",
        "   Analysis Sections Generated:\n",
        "   1. System Limitations\n",
        "   2. Real-World Applications\n",
        "   3. Scalability Analysis\n",
        "   4. Accuracy & Reliability\n",
        "   5. User Experience\n",
        "   6. Deployment Considerations\n",
        "   7. Improvement Opportunities\n",
        "   \n",
        "   ğŸ“¦ FILES:\n",
        "      â€¢ {OUTPUT_DIR}/system_analysis.txt\n",
        "      â€¢ {OUTPUT_DIR}/metrics_summary.csv\n",
        "      â€¢ {OUTPUT_DIR}/recommendations.txt\n",
        "   \n",
        "   ğŸ“ KEY FINDINGS:\n",
        "      â€¢ Best Model: {perf_analysis['best_model']}\n",
        "      â€¢ System ready for: Small-scale deployment\n",
        "      â€¢ Main limitation: Knowledge base size ({kb_analysis['total_pairs']} pairs)\n",
        "      â€¢ Priority improvement: Expand Q&A database\n",
        "\"\"\")\n",
        "print(\"=\"*70)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
