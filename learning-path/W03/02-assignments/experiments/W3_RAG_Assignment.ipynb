{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/oviya-raja/ist-402/blob/main/learning-path/W03/02-assignments/experiments/W3_RAG_Assignment.ipynb)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# RAG Assignment: \n",
    "## Building an Intelligent Q&A System with FAISS and Mistral\n",
    "\n",
    "**IST402 - AI Agents & RAG Systems**  \n",
    "**Student**: Oviya Raja\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Assignment Overview\n",
    "\n",
    "### ğŸ¯ Goal\n",
    "I will design and implement a complete **Retrieval-Augmented Generation (RAG) system** that can answer questions based on a custom knowledge base, evaluate multiple AI models, and analyze system performance.\n",
    "\n",
    "**What I'm Building:**\n",
    "A production-ready RAG system that combines semantic search (FAISS) with large language models (Mistral) to provide accurate, context-aware answers for customer service scenarios.\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ¢ Business Context</b> (Click to expand)</summary>\n",
    "\n",
    "**Company:** GreenTech Marketplace\n",
    "\n",
    "**Company Description:**\n",
    "GreenTech Marketplace is an e-commerce platform specializing in sustainable technology products. The company focuses on eco-friendly technology solutions, renewable energy products, and smart home devices that help customers reduce their environmental footprint.\n",
    "\n",
    "**Company Details:**\n",
    "- **Type:** E-commerce platform\n",
    "- **Specialization:** Sustainable technology products\n",
    "- **Mission:** Providing eco-friendly technology solutions for modern living\n",
    "\n",
    "**Contact Information:**\n",
    "- **Email:** support@greentechmarketplace.com\n",
    "- **Phone:** 1-800-GREEN-TECH\n",
    "- **Business Hours:** \n",
    "  - Monday-Friday: 9 AM - 6 PM EST\n",
    "  - Saturday: 10 AM - 4 PM EST\n",
    "  - Sunday: Closed\n",
    "\n",
    "**Shipping & Policies:**\n",
    "- **Standard Shipping:** 5-7 business days (free over $75)\n",
    "- **Express Shipping:** 2-3 business days (additional fee)\n",
    "- **Return Policy:** 30-day return policy for unopened items in original packaging\n",
    "- **Refund Processing:** 5-7 business days after receipt\n",
    "- **Warranty:** 1-3 years depending on product category\n",
    "\n",
    "**Product Categories:**\n",
    "- **Solar Panels** - Renewable energy solutions for homes and businesses\n",
    "- **Energy-Efficient Appliances** - Eco-friendly home appliances\n",
    "- **Smart Home Devices** - Home automation and IoT solutions\n",
    "- **Eco-Friendly Accessories** - Sustainable lifestyle products\n",
    "\n",
    "**Q&A Database Categories (21 pairs total):**\n",
    "\n",
    "**Answerable Categories (18 pairs - 6 categories Ã— 3 pairs each):**\n",
    "\n",
    "| Category | Count | Description |\n",
    "|----------|-------|-------------|\n",
    "| **products** | 3 pairs | Types of products sold, solar panels, smart home devices, eco-friendly items |\n",
    "| **shipping** | 3 pairs | Delivery times, shipping costs, free shipping threshold, tracking |\n",
    "| **returns** | 3 pairs | Return policy, refund process, conditions, 30-day policy |\n",
    "| **customer_service** | 3 pairs | Business hours (Mon-Fri 9AM-6PM, Sat 10AM-4PM), contact email and phone |\n",
    "| **warranty** | 3 pairs | Warranty duration (1-3 years), coverage, claims process |\n",
    "| **orders** | 3 pairs | Order status, tracking, modifications, cancellations |\n",
    "\n",
    "**Unanswerable Types (3 pairs - 3 types Ã— 1 pair each):**\n",
    "\n",
    "| Type | Count | Description |\n",
    "|------|-------|-------------|\n",
    "| **competitor** | 1 pair | Questions about competitor pricing, products, or comparisons |\n",
    "| **personal_advice** | 1 pair | Questions asking for personal recommendations or opinions |\n",
    "| **future_events** | 1 pair | Questions about future sales, unreleased products, or predictions |\n",
    "\n",
    "**Total:** 18 answerable + 3 unanswerable = 21 Q&A pairs\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ”§ Technologies & Tools</b> (Click to expand)</summary>\n",
    "\n",
    "| Technology | Purpose | Version/Model |\n",
    "|------------|---------|---------------|\n",
    "| **Mistral-7B-Instruct** | LLM for generating Q&A pairs and system prompts | `mistralai/Mistral-7B-Instruct-v0.3` |\n",
    "| **FAISS** | Vector database for efficient similarity search | `faiss-cpu` |\n",
    "| **Sentence Transformers** | Text embeddings for semantic search | `all-MiniLM-L6-v2` |\n",
    "| **Hugging Face Transformers** | Multiple QA models for evaluation | Various models |\n",
    "| **RAGAS** | RAG Assessment framework for evaluation | Latest version |\n",
    "| **LangChain** | LLM pipeline integration | Latest version |\n",
    "\n",
    "**Key Libraries:**\n",
    "- `transformers` - Model loading and inference\n",
    "- `torch` - Deep learning framework\n",
    "- `sentence-transformers` - Embedding generation\n",
    "- `faiss-cpu` - Vector similarity search\n",
    "- `datasets` - Data handling\n",
    "- `ragas` - RAG evaluation metrics\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ—ï¸ System Architecture</b> (Click to expand)</summary>\n",
    "\n",
    "**Complete Left-to-Right Flow:**\n",
    "\n",
    "```\n",
    "USER QUESTION\n",
    "     â”‚\n",
    "     â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚Objective â”‚â”€â”€â”€â–¶â”‚Objective â”‚â”€â”€â”€â–¶â”‚Objective â”‚â”€â”€â”€â–¶â”‚Objective â”‚â”€â”€â”€â–¶â”‚Objective â”‚â”€â”€â”€â–¶â”‚Objective â”‚\n",
    "â”‚    0     â”‚    â”‚    1     â”‚    â”‚    2     â”‚    â”‚    3     â”‚    â”‚    4     â”‚    â”‚    5     â”‚\n",
    "â”‚  Setup   â”‚    â”‚  Mistral â”‚    â”‚  Q&A DB  â”‚    â”‚  FAISS   â”‚    â”‚   RAG    â”‚    â”‚ Evaluate â”‚\n",
    "â”‚          â”‚    â”‚  Model   â”‚    â”‚ 21 pairs â”‚    â”‚  Index   â”‚    â”‚ Pipeline â”‚    â”‚  Models  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
    "                                                                                      â”‚\n",
    "                                                                                      â–¼\n",
    "                                                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                                                                              â”‚Objective â”‚\n",
    "                                                                              â”‚    6     â”‚\n",
    "                                                                              â”‚ Analysis â”‚\n",
    "                                                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**RAG Pipeline Flow (Left-to-Right):**\n",
    "\n",
    "```\n",
    "User Question\n",
    "     â”‚\n",
    "     â”œâ”€â–¶ [Embed] â”€â”€â–¶ [FAISS Search] â”€â”€â–¶ [Retrieve Context] â”€â”€â–¶ [Mistral LLM] â”€â”€â–¶ Final Answer\n",
    "     â”‚\n",
    "     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Objective Mapping:**\n",
    "\n",
    "| Objective | Component | Input | Output | Purpose |\n",
    "|-----------|-----------|-------|--------|---------|\n",
    "| **0: Setup** | Environment | None | `hf_token`, packages | Configure environment and install dependencies |\n",
    "| **1: System Prompt** | Mistral Model | `hf_token` | `mistral_model`, `system_prompt` | Load LLM and create customer service prompt |\n",
    "| **2: Q&A Database** | Knowledge Base | `mistral_model` | `qa_database` (21 pairs) | Generate Q&A pairs for the knowledge base |\n",
    "| **3: FAISS Index** | Vector DB | `qa_database` | `faiss_index`, `embed_query()` | Build semantic search index |\n",
    "| **4: RAG Pipeline** | Complete System | `mistral_model`, `qa_database`, `faiss_index` | `rag_query()`, `search_faiss()` | Create end-to-end RAG system |\n",
    "| **5: Model Evaluation** | Evaluation | `rag_query()`, test questions | `model_evaluations`, rankings | Evaluate 6 QA models using RAGAS |\n",
    "| **6: System Analysis** | Analysis | `model_evaluations`, all components | `system_analysis` report | Analyze limitations and provide recommendations |\n",
    "\n",
    "**Dependency Chain:**\n",
    "```\n",
    "0 (Setup) â†’ 1 (Mistral) â†’ 2 (Q&A DB) â†’ 3 (FAISS) â†’ 4 (RAG) â†’ 5 (Evaluate) â†’ 6 (Analysis)\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ“š Objectives Breakdown</b> (Click to expand)</summary>\n",
    "\n",
    "| Objective | Purpose | Key Deliverable | Estimated Time |\n",
    "|-----------|---------|----------------|----------------|\n",
    "| **0: Setup** | Install packages, configure environment | `hf_token`, environment variables | 1-2 min |\n",
    "| **1: System Prompt** | Design customer service system prompt | `system_prompt`, `mistral_model` | 3-5 min (CPU) / 1-2 min (GPU) |\n",
    "| **2: Q&A Database** | Generate 21 Q&A pairs (18 answerable + 3 unanswerable) | `qa_database` (21 pairs) | 2-3 min |\n",
    "| **3: FAISS Index** | Build vector database for semantic search | `faiss_index`, `embed_query()` | 30 sec |\n",
    "| **4: RAG Pipeline** | Create complete RAG system | `rag_query()`, `search_faiss()` | 30 sec |\n",
    "| **5: Model Evaluation** | Evaluate 6 QA models using RAGAS | `model_evaluations`, rankings | 15-25 min (CPU) / 8-15 min (GPU) |\n",
    "| **6: System Analysis** | Analyze limitations and provide recommendations | `system_analysis` report | 30 sec |\n",
    "\n",
    "**Total Estimated Time:**\n",
    "- **CPU**: ~25-30 minutes\n",
    "- **GPU**: ~15-20 minutes\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>âœ… Requirements Checklist</b> (Click to expand)</summary>\n",
    "\n",
    "**Technical Requirements:**\n",
    "\n",
    "| Requirement | Status | Description |\n",
    "|-------------|--------|-------------|\n",
    "| System Prompt Design | âœ… Required | E-commerce customer service context |\n",
    "| Generate Business Database | âœ… Required | 21 Q&A pairs (18 answerable + 3 unanswerable) |\n",
    "| FAISS Implementation | âœ… Required | Vector database with semantic search |\n",
    "| Test Questions | âœ… Required | 5 answerable + 5 unanswerable questions |\n",
    "| RAG Pipeline | âœ… Required | Complete retrieval-augmented generation system |\n",
    "| Model Experimentation | âœ… Required | 4 required + 2 additional models (6 total) |\n",
    "| Model Ranking | âœ… Required | Rankings with explanations |\n",
    "| Confidence Analysis | âš ï¸ Partial | BERTScore used, detailed analysis recommended |\n",
    "| Reflection Section | âœ… Required | System analysis and recommendations |\n",
    "\n",
    "**Deliverables:**\n",
    "- âœ… Working RAG system\n",
    "- âœ… Model evaluation results\n",
    "- âœ… System analysis report\n",
    "- âœ… CSV files with results\n",
    "- âœ… Well-documented notebook\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ“ Learning Outcomes</b> (Click to expand)</summary>\n",
    "\n",
    "By completing this assignment, I will:\n",
    "\n",
    "1. **Understand RAG Architecture**\n",
    "   - How retrieval and generation work together\n",
    "   - Semantic search vs keyword search\n",
    "   - Context window management\n",
    "\n",
    "2. **Master Vector Databases**\n",
    "   - FAISS indexing and search\n",
    "   - Embedding generation and storage\n",
    "   - Similarity metrics (L2, cosine)\n",
    "\n",
    "3. **Evaluate AI Models**\n",
    "   - Multiple evaluation approaches (RAGAS, LLM-as-Judge)\n",
    "   - Model comparison and ranking\n",
    "   - Confidence score analysis\n",
    "\n",
    "4. **System Design Skills**\n",
    "   - Modular code design (SOLID principles)\n",
    "   - Error handling and validation\n",
    "   - Performance optimization\n",
    "\n",
    "5. **Critical Analysis**\n",
    "   - System limitations identification\n",
    "   - Real-world application assessment\n",
    "   - Improvement recommendations\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ“Š Business Context Options</b> (Click to expand)</summary>\n",
    "\n",
    "I could choose any business context for my RAG system. Examples:\n",
    "\n",
    "| Context | Use Case | Example Questions |\n",
    "|---------|----------|-------------------|\n",
    "| **E-commerce** | Customer service | \"What is your return policy?\", \"How long does shipping take?\" |\n",
    "| **Healthcare** | Patient information | \"What are visiting hours?\", \"Do you accept insurance?\" |\n",
    "| **Education** | Student support | \"When is registration?\", \"What courses are available?\" |\n",
    "| **Finance** | Banking support | \"What are account fees?\", \"How do I transfer money?\" |\n",
    "| **Real Estate** | Property inquiries | \"What properties are available?\", \"What are the prices?\" |\n",
    "\n",
    "**My Assignment Context:** E-commerce Customer Service (GreenTech Marketplace)\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>â±ï¸ Time Management Tips</b> (Click to expand)</summary>\n",
    "\n",
    "**Recommended Workflow:**\n",
    "\n",
    "1. **Setup (5 min)**\n",
    "   - Install packages\n",
    "   - Configure Hugging Face token\n",
    "   - Verify GPU access (if available)\n",
    "\n",
    "2. **Objectives 1-3 (10-15 min)**\n",
    "   - Load Mistral model\n",
    "   - Generate Q&A database\n",
    "   - Build FAISS index\n",
    "\n",
    "3. **Objective 4 (5 min)**\n",
    "   - Create RAG pipeline\n",
    "   - Test with sample questions\n",
    "\n",
    "4. **Objective 5 (15-25 min)**\n",
    "   - Evaluate 6 models\n",
    "   - Generate rankings\n",
    "   - **Longest step - be patient!**\n",
    "\n",
    "5. **Objective 6 (5 min)**\n",
    "   - Analyze system\n",
    "   - Write reflection\n",
    "\n",
    "**Total Time:** ~40-55 minutes\n",
    "\n",
    "**Tips:**\n",
    "- Use GPU for faster model loading and inference\n",
    "- Run Objective 5 overnight if needed (models take time)\n",
    "- Save intermediate results to avoid re-running\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ“ Output Files Generated</b> (Click to expand)</summary>\n",
    "\n",
    "I will generate several output files:\n",
    "\n",
    "| File | Location | Description |\n",
    "|------|----------|-------------|\n",
    "| `qa_database.csv` | `data/qa_database/` | All 21 Q&A pairs with metadata |\n",
    "| `evaluation_results.csv` | `data/qa_database/` | LLM-as-Judge quality scores |\n",
    "| `qa_embeddings.npy` | `data/vector_database/` | Embedding vectors (numpy array) |\n",
    "| `qa_index.faiss` | `data/vector_database/` | FAISS index file (serialized) |\n",
    "| `retrieval_test_results.csv` | `data/vector_database/` | Test query results |\n",
    "| `model_scores.csv` | `data/model_evaluation/` | All model evaluation scores |\n",
    "| `model_responses.csv` | `data/model_evaluation/` | All model responses |\n",
    "| `model_rankings.csv` | `data/model_evaluation/` | Final model rankings |\n",
    "| `system_analysis.txt` | `data/system_analysis/` | Complete analysis report |\n",
    "\n",
    "**All files will be saved automatically during execution.**\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ’¡ Getting Started</b> (Click to expand)</summary>\n",
    "\n",
    "**Step 1: Environment Setup**\n",
    "1. I'll open Google Colab (recommended) or Jupyter Notebook\n",
    "2. Enable GPU runtime (Colab: Runtime â†’ Change runtime type â†’ GPU)\n",
    "3. Get Hugging Face token from [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
    "\n",
    "**Step 2: Run Setup Cell**\n",
    "- Execute Objective 0 (Setup & Prerequisites)\n",
    "- Install required packages\n",
    "- Authenticate with Hugging Face\n",
    "\n",
    "**Step 3: Execute Objectives Sequentially**\n",
    "- Run objectives in order (0 â†’ 1 â†’ 2 â†’ 3 â†’ 4 â†’ 5 â†’ 6)\n",
    "- Each objective builds on the previous one\n",
    "- Don't skip ahead!\n",
    "\n",
    "**Step 4: Review Results**\n",
    "- Check generated CSV files\n",
    "- Review model rankings\n",
    "- Read system analysis report\n",
    "\n",
    "**If I Encounter Issues:**\n",
    "- Check error messages carefully\n",
    "- Verify prerequisites are met\n",
    "- Ensure GPU is enabled for faster execution\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to begin?** I'll start with **Objective 0: Setup & Prerequisites** to configure my environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## ğŸ“– Personal Notes\n",
    "\n",
    "### ğŸ“š Learning Resources\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ“„ Reference Documentation</b> (Click to expand)</summary>\n",
    "\n",
    "For detailed explanations of key concepts, I refer to:\n",
    "\n",
    "**ğŸ“„ `learning-path/W02/03-learnings/langchain-faiss-rag-embeddings.md`**\n",
    "\n",
    "This comprehensive reference file contains detailed explanations of:\n",
    "\n",
    "| Topic | What I Learned |\n",
    "|-------|----------------|\n",
    "| **ğŸ¦œ LangChain Framework** | How to build AI systems with modular components, chain operations, and integrate with various LLMs |\n",
    "| **ğŸ” FAISS Vector Database** | Similarity search algorithms, indexing strategies, and performance optimization |\n",
    "| **ğŸ”„ RAG Architecture** | How retrieval and generation work together, context window management, and pipeline design |\n",
    "| **ğŸ§® Embeddings & Vectors** | Semantic understanding, vector representations, and similarity metrics |\n",
    "| **ğŸ”— Component Integration** | How LangChain, FAISS, and embeddings work together in a complete RAG pipeline |\n",
    "| **ğŸ’¡ Best Practices** | Getting started tips, common pitfalls, and implementation strategies |\n",
    "\n",
    "**Key Takeaways:**\n",
    "- RAG combines the accuracy of retrieval with the fluency of generation\n",
    "- FAISS enables fast semantic search at scale\n",
    "- Embeddings capture meaning, not just keywords\n",
    "- Proper context formatting is crucial for good answers\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ’­ My Learning Journey</b> (Click to expand)</summary>\n",
    "\n",
    "**What I'm Learning:**\n",
    "\n",
    "1. **RAG System Design**\n",
    "   - How to structure a complete RAG pipeline\n",
    "   - Balancing retrieval quality with generation quality\n",
    "   - Context window management strategies\n",
    "\n",
    "2. **Vector Databases**\n",
    "   - FAISS indexing and search mechanisms\n",
    "   - Embedding model selection and optimization\n",
    "   - Similarity metrics and their trade-offs\n",
    "\n",
    "3. **Model Evaluation**\n",
    "   - Multiple evaluation approaches (RAGAS, library-based)\n",
    "   - Interpreting evaluation metrics\n",
    "   - Model comparison and selection\n",
    "\n",
    "4. **System Analysis**\n",
    "   - Identifying limitations and edge cases\n",
    "   - Scalability considerations\n",
    "   - Real-world deployment challenges\n",
    "\n",
    "**Challenges I Encountered:**\n",
    "- Understanding embedding dimensions and their impact\n",
    "- Balancing model size vs. performance\n",
    "- Interpreting evaluation metrics correctly\n",
    "- Optimizing retrieval for better context quality\n",
    "\n",
    "**Solutions I Found:**\n",
    "- Using pre-trained embedding models (sentence-transformers)\n",
    "- Implementing proper error handling\n",
    "- Testing with both answerable and unanswerable questions\n",
    "- Documenting design choices for clarity\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ”§ Implementation Notes</b> (Click to expand)</summary>\n",
    "\n",
    "**Design Decisions I Made:**\n",
    "\n",
    "| Decision | Choice | Rationale |\n",
    "|----------|--------|-----------|\n",
    "| **Embedding Model** | `all-MiniLM-L6-v2` | Fast, 384 dims, good quality for small datasets |\n",
    "| **FAISS Index Type** | `IndexFlatL2` | Exact search, best for <100k vectors |\n",
    "| **Top-K Retrieval** | 3 documents | Balance between context and noise |\n",
    "| **Business Context** | E-commerce | Rich domain with clear policies |\n",
    "| **Q&A Count** | 21 pairs | Sufficient for demo, covers key topics |\n",
    "| **Evaluation Approach** | RAGAS + Library | Multiple perspectives for comprehensive evaluation |\n",
    "\n",
    "**Code Organization:**\n",
    "- Modular functions following SOLID principles\n",
    "- Clear separation of concerns\n",
    "- Reusable components\n",
    "- Comprehensive error handling\n",
    "\n",
    "**Performance Optimizations:**\n",
    "- Model caching to avoid redundant loads\n",
    "- Batch processing where possible\n",
    "- GPU utilization when available\n",
    "- Efficient data structures\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ“ Key Concepts Reference</b> (Click to expand)</summary>\n",
    "\n",
    "**Quick Reference for Key Terms:**\n",
    "\n",
    "| Term | Definition | Example |\n",
    "|------|------------|--------|\n",
    "| **RAG** | Retrieval-Augmented Generation - combines search with LLM | Search KB â†’ Get context â†’ Generate answer |\n",
    "| **Embedding** | Vector representation of text capturing semantic meaning | \"shipping\" â†’ [0.12, -0.45, 0.78, ...] |\n",
    "| **FAISS** | Facebook AI Similarity Search - fast vector search library | Find similar documents in milliseconds |\n",
    "| **Semantic Search** | Finding content by meaning, not just keywords | \"delivery time\" matches \"shipping duration\" |\n",
    "| **Context Window** | Text provided to LLM along with the question | Retrieved Q&A pairs formatted as context |\n",
    "| **Top-K Retrieval** | Get K most similar documents from index | Top 3 most relevant Q&A pairs |\n",
    "| **RAGAS** | RAG Assessment - framework for evaluating RAG systems | Metrics: faithfulness, answer relevance |\n",
    "\n",
    "**How They Work Together:**\n",
    "1. **Embedding** converts text to vectors\n",
    "2. **FAISS** searches for similar vectors\n",
    "3. **Semantic Search** finds relevant content by meaning\n",
    "4. **Context Window** provides retrieved info to LLM\n",
    "5. **RAG** combines retrieval + generation\n",
    "6. **RAGAS** evaluates the complete system\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ“ Assignment Insights</b> (Click to expand)</summary>\n",
    "\n",
    "**What This Assignment Teaches Me:**\n",
    "\n",
    "1. **End-to-End System Building**\n",
    "   - From data generation to deployment considerations\n",
    "   - How components integrate in a production system\n",
    "   - Trade-offs between different approaches\n",
    "\n",
    "2. **Evaluation is Critical**\n",
    "   - Multiple metrics provide different perspectives\n",
    "   - Both answerable and unanswerable questions are important\n",
    "   - Model selection requires careful analysis\n",
    "\n",
    "3. **Documentation Matters**\n",
    "   - Clear documentation helps understand design choices\n",
    "   - Well-structured code is easier to maintain\n",
    "   - Reflection reveals improvement opportunities\n",
    "\n",
    "4. **Real-World Considerations**\n",
    "   - Scalability planning is essential\n",
    "   - Edge cases must be handled gracefully\n",
    "   - User experience depends on system design\n",
    "\n",
    "**Skills I'm Developing:**\n",
    "- System architecture design\n",
    "- Model evaluation and comparison\n",
    "- Critical analysis and reflection\n",
    "- Technical documentation\n",
    "- Problem-solving and debugging\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ”— Additional Resources</b> (Click to expand)</summary>\n",
    "\n",
    "**Documentation I Referenced:**\n",
    "\n",
    "1. **LangChain & FAISS Guide**\n",
    "   - Location: `learning-path/W02/03-learnings/langchain-faiss-rag-embeddings.md`\n",
    "   - Covers: LangChain, FAISS, RAG, embeddings concepts\n",
    "\n",
    "2. **Hugging Face Documentation**\n",
    "   - Models: [huggingface.co/models](https://huggingface.co/models)\n",
    "   - Transformers: [huggingface.co/docs/transformers](https://huggingface.co/docs/transformers)\n",
    "\n",
    "3. **FAISS Documentation**\n",
    "   - GitHub: [github.com/facebookresearch/faiss](https://github.com/facebookresearch/faiss)\n",
    "   - Index types and performance guides\n",
    "\n",
    "4. **RAGAS Documentation**\n",
    "   - Framework: [docs.ragas.io](https://docs.ragas.io)\n",
    "   - Evaluation metrics and best practices\n",
    "\n",
    "**Useful Commands:**\n",
    "```python\n",
    "# Check GPU\n",
    "import torch\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Check installed packages\n",
    "!pip list | grep -E \"transformers|faiss|ragas|langchain\"\n",
    "\n",
    "# Verify token\n",
    "import os\n",
    "print(f\"HF Token: {'Set' if os.getenv('HUGGINGFACE_HUB_TOKEN') else 'Not set'}\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective 0: Setup & Prerequisites\n",
    "\n",
    "### ğŸ¯ Goal\n",
    "I will set up my environment with all required packages, configure authentication, and verify system capabilities before starting the RAG system implementation.\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ“‹ Prerequisites Checklist</b> (Click to expand)</summary>\n",
    "\n",
    "**Knowledge Prerequisites:**\n",
    "\n",
    "| Requirement | Level | Description |\n",
    "|-------------|-------|-------------|\n",
    "| **Python Programming** | Basic | Variables, functions, data structures, imports |\n",
    "| **Jupyter Notebooks** | Basic | Running cells, markdown formatting, code execution |\n",
    "| **Machine Learning Concepts** | High-level | Neural networks, transformers, embeddings, model inference |\n",
    "| **Evaluation Metrics** | Basic | Understanding of accuracy, confidence scores |\n",
    "\n",
    "**Technical Prerequisites:**\n",
    "\n",
    "| Item | Required | How to Get |\n",
    "|------|----------|------------|\n",
    "| **Hugging Face Account** | âœ… Yes | Sign up at [huggingface.co](https://huggingface.co) |\n",
    "| **Hugging Face Token** | âœ… Yes | Get from [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens) |\n",
    "| **Google Colab Account** | âš ï¸ Recommended | Free account at [colab.research.google.com](https://colab.research.google.com) |\n",
    "| **GPU Access** | âš ï¸ Optional | Colab provides free GPU, or local GPU setup |\n",
    "| **Python 3.8+** | âœ… Yes | Pre-installed in Colab, or install locally |\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ“¦ Required Packages</b> (Click to expand)</summary>\n",
    "\n",
    "**Core Packages:**\n",
    "\n",
    "| Package | Purpose | Version |\n",
    "|---------|---------|---------|\n",
    "| `transformers` | Load and use Hugging Face models (Mistral, QA models) | Latest |\n",
    "| `torch` | Deep learning framework for model inference | Latest |\n",
    "| `sentence-transformers` | Generate embeddings for semantic search | Latest |\n",
    "| `faiss-cpu` | Vector similarity search library | Latest |\n",
    "| `datasets` | Data handling and processing | Latest |\n",
    "| `accelerate` | Model loading optimization | Latest |\n",
    "\n",
    "**Evaluation & Integration:**\n",
    "\n",
    "| Package | Purpose | Version |\n",
    "|---------|---------|---------|\n",
    "| `ragas` | RAG Assessment framework for evaluation | Latest |\n",
    "| `langchain` | LLM pipeline integration | Latest |\n",
    "| `langchain-community` | Community integrations | Latest |\n",
    "| `langchain-huggingface` | Hugging Face integration for LangChain | Latest |\n",
    "| `nest_asyncio` | Async support for RAGAS | Latest |\n",
    "\n",
    "**Installation Command:**\n",
    "```python\n",
    "!pip install transformers torch sentence-transformers faiss-cpu datasets accelerate ragas langchain langchain-community langchain-huggingface nest_asyncio\n",
    "```\n",
    "\n",
    "**Note:** All packages will be installed automatically by the setup code cell.\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ–¥ï¸ Environment Setup</b> (Click to expand)</summary>\n",
    "\n",
    "**Option 1: Google Colab (Recommended)**\n",
    "\n",
    "**Advantages:**\n",
    "- âœ… Free GPU access (T4 GPU)\n",
    "- âœ… No local installation needed\n",
    "- âœ… Pre-configured environment\n",
    "- âœ… Easy sharing and collaboration\n",
    "\n",
    "**Setup Steps:**\n",
    "1. Go to [colab.research.google.com](https://colab.research.google.com)\n",
    "2. Create a new notebook\n",
    "3. Enable GPU: **Runtime â†’ Change runtime type â†’ GPU â†’ Save**\n",
    "4. Upload or create the notebook file\n",
    "5. Run the setup cell\n",
    "\n",
    "**Option 2: Local Jupyter Notebook**\n",
    "\n",
    "**Requirements:**\n",
    "- Python 3.8 or higher\n",
    "- Jupyter Notebook installed\n",
    "- GPU optional (CPU works but slower)\n",
    "\n",
    "**Setup Steps:**\n",
    "1. Install Python 3.8+\n",
    "2. Install Jupyter: `pip install jupyter`\n",
    "3. Install required packages (see above)\n",
    "4. Launch: `jupyter notebook`\n",
    "5. Open the notebook file\n",
    "\n",
    "**GPU Setup (Optional but Recommended):**\n",
    "\n",
    "| Platform | GPU Type | Setup |\n",
    "|----------|----------|-------|\n",
    "| **Colab** | T4 (Free) | Runtime â†’ Change runtime type â†’ GPU |\n",
    "| **Local** | NVIDIA GPU | Install CUDA toolkit, PyTorch with CUDA |\n",
    "| **CPU Only** | N/A | Works but 2-3x slower |\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ”‘ Hugging Face Authentication</b> (Click to expand)</summary>\n",
    "\n",
    "**Step 1: Get Your Token**\n",
    "\n",
    "1. Go to [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
    "2. Click **\"New token\"**\n",
    "3. Name it (e.g., \"RAG Assignment\")\n",
    "4. Select **\"Read\"** access (sufficient for this assignment)\n",
    "5. Click **\"Generate token\"**\n",
    "6. **Copy the token immediately** (you won't see it again!)\n",
    "\n",
    "**Step 2: Store Your Token**\n",
    "\n",
    "**In Google Colab:**\n",
    "```python\n",
    "from google.colab import userdata\n",
    "userdata.set('HUGGINGFACE_HUB_TOKEN', 'your_token_here')\n",
    "```\n",
    "\n",
    "**Or use Colab Secrets:**\n",
    "1. Click the ğŸ”‘ icon in the left sidebar\n",
    "2. Add secret: `HUGGINGFACE_HUB_TOKEN` = `your_token_here`\n",
    "\n",
    "**Locally (Environment Variable):**\n",
    "```bash\n",
    "export HUGGINGFACE_HUB_TOKEN=your_token_here\n",
    "```\n",
    "\n",
    "**Or create `.env` file:**\n",
    "```\n",
    "HUGGINGFACE_HUB_TOKEN=your_token_here\n",
    "```\n",
    "\n",
    "**Step 3: Verify Authentication**\n",
    "\n",
    "The setup code will automatically:\n",
    "- Try Colab userdata first\n",
    "- Try environment variables\n",
    "- Prompt for manual input if needed\n",
    "- Authenticate with Hugging Face Hub\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>âœ… Setup Verification</b> (Click to expand)</summary>\n",
    "\n",
    "After running the setup cell, verify these are set:\n",
    "\n",
    "**Environment Variables:**\n",
    "- âœ… `IN_COLAB` - True if running in Colab\n",
    "- âœ… `HAS_GPU` - True if GPU is available\n",
    "- âœ… `hf_token` - Your Hugging Face token\n",
    "\n",
    "**Verification Code:**\n",
    "```python\n",
    "# Check setup\n",
    "print(\"ğŸ” Setup Verification:\")\n",
    "print(f\"   IN_COLAB: {IN_COLAB}\")\n",
    "print(f\"   HAS_GPU: {HAS_GPU}\")\n",
    "print(f\"   hf_token: {'âœ… Set' if hf_token else 'âŒ Not set'}\")\n",
    "\n",
    "if HAS_GPU:\n",
    "    import torch\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "```\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "ğŸ” Setup Verification:\n",
    "   IN_COLAB: True\n",
    "   HAS_GPU: True\n",
    "   hf_token: âœ… Set\n",
    "   GPU: Tesla T4\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>âš™ï¸ Setup Functions</b> (Click to expand)</summary>\n",
    "\n",
    "The setup code provides modular functions following SOLID principles:\n",
    "\n",
    "**Available Functions:**\n",
    "\n",
    "| Function | Purpose | Returns |\n",
    "|----------|---------|---------|\n",
    "| `check_environment()` | Detect Colab and GPU availability | `(is_colab, has_gpu)` |\n",
    "| `get_hf_token()` | Retrieve Hugging Face token from various sources | `str` (token) |\n",
    "| `install_packages()` | Install required packages if missing | None |\n",
    "| `import_libraries()` | Import all required libraries with error handling | `bool` (success) |\n",
    "| `authenticate_hf(token)` | Authenticate with Hugging Face Hub | `bool` (success) |\n",
    "\n",
    "**Design Principles:**\n",
    "- **KISS** (Keep It Simple, Stupid) - Each function has single responsibility\n",
    "- **DRY** (Don't Repeat Yourself) - Reusable functions\n",
    "- **SOLID** - Modular, extensible design\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸš€ Quick Start Guide</b> (Click to expand)</summary>\n",
    "\n",
    "**Step-by-Step Setup:**\n",
    "\n",
    "1. **Open Environment**\n",
    "   - Colab: Create new notebook\n",
    "   - Local: Launch Jupyter Notebook\n",
    "\n",
    "2. **Enable GPU (Recommended)**\n",
    "   - Colab: Runtime â†’ Change runtime type â†’ GPU\n",
    "   - Local: Ensure CUDA is installed\n",
    "\n",
    "3. **Run Setup Cell**\n",
    "   - Execute the \"Prerequisites & Setup\" code cell\n",
    "   - Wait for packages to install (~2-3 minutes first time)\n",
    "\n",
    "4. **Authenticate**\n",
    "   - Enter Hugging Face token when prompted\n",
    "   - Or set it in Colab secrets/environment variable\n",
    "\n",
    "5. **Verify**\n",
    "   - Check console output for âœ… marks\n",
    "   - Verify `hf_token` is set\n",
    "   - Confirm GPU is detected (if available)\n",
    "\n",
    "**Expected Setup Time:**\n",
    "- First run: 2-3 minutes (package installation)\n",
    "- Subsequent runs: <30 seconds (packages cached)\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>âš ï¸ Troubleshooting</b> (Click to expand)</summary>\n",
    "\n",
    "**Common Issues:**\n",
    "\n",
    "| Issue | Solution |\n",
    "|-------|----------|\n",
    "| **Token not found** | Check Colab secrets or environment variables. Re-enter token manually. |\n",
    "| **GPU not detected** | In Colab: Runtime â†’ Change runtime type â†’ GPU. Local: Install CUDA toolkit. |\n",
    "| **Package installation fails** | Restart runtime/kernel and try again. Check internet connection. |\n",
    "| **Import errors** | Run `pip install --upgrade <package>` for the failing package. |\n",
    "| **Out of memory** | Use CPU instead of GPU, or restart runtime to clear memory. |\n",
    "\n",
    "**Getting Help:**\n",
    "- Check error messages carefully - they usually indicate the issue\n",
    "- Verify all prerequisites are met\n",
    "- Ensure internet connection is stable\n",
    "- Restart runtime/kernel if issues persist\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ“Š System Requirements</b> (Click to expand)</summary>\n",
    "\n",
    "**Minimum Requirements (CPU):**\n",
    "- Python 3.8+\n",
    "- 8GB RAM\n",
    "- 20GB free disk space (for model downloads)\n",
    "- Stable internet connection\n",
    "\n",
    "**Recommended (GPU):**\n",
    "- Python 3.8+\n",
    "- 16GB+ RAM\n",
    "- NVIDIA GPU with 8GB+ VRAM\n",
    "- 30GB+ free disk space\n",
    "- Fast internet connection\n",
    "\n",
    "**Colab Specifications:**\n",
    "- **Free Tier:** T4 GPU (16GB VRAM), 12GB RAM\n",
    "- **Pro Tier:** V100/A100 GPU, more RAM\n",
    "- **Storage:** 15GB free space\n",
    "\n",
    "**Model Download Sizes:**\n",
    "- Mistral-7B: ~14GB (first download)\n",
    "- QA Models: ~500MB - 3GB each\n",
    "- Embedding Model: ~90MB\n",
    "- **Total:** ~20-25GB for all models\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Next Step:** After setup is complete, proceed to **Objective 1: Design System Prompts** to load the Mistral model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Prerequisites & Setup - Modular Functions\n",
    "# ============================================================================\n",
    "# This cell contains reusable setup functions following KISS, DRY, and SOLID principles\n",
    "# Run this cell FIRST before any other cells\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Function 1: Check Environment (KISS - Single Responsibility)\n",
    "# ----------------------------------------------------------------------------\n",
    "def check_environment():\n",
    "    \"\"\"Check if running in Colab and detect GPU availability.\"\"\"\n",
    "    print(\"ğŸ” Checking environment...\")\n",
    "    print(f\"   Python version: {sys.version.split()[0]}\")\n",
    "    \n",
    "    # Check Colab\n",
    "    try:\n",
    "        import google.colab\n",
    "        is_colab = True\n",
    "        print(\"   âœ… Running in Google Colab\")\n",
    "    except ImportError:\n",
    "        is_colab = False\n",
    "        print(\"   âš ï¸  Not running in Google Colab (local environment)\")\n",
    "    \n",
    "    # Check GPU\n",
    "    try:\n",
    "        import torch\n",
    "        has_gpu = torch.cuda.is_available()\n",
    "        if has_gpu:\n",
    "            print(f\"   âœ… GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"   âœ… CUDA Version: {torch.version.cuda}\")\n",
    "        else:\n",
    "            print(\"   âš ï¸  GPU NOT detected\")\n",
    "            if is_colab:\n",
    "                print(\"   ğŸ’¡ TIP: Runtime â†’ Change runtime type â†’ Select GPU â†’ Save\")\n",
    "    except ImportError:\n",
    "        has_gpu = False\n",
    "        print(\"   âš ï¸  PyTorch not installed yet\")\n",
    "    \n",
    "    return is_colab, has_gpu\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Function 2: Get Hugging Face Token (DRY - Don't Repeat Yourself)\n",
    "# ----------------------------------------------------------------------------\n",
    "def get_hf_token():\n",
    "    \"\"\"Get Hugging Face token from Colab userdata or environment variable.\"\"\"\n",
    "    # Try Colab userdata first\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        token = userdata.get('HUGGINGFACE_HUB_TOKEN')\n",
    "        if token:\n",
    "            print(\"âœ… Hugging Face token loaded from Colab userdata!\")\n",
    "            print(f\"   Token preview: {token[:10]}...{token[-4:] if len(token) > 14 else '****'}\")\n",
    "            return token\n",
    "    except (ImportError, ValueError):\n",
    "        pass\n",
    "    \n",
    "    # Try environment variable\n",
    "    try:\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv()\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    token = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "    if token:\n",
    "        print(\"âœ… Hugging Face token loaded from environment!\")\n",
    "        print(f\"   Token preview: {token[:10]}...{token[-4:] if len(token) > 14 else '****'}\")\n",
    "        return token\n",
    "    \n",
    "    # No token found\n",
    "    print(\"âŒ Hugging Face token not found!\")\n",
    "    print(\"   Get your token from: https://huggingface.co/settings/tokens\")\n",
    "    print(\"\\n   In Colab: userdata.set('HUGGINGFACE_HUB_TOKEN', 'your_token')\")\n",
    "    print(\"   Locally: export HUGGINGFACE_HUB_TOKEN=your_token\")\n",
    "    print(\"\\nâš ï¸  Some models may require authentication!\")\n",
    "    return None\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Function 3: Install Packages (SOLID - Open/Closed Principle)\n",
    "# ----------------------------------------------------------------------------\n",
    "def install_packages():\n",
    "    \"\"\"Install required packages if not already installed.\"\"\"\n",
    "    packages = [\n",
    "        \"transformers\",\n",
    "        \"torch\",\n",
    "        \"sentence-transformers\",\n",
    "        \"datasets\",\n",
    "        \"python-dotenv\",\n",
    "        \"faiss-cpu\",\n",
    "        \"ragas\"\n",
    "    ]\n",
    "    \n",
    "    print(\"ğŸ“¦ Installing required packages...\")\n",
    "    for package in packages:\n",
    "        try:\n",
    "            __import__(package.replace(\"-\", \"_\"))\n",
    "            print(f\"   âœ… {package} already installed\")\n",
    "        except ImportError:\n",
    "            print(f\"   â³ Installing {package}...\")\n",
    "            os.system(f\"pip install -q {package}\")\n",
    "            print(f\"   âœ… {package} installed\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Function 4: Import Libraries (KISS - Keep It Simple)\n",
    "# ----------------------------------------------------------------------------\n",
    "def import_libraries():\n",
    "    \"\"\"Import all required libraries with error handling.\"\"\"\n",
    "    try:\n",
    "        from transformers import (\n",
    "            pipeline, \n",
    "            AutoModelForCausalLM, \n",
    "            AutoTokenizer, \n",
    "            logging as transformers_logging\n",
    "        )\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        import torch\n",
    "        import numpy as np\n",
    "        import faiss\n",
    "        \n",
    "        transformers_logging.set_verbosity_error()\n",
    "        \n",
    "        print(\"âœ… All required libraries imported successfully!\")\n",
    "        return True\n",
    "    except ImportError as e:\n",
    "        print(f\"âŒ Import error: {e}\")\n",
    "        print(\"   Run: pip install transformers torch sentence-transformers faiss-cpu\")\n",
    "        return False\n",
    "    except RuntimeError as e:\n",
    "        if \"register_fake\" in str(e) or \"torch.library\" in str(e):\n",
    "            print(\"âŒ Dependency version mismatch!\")\n",
    "            print(\"   Fix: pip install --upgrade torch torchvision\")\n",
    "            print(\"   Then restart kernel and run this cell again.\")\n",
    "        return False\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Function 5: Authenticate Hugging Face (DRY - Reusable)\n",
    "# ----------------------------------------------------------------------------\n",
    "def authenticate_hf(token=None):\n",
    "    \"\"\"Authenticate with Hugging Face using provided token or global token.\"\"\"\n",
    "    if token is None:\n",
    "        token = globals().get('hf_token')\n",
    "    \n",
    "    if not token:\n",
    "        print(\"âš ï¸  No token provided, skipping authentication\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        from huggingface_hub import login\n",
    "        login(token=token)\n",
    "        print(\"âœ… Authenticated with Hugging Face\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Authentication failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Main Setup Execution\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"=\" * 80)\n",
    "print(\"PREREQUISITES & SETUP\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Step 1: Check environment\n",
    "IN_COLAB, HAS_GPU = check_environment()\n",
    "globals()['IN_COLAB'] = IN_COLAB\n",
    "globals()['HAS_GPU'] = HAS_GPU\n",
    "print()\n",
    "\n",
    "# Step 2: Get token\n",
    "hf_token = get_hf_token()\n",
    "globals()['hf_token'] = hf_token\n",
    "print()\n",
    "\n",
    "# Step 3: Install packages\n",
    "install_packages()\n",
    "print()\n",
    "\n",
    "# Step 4: Import libraries\n",
    "if import_libraries():\n",
    "    # Step 5: Authenticate if token available\n",
    "    if hf_token:\n",
    "        authenticate_hf(hf_token)\n",
    "    \n",
    "    print()\n",
    "    print(\"=\" * 80)\n",
    "    print(\"âœ… PREREQUISITES & SETUP COMPLETED!\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    print(\"ğŸ“Œ Available Globals:\")\n",
    "    print(f\"   - IN_COLAB: {IN_COLAB}\")\n",
    "    print(f\"   - HAS_GPU: {HAS_GPU}\")\n",
    "    print(f\"   - hf_token: {'Set' if hf_token else 'Not set'}\")\n",
    "    print()\n",
    "    print(\"ğŸ’¡ You can now use these functions in other cells:\")\n",
    "    print(\"   - check_environment()\")\n",
    "    print(\"   - get_hf_token()\")\n",
    "    print(\"   - authenticate_hf(token)\")\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    print(\"âŒ Setup incomplete. Please fix errors above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective 1: Design System Prompts\n",
    "\n",
    "### ğŸ¯ Goal\n",
    "Create a system prompt that defines Mistral's role as a customer service assistant for an e-commerce business context, enabling consistent, context-aware responses aligned with business requirements.\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ“¥ Prerequisites</b> (Click to expand)</summary>\n",
    "\n",
    "| Item | Source | Required | Description |\n",
    "|------|--------|----------|-------------|\n",
    "| `hf_token` | Setup cell (Objective 0) | âœ… Yes | Hugging Face API token for model access |\n",
    "| GPU (recommended) | Colab settings | âš ï¸ Optional | Faster model loading (~1-2 min vs 3-5 min on CPU) |\n",
    "| Python packages | Setup cell | âœ… Yes | `transformers`, `torch`, `huggingface_hub` |\n",
    "\n",
    "**Note:** If running locally without GPU, model loading will be slower but fully functional.\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ”§ Core Concepts</b> (Click to expand)</summary>\n",
    "\n",
    "| Concept | Description |\n",
    "|--------|-------------|\n",
    "| **System Prompt** | Instructions that define the AI's role, personality, constraints, and knowledge boundaries. Acts as the \"constitution\" for LLM behavior. |\n",
    "| **Mistral-7B-Instruct** | Open-source 7 billion parameter model, instruction-tuned for following prompts and generating structured responses. |\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ“Š Design Choices</b> (Click to expand)</summary>\n",
    "\n",
    "| Choice | Selected | Rationale |\n",
    "|--------|----------|-----------|\n",
    "| **Model** | Mistral-7B-Instruct-v0.3 | Open-source, instruction-tuned, good quality, no API costs |\n",
    "| **Precision** | float16 (GPU) / float32 (CPU) | GPU: Memory efficient. CPU: Avoids numerical issues |\n",
    "| **Business Context** | E-commerce Customer Service | Rich domain with clear policies, procedures, and common customer inquiries |\n",
    "| **Prompt Structure** | Multi-section with role, knowledge base, guidelines | Ensures comprehensive coverage of all customer service aspects |\n",
    "\n",
    "**Why This Approach:**\n",
    "Well-defined system prompts ensure consistent, context-aware responses aligned with business requirements. The e-commerce context provides diverse Q&A pairs covering products, shipping, returns, support, etc.\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ”„ Process Flow</b> (Click to expand)</summary>\n",
    "\n",
    "**Left-to-Right Flow:**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Authenticateâ”‚â”€â”€â”€â–¶â”‚  Load        â”‚â”€â”€â”€â–¶â”‚  Create      â”‚â”€â”€â”€â–¶â”‚  Test        â”‚â”€â”€â”€â–¶â”‚  Save        â”‚\n",
    "â”‚  with HF     â”‚    â”‚  Mistral     â”‚    â”‚  System      â”‚    â”‚  Generation  â”‚    â”‚  Files       â”‚\n",
    "â”‚              â”‚    â”‚  Model       â”‚    â”‚  Prompt      â”‚    â”‚              â”‚    â”‚              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Detailed Steps:**\n",
    "1. **Authenticate**: Login to Hugging Face with API token\n",
    "2. **Load Model**: Download and load Mistral-7B-Instruct (first run: ~14GB download)\n",
    "3. **Create System Prompt**: Define role, business context, communication guidelines\n",
    "4. **Test Generation**: Generate sample response to verify prompt effectiveness\n",
    "5. **Save Files**: Persist system prompt and test response to disk\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ“¤ Outputs</b> (Click to expand)</summary>\n",
    "\n",
    "| Variable | Type | Description |\n",
    "|----------|------|-------------|\n",
    "| `mistral_model` | `AutoModelForCausalLM` | Loaded Mistral-7B model |\n",
    "| `mistral_tokenizer` | `AutoTokenizer` | Model tokenizer for text processing and encoding |\n",
    "| `system_prompt` | `str` | Customer service system prompt with e-commerce context (~2000 chars) |\n",
    "\n",
    "**Files Created:**\n",
    "| File | Location | Description |\n",
    "|------|----------|-------------|\n",
    "| `system_prompt.txt` | `data/system_prompt_engineering/` | Full system prompt text |\n",
    "| `test_response.txt` | `data/system_prompt_engineering/` | Generated response to test question |\n",
    "\n",
    "**Key Functions Created:**\n",
    "- `load_mistral_model()`: Loads and caches Mistral model\n",
    "- `create_system_prompt()`: Generates optimized system prompt\n",
    "- `format_mistral_prompt()`: Formats prompt for Mistral Instruct template\n",
    "- `generate_response()`: Generates text using Mistral with system prompt\n",
    "- `save_system_prompt()`: Saves system prompt to file\n",
    "- `save_response()`: Saves generated response to file\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ“‹ System Prompt Components</b> (Click to expand)</summary>\n",
    "\n",
    "The system prompt includes:\n",
    "\n",
    "| Component | Content |\n",
    "|-----------|---------|\n",
    "| **Role Definition** | Customer service assistant for e-commerce platform |\n",
    "| **Business Context** | Company name, product categories, policies |\n",
    "| **Knowledge Base** | Products, shipping, returns, customer service hours, contact methods |\n",
    "| **Communication Guidelines** | Tone (warm, professional), style (clear, concise) |\n",
    "| **Limitation Handling** | Instructions for unknown information (\"I don't have that information\") |\n",
    "| **Response Format** | Structure expectations for consistent outputs |\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>â–¶ï¸ How to Run</b> (Click to expand)</summary>\n",
    "\n",
    "1. **Ensure Setup Complete**: \n",
    "   - Run Objective 0 (Setup) first\n",
    "   - Verify `hf_token` is set\n",
    "   - Check GPU availability (optional but recommended)\n",
    "\n",
    "2. **Run the Code Cell**: \n",
    "   - Execute the Objective 1 code cell\n",
    "   - First run will download ~14GB model (takes 5-10 minutes)\n",
    "   - Subsequent runs use cached model (instant)\n",
    "\n",
    "3. **Verify Output**: \n",
    "   - Check console for: \"âœ… Loaded on GPU (CUDA)\" or \"âœ… Loaded on CPU\"\n",
    "   - Verify system prompt is created\n",
    "   - Test response should be generated\n",
    "   - Files saved to `data/system_prompt_engineering/`\n",
    "\n",
    "**Expected Runtime:**\n",
    "- GPU: 1-2 minutes (after initial download)\n",
    "- CPU: 3-5 minutes (after initial download)\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>âœ… Verification</b> (Click to expand)</summary>\n",
    "\n",
    "After running, verify these exist:\n",
    "\n",
    "```python\n",
    "verify_objective1()\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ”— Dependencies</b> (Click to expand)</summary>\n",
    "\n",
    "**This Objective:**\n",
    "- âœ… Requires Objective 0 (Setup) for `hf_token`\n",
    "\n",
    "**Used By:**\n",
    "- Objective 2: Generates Q&A pairs using `mistral_model` and `system_prompt`\n",
    "- Objective 4: Uses `mistral_model` for RAG answer generation\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ“š Learning Objectives Demonstrated</b> (Click to expand)</summary>\n",
    "\n",
    "1. **System Prompt Engineering**: Crafting prompts that shape LLM behavior\n",
    "2. **Modular Design**: SOLID, KISS, DRY principles in practice\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ’¡ Tips</b> (Click to expand)</summary>\n",
    "\n",
    "- **First Run**: Be patient - model download is large (~14GB)\n",
    "- **GPU Recommended**: Significantly faster inference (2-3x speedup)\n",
    "- **Prompt Tuning**: Experiment with different prompt structures to optimize responses\n",
    "- **Caching**: Model is cached globally - subsequent runs are instant\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Next Step:** Proceed to Objective 2 to generate Q&A database using the system prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OBJECTIVE 1: DESIGN SYSTEM PROMPTS FOR LLM-BASED CUSTOMER SERVICE\n",
    "# ============================================================================\n",
    "#\n",
    "# LEARNING OBJECTIVES DEMONSTRATED:\n",
    "#   1. System Prompt Engineering - Crafting prompts that shape LLM behavior\n",
    "#   2. Modular Design - SOLID, KISS, DRY principles in practice\n",
    "#\n",
    "# THEORETICAL BACKGROUND:\n",
    "#   System prompts serve as the \"constitution\" for LLM behavior, establishing:\n",
    "#   - Role Identity: Who the model should act as\n",
    "#   - Knowledge Boundaries: What the model knows and doesn't know\n",
    "#   - Behavioral Constraints: Response style, escalation rules\n",
    "#   - Domain Context: Business-specific information\n",
    "#\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 1: IMPORTS & ENVIRONMENT VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    from huggingface_hub import login\n",
    "except ImportError as e:\n",
    "    raise ImportError(f\"Missing: {e}. Run: pip install torch transformers huggingface_hub\")\n",
    "\n",
    "if 'authenticate_hf' not in globals():\n",
    "    def authenticate_hf(token=None):\n",
    "        \"\"\"Authenticate with Hugging Face Hub.\"\"\"\n",
    "        token = token or globals().get('hf_token')\n",
    "        if token:\n",
    "            login(token=token)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 2: CONFIGURATION CONSTANTS\n",
    "# ============================================================================\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "# Generation parameters\n",
    "MAX_NEW_TOKENS = 200      # Maximum tokens to generate per response\n",
    "TEMPERATURE = 0.7         # Randomness: 0=deterministic, 1=creative\n",
    "TOP_P = 0.9               # Nucleus sampling: considers top 90% probability mass\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = \"data/system_prompt_engineering\"\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 3: CORE FUNCTIONS - MODEL MANAGEMENT\n",
    "# ============================================================================\n",
    "\n",
    "def load_mistral_model(model_name=MODEL_NAME, force_reload=False):\n",
    "    \"\"\"\n",
    "    Load Mistral model with caching support.\n",
    "    \n",
    "    Design Decisions:\n",
    "    - float16 on GPU for memory efficiency\n",
    "    - float32 on CPU (float16 can cause numerical issues)\n",
    "    - Global caching to avoid redundant loads (DRY principle)\n",
    "    \n",
    "    Args:\n",
    "        model_name: HuggingFace model identifier\n",
    "        force_reload: If True, reload even if cached\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (tokenizer, model)\n",
    "        \n",
    "    Raises:\n",
    "        RuntimeError: If model loading fails\n",
    "    \"\"\"\n",
    "    if not force_reload and 'mistral_tokenizer' in globals() and 'mistral_model' in globals():\n",
    "        print(\"   â„¹ï¸  Reusing cached model instance\")\n",
    "        return globals()['mistral_tokenizer'], globals()['mistral_model']\n",
    "    \n",
    "    print(f\"   Loading: {model_name}\")\n",
    "    print(\"   â³ First run downloads ~14GB model...\")\n",
    "    \n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        \n",
    "        if not torch.cuda.is_available():\n",
    "            model = model.to(\"cpu\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load model: {e}\")\n",
    "    \n",
    "    globals()['mistral_tokenizer'] = tokenizer\n",
    "    globals()['mistral_model'] = model\n",
    "    \n",
    "    device = 'GPU (CUDA)' if torch.cuda.is_available() else 'CPU'\n",
    "    print(f\"   âœ… Loaded on {device}\")\n",
    "    \n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 4: CORE FUNCTIONS - PROMPT ENGINEERING\n",
    "# ============================================================================\n",
    "#\n",
    "# Best Practices for System Prompts:\n",
    "#   1. Clear Role Definition - Explicit identity and expertise\n",
    "#   2. Comprehensive Business Context - All relevant facts\n",
    "#   3. Detailed Communication Guidelines - Tone, style, format\n",
    "#   4. Explicit Limitation Handling - What to do when uncertain\n",
    "#   5. Response Format Instructions - Structure expectations\n",
    "#\n",
    "# ============================================================================\n",
    "\n",
    "def create_system_prompt(business_name=\"GreenTech Marketplace\", \n",
    "                         business_type=\"e-commerce\",\n",
    "                         support_email=\"support@greentechmarketplace.com\",\n",
    "                         support_phone=\"1-800-GREEN-TECH\"):\n",
    "    \"\"\"\n",
    "    Create an optimized customer service system prompt.\n",
    "    \n",
    "    Prompt Engineering Best Practices Applied:\n",
    "    1. Role Definition: Clear, specific identity with expertise level\n",
    "    2. Task Boundaries: Explicit scope of responsibilities\n",
    "    3. Knowledge Base: Comprehensive domain information\n",
    "    4. Behavioral Rules: Specific guidelines for tone and style\n",
    "    5. Edge Case Handling: Instructions for unknowns/limitations\n",
    "    6. Output Format: Clear expectations for response structure\n",
    "    \n",
    "    Args:\n",
    "        business_name: Company name to use in prompt\n",
    "        business_type: Type of business (e.g., \"e-commerce\")\n",
    "        support_email: Customer support email address\n",
    "        support_phone: Customer support phone number\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted system prompt\n",
    "    \"\"\"\n",
    "    return f\"\"\"You are a friendly and knowledgeable customer service assistant for {business_name}, a leading {business_type} platform specializing in sustainable technology products.\n",
    "\n",
    "## YOUR ROLE\n",
    "You are the first point of contact for customers. Your expertise includes product information, orders, shipping, returns, and general inquiries. You embody the company's commitment to sustainability and excellent service.\n",
    "\n",
    "## KNOWLEDGE BASE\n",
    "\n",
    "**Products:**\n",
    "- Solar panels, energy-efficient appliances, smart home devices, eco-friendly accessories\n",
    "- Warranty: 1-3 years depending on product category\n",
    "\n",
    "**Shipping:**\n",
    "- Standard: 5-7 business days (free over $75)\n",
    "- Express: 2-3 business days (additional fee)\n",
    "\n",
    "**Returns & Refunds:**\n",
    "- 30-day return policy for unopened items in original packaging\n",
    "- Refunds processed within 5-7 business days after receipt\n",
    "\n",
    "**Customer Service Hours:**\n",
    "- Monday-Friday: 9 AM - 6 PM EST\n",
    "- Saturday: 10 AM - 4 PM EST\n",
    "- Closed Sunday\n",
    "\n",
    "**Contact Methods:**\n",
    "- Email: {support_email}\n",
    "- Phone: {support_phone}\n",
    "- Live chat: Available during business hours in EST \n",
    "\n",
    "## COMMUNICATION GUIDELINES\n",
    "\n",
    "**Tone:** Warm, professional, solution-oriented\n",
    "**Style:** Clear, concise, helpful\n",
    "\n",
    "**Always:**\n",
    "- Greet customers warmly\n",
    "- Acknowledge their concerns before providing solutions\n",
    "- Provide specific, actionable information\n",
    "- Include relevant timeframes and next steps\n",
    "- Thank them for choosing {business_name}\n",
    "\n",
    "**Never:**\n",
    "- Make promises you cannot keep\n",
    "- Provide information not in your knowledge base\n",
    "- Use technical jargon without explanation\n",
    "\n",
    "## HANDLING LIMITATIONS\n",
    "\n",
    "If you don't know the answer:\n",
    "1. Acknowledge the question honestly\n",
    "2. Explain that the information is not in your current knowledge base\n",
    "3. Offer to connect them with a specialist or provide contact information\n",
    "4. Suggest alternative resources if available\n",
    "\n",
    "## RESPONSE FORMAT\n",
    "\n",
    "Keep responses concise but complete. Structure longer responses with clear sections. Always end with an offer to help further.\"\"\"\n",
    "\n",
    "\n",
    "def format_mistral_prompt(system_prompt, user_input):\n",
    "    \"\"\"\n",
    "    Format for Mistral Instruct template.\n",
    "    \n",
    "    Template: <s>[INST] {system} {user} [/INST]\n",
    "    \n",
    "    Args:\n",
    "        system_prompt: The system prompt defining assistant behavior\n",
    "        user_input: The user's question or message\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted prompt string\n",
    "    \"\"\"\n",
    "    return f\"<s>[INST] {system_prompt}\\n\\nCustomer Question: {user_input} [/INST]\"\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 5: CORE FUNCTIONS - GENERATION\n",
    "# ============================================================================\n",
    "\n",
    "def generate_response(tokenizer, model, formatted_prompt, \n",
    "                      max_new_tokens=MAX_NEW_TOKENS, \n",
    "                      temperature=TEMPERATURE, \n",
    "                      top_p=TOP_P):\n",
    "    \"\"\"\n",
    "    Generate model response.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer: Model tokenizer\n",
    "        model: Loaded model instance\n",
    "        formatted_prompt: Prompt formatted with format_mistral_prompt()\n",
    "        max_new_tokens: Maximum tokens to generate\n",
    "        temperature: Sampling temperature (0=deterministic, 1=creative)\n",
    "        top_p: Nucleus sampling threshold\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated response text\n",
    "        \n",
    "    Raises:\n",
    "        RuntimeError: If generation fails (e.g., OOM error)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        inputs = tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                top_p=top_p,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode only the NEW tokens (not the input)\n",
    "        input_length = inputs['input_ids'].shape[1]\n",
    "        generated_tokens = outputs[0][input_length:]\n",
    "        response = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except torch.cuda.OutOfMemoryError:\n",
    "        raise RuntimeError(\"GPU out of memory. Try reducing max_new_tokens or use CPU.\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Generation failed: {e}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 6: FILE STORAGE UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "def save_system_prompt(system_prompt, output_dir=OUTPUT_DIR, filename=\"system_prompt.txt\"):\n",
    "    \"\"\"\n",
    "    Save system prompt to text file.\n",
    "    \n",
    "    Args:\n",
    "        system_prompt: The system prompt text\n",
    "        output_dir: Directory to save to\n",
    "        filename: Output filename\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to saved file\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    with open(filepath, 'w') as f:\n",
    "        f.write(system_prompt)\n",
    "    print(f\"   âœ… System prompt saved to: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def save_response(response, question, output_dir=OUTPUT_DIR, filename=\"test_response.txt\"):\n",
    "    \"\"\"\n",
    "    Save generated response to text file.\n",
    "    \n",
    "    Args:\n",
    "        response: The generated response text\n",
    "        question: The question that was asked\n",
    "        output_dir: Directory to save to\n",
    "        filename: Output filename\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to saved file\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    with open(filepath, 'w') as f:\n",
    "        f.write(f\"Question: {question}\\n\")\n",
    "        f.write(f\"\\n{'='*50}\\n\\n\")\n",
    "        f.write(f\"Response:\\n{response}\")\n",
    "    print(f\"   âœ… Response saved to: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "def verify_objective1():\n",
    "    \"\"\"\n",
    "    Verify that Objective 1 completed successfully.\n",
    "    Checks all variables, files, and model loading.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"ğŸ” OBJECTIVE 1 VERIFICATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    errors = []\n",
    "    \n",
    "    # Check if variables exist\n",
    "    if 'mistral_model' not in globals():\n",
    "        errors.append(\"âŒ mistral_model not found\")\n",
    "    if 'mistral_tokenizer' not in globals():\n",
    "        errors.append(\"âŒ mistral_tokenizer not found\")\n",
    "    if 'system_prompt' not in globals():\n",
    "        errors.append(\"âŒ system_prompt not found\")\n",
    "    \n",
    "    if errors:\n",
    "        print(\"\\n\".join(errors))\n",
    "        print(\"=\"*70)\n",
    "        return False\n",
    "    \n",
    "    # Check system prompt\n",
    "    if len(system_prompt) < 100:\n",
    "        errors.append(f\"âŒ System prompt too short ({len(system_prompt)} chars)\")\n",
    "    \n",
    "    # Check files exist\n",
    "    if not os.path.exists(\"data/system_prompt_engineering/system_prompt.txt\"):\n",
    "        errors.append(\"âŒ system_prompt.txt not found\")\n",
    "    if not os.path.exists(\"data/system_prompt_engineering/test_response.txt\"):\n",
    "        errors.append(\"âŒ test_response.txt not found\")\n",
    "    \n",
    "    # Check model is loaded\n",
    "    try:\n",
    "        import torch\n",
    "        device = 'GPU (CUDA)' if torch.cuda.is_available() else 'CPU'\n",
    "        model_loaded = mistral_model is not None and mistral_tokenizer is not None\n",
    "        if not model_loaded:\n",
    "            errors.append(\"âŒ Model or tokenizer not properly loaded\")\n",
    "    except Exception as e:\n",
    "        errors.append(f\"âŒ Error checking model: {e}\")\n",
    "    \n",
    "    # Print results\n",
    "    if errors:\n",
    "        print(\"\\nâŒ VERIFICATION FAILED:\")\n",
    "        print(\"\\n\".join(errors))\n",
    "        print(\"=\"*70)\n",
    "        return False\n",
    "    else:\n",
    "        print(\"\\nâœ… Objective 1 Complete - All variables and files verified\")\n",
    "        print(f\"   â€¢ Model: Loaded on {device}\")\n",
    "        print(f\"   â€¢ System Prompt: {len(system_prompt)} characters\")\n",
    "        print(f\"   â€¢ Tokenizer: Ready\")\n",
    "        print(f\"   â€¢ Files: Saved to data/system_prompt_engineering/\")\n",
    "        print(\"=\"*70)\n",
    "        return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# SECTION 7: EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"   OBJECTIVE 1: DESIGN SYSTEM PROMPTS\")\n",
    "print(\"   Demonstrating Prompt Engineering\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# --- Step 1: Authentication ---\n",
    "print(\"\\nğŸ”‘ STEP 1: Authenticate with Hugging Face\")\n",
    "print(\"-\"*70)\n",
    "authenticate_hf()\n",
    "\n",
    "# --- Step 2: Load Model ---\n",
    "print(\"\\nğŸ¤– STEP 2: Load Mistral Model\")\n",
    "print(\"-\"*70)\n",
    "tokenizer, model = load_mistral_model()\n",
    "\n",
    "# --- Step 3: Create System Prompt ---\n",
    "print(\"\\nğŸ“ STEP 3: Create Optimized System Prompt\")\n",
    "print(\"-\"*70)\n",
    "system_prompt = create_system_prompt()\n",
    "print(f\"   âœ… Created ({len(system_prompt)} chars, {len(system_prompt.split(chr(10)))} lines)\")\n",
    "print(\"\\nğŸ“„ System Prompt Preview (first 500 chars):\")\n",
    "print(\"-\"*70)\n",
    "print(system_prompt[:500] + \"...\")\n",
    "\n",
    "globals()['system_prompt'] = system_prompt\n",
    "\n",
    "# --- Step 4: Test Generation ---\n",
    "print(\"\\nğŸ§ª STEP 4: Test with Sample Question\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "test_question = \"What are your store hours and how can I contact customer support?\"\n",
    "print(f\"   Question: {test_question}\\n\")\n",
    "\n",
    "formatted_prompt = format_mistral_prompt(system_prompt, test_question)\n",
    "print(\"   â³ Generating response...\")\n",
    "\n",
    "generated_response = generate_response(tokenizer, model, formatted_prompt)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“¥ GENERATED RESPONSE:\")\n",
    "print(\"=\"*70)\n",
    "print(generated_response)\n",
    "print(\"=\"*70)\n",
    "\n",
    "# --- Step 5: Save Files ---\n",
    "print(\"\\nğŸ’¾ STEP 5: Save Files\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "save_system_prompt(system_prompt)\n",
    "save_response(generated_response, test_question)\n",
    "\n",
    "# --- Step 6: Verify Objective 1 ---\n",
    "print(\"\\nâœ… STEP 6: Verify Objective 1\")\n",
    "print(\"-\"*70)\n",
    "verify_objective1()\n",
    "\n",
    "\n",
    "# --- Summary ---\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… OBJECTIVE 1 COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\"\"\n",
    "Key Concepts Demonstrated:\n",
    "  1. System Prompt Engineering - Structured prompt with role, context, guidelines\n",
    "  2. Modular Design - Reusable functions following SOLID/DRY/KISS\n",
    "\n",
    "ğŸ“¦ FILES SAVED:\n",
    "  â€¢ {OUTPUT_DIR}/system_prompt.txt\n",
    "  â€¢ {OUTPUT_DIR}/test_response.txt\n",
    "\n",
    "Available Functions:\n",
    "  â€¢ load_mistral_model()     - Load and cache Mistral model\n",
    "  â€¢ create_system_prompt()   - Generate customer service prompt\n",
    "  â€¢ format_mistral_prompt()  - Format for Mistral Instruct template\n",
    "  â€¢ generate_response()      - Generate text with error handling\n",
    "  â€¢ save_system_prompt()     - Save prompt to file\n",
    "  â€¢ save_response()          - Save response to file\n",
    "\"\"\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective 2: Generate Custom Q&A Databases for E-commerce Customer Service\n",
    "\n",
    "### ğŸ¯ Goal\n",
    "Generate 21 Q&A pairs (18 answerable + 3 unanswerable) covering e-commerce customer service topics to create a domain-specific knowledge base for the RAG system.\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ“¥ Prerequisites</b> (Click to expand)</summary>\n",
    "\n",
    "| Item | Source | Required | Description |\n",
    "|------|--------|----------|-------------|\n",
    "| `mistral_model` | Objective 1 | âœ… Yes | Mistral model for generating Q&A pairs |\n",
    "| `mistral_tokenizer` | Objective 1 | âœ… Yes | Tokenizer for text processing and encoding |\n",
    "| `system_prompt` | Objective 1 | âœ… Yes | System prompt defining business context and role |\n",
    "| `hf_token` | Objective 0 | âœ… Yes | Hugging Face API token for model access |\n",
    "\n",
    "**Note:** Objective 1 must be completed first to provide the model and system prompt.\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ”§ Core Concepts</b> (Click to expand)</summary>\n",
    "\n",
    "| Concept | Description |\n",
    "|--------|-------------|\n",
    "| **Q&A Generation** | Using LLMs to create domain-specific question-answer pairs that serve as the knowledge base for RAG systems |\n",
    "| **Zero-Shot Prompting** | Providing instructions without examples - the model generates Q&A pairs based solely on the system prompt and task description |\n",
    "| **Few-Shot Prompting** | Providing examples in the prompt to guide the model's output format and style - improves consistency and quality |\n",
    "| **System Prompt Reuse** | Leveraging the system prompt from Objective 1 to ensure Q&A pairs align with the business context and customer service role |\n",
    "| **Answerable Questions** | Questions that can be answered from the business knowledge base (products, policies, procedures) |\n",
    "| **Unanswerable Questions** | Questions outside the knowledge base scope (competitor info, personal advice, future events) |\n",
    "| **Delimiter Parsing** | Using a consistent delimiter (`|||`) to reliably extract structured data from LLM-generated text |\n",
    "| **DataFrame Structure** | Converting Q&A pairs to pandas DataFrame for easy filtering, querying, and analysis |\n",
    "\n",
    "**Why This Matters:**\n",
    "A well-structured Q&A database is the foundation of the RAG system. It provides the knowledge that will be retrieved and used to generate accurate, context-aware responses. Using the system prompt from Objective 1 ensures consistency with the customer service role and business context.\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ“Š Design Choices</b> (Click to expand)</summary>\n",
    "\n",
    "| Choice | Selected | Rationale |\n",
    "|--------|----------|-----------|\n",
    "| **Prompt Technique** | Zero-Shot with System Prompt | Uses the system prompt from Objective 1 to provide business context, ensuring Q&A pairs align with customer service role and domain |\n",
    "| **System Prompt Integration** | Reuse from Objective 1 | The `system_prompt` variable is embedded in generation prompts to maintain consistency with business context, role definition, and knowledge boundaries |\n",
    "| **Total Q&A Pairs** | 21 (18 answerable + 3 unanswerable) | Sufficient coverage for testing while manageable in size |\n",
    "| **Answerable Categories** | 6 categories Ã— 3 pairs = 18 | Products, shipping, returns, customer service, warranty, orders - core business areas |\n",
    "| **Unanswerable Types** | 3 types Ã— 1 pair = 3 | Competitor, personal advice, future events - tests system limitations |\n",
    "| **Data Format** | List of dictionaries + DataFrame | Flexible access patterns (list for iteration, DataFrame for filtering) |\n",
    "| **Delimiter** | `|||` | Uncommon in natural text, reliable for parsing LLM output |\n",
    "| **Answerable Flag** | Boolean (`True`/`False`) | Enables easy filtering and testing of system's ability to decline appropriately |\n",
    "\n",
    "**Prompt Engineering Approach:**\n",
    "- **Zero-Shot Technique**: Instructions are provided without examples, relying on the model's pre-training and the system prompt's context\n",
    "- **System Prompt Embedding**: The first 1200 characters of `system_prompt` from Objective 1 are included in each generation prompt to ensure:\n",
    "  - Business context (GreenTech Marketplace, e-commerce)\n",
    "  - Role definition (customer service assistant)\n",
    "  - Knowledge boundaries (what can/cannot be answered)\n",
    "  - Communication style (warm, professional, helpful)\n",
    "- **Format Instructions**: Clear delimiter-based format (`category ||| question ||| answer`) ensures reliable parsing\n",
    "\n",
    "**Why This Approach:**\n",
    "- **System Prompt Reuse**: Ensures Q&A pairs are consistent with the customer service role and business context defined in Objective 1\n",
    "- **Zero-Shot Efficiency**: Faster than few-shot (no example formatting needed) while still producing quality results\n",
    "- **21 pairs**: Balances comprehensiveness with generation time and evaluation complexity\n",
    "- **6 answerable categories**: Covers all major customer service touchpoints\n",
    "- **3 unanswerable types**: Tests system's ability to recognize and handle out-of-scope questions\n",
    "- **Dual format**: List for programmatic access, DataFrame for analysis and filtering\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ”„ Process Flow</b> (Click to expand)</summary>\n",
    "\n",
    "**Left-to-Right Flow:**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Validate    â”‚â”€â”€â”€â–¶â”‚  Generate    â”‚â”€â”€â”€â–¶â”‚  Convert to  â”‚â”€â”€â”€â–¶â”‚  Display     â”‚â”€â”€â”€â–¶â”‚  Save to     â”‚\n",
    "â”‚ Prerequisitesâ”‚    â”‚  Q&A Pairs   â”‚    â”‚  DataFrame   â”‚    â”‚  Statistics  â”‚    â”‚  CSV Files   â”‚\n",
    "â”‚              â”‚    â”‚  (21 total)  â”‚    â”‚              â”‚    â”‚              â”‚    â”‚              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Detailed Steps:**\n",
    "1. **Validate Prerequisites**: Ensure Objective 1 completed (model, tokenizer, system prompt available)\n",
    "2. **Generate Q&A Pairs**: Use Mistral with zero-shot prompting, embedding the system prompt from Objective 1 to generate:\n",
    "   - 18 answerable pairs (6 categories Ã— 3) using business context from system prompt\n",
    "   - 3 unanswerable pairs (3 types Ã— 1) to test system limitations\n",
    "3. **Create DataFrame**: Convert list of dictionaries to pandas DataFrame with computed metrics\n",
    "4. **Display Statistics**: Show coverage, distribution, and quality metrics\n",
    "5. **Display All Q&A Pairs**: Print all pairs with comments for documentation\n",
    "6. **Save to CSV**: Persist Q&A database to `data/qa_database/qa_database.csv`\n",
    "\n",
    "**Prompt Technique Details:**\n",
    "- **Zero-Shot Approach**: Each generation prompt includes:\n",
    "  - First 1200 characters of `system_prompt` from Objective 1 (business context, role, knowledge base)\n",
    "  - Category-specific instructions\n",
    "  - Format requirements (delimiter-based structure)\n",
    "- **System Prompt Integration**: The system prompt ensures generated Q&A pairs:\n",
    "  - Align with customer service role\n",
    "  - Use correct business terminology\n",
    "  - Follow communication guidelines\n",
    "  - Respect knowledge boundaries\n",
    "\n",
    "**Category Breakdown:**\n",
    "- **Answerable (18 pairs)**: products (3), shipping (3), returns (3), customer_service (3), warranty (3), orders (3)\n",
    "- **Unanswerable (3 pairs)**: competitor (1), personal_advice (1), future_events (1)\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ“¤ Outputs</b> (Click to expand)</summary>\n",
    "\n",
    "| Variable | Type | Description |\n",
    "|----------|------|-------------|\n",
    "| `qa_database` | `List[Dict]` | List of 21 Q&A dictionaries with keys: `category`, `answerable`, `question`, `answer` |\n",
    "| `qa_df` | `pd.DataFrame` | DataFrame with Q&A pairs plus computed columns: `answer_length`, `word_count` |\n",
    "\n",
    "**Files Created:**\n",
    "| File | Location | Description |\n",
    "|------|----------|-------------|\n",
    "| `qa_database.csv` | `data/qa_database/` | All 21 Q&A pairs with answerable flag, ready for RAG system |\n",
    "\n",
    "**Key Functions Created:**\n",
    "- `generate_qa_for_category()`: Generates Q&A pairs for a specific category\n",
    "- `generate_full_qa_database()`: Generates complete 21-pair database\n",
    "- `qa_to_dataframe()`: Converts Q&A list to DataFrame with metrics\n",
    "- `display_statistics()`: Shows database coverage and quality metrics\n",
    "- `display_all_qa_pairs()`: Prints all Q&A pairs with comments\n",
    "- `save_qa_to_csv()`: Saves Q&A database to CSV file\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ“‹ Q&A Database Structure</b> (Click to expand)</summary>\n",
    "\n",
    "**Answerable Categories (18 pairs):**\n",
    "\n",
    "| Category | Count | Topics Covered |\n",
    "|----------|-------|---------------|\n",
    "| **products** | 3 | Types of products, specifications, features |\n",
    "| **shipping** | 3 | Delivery times, shipping costs, free shipping threshold, tracking |\n",
    "| **returns** | 3 | Return policy, refund process, conditions, 30-day policy |\n",
    "| **customer_service** | 3 | Business hours (Mon-Fri 9AM-6PM, Sat 10AM-4PM), contact email and phone |\n",
    "| **warranty** | 3 | Warranty duration (1-3 years), coverage, claims process |\n",
    "| **orders** | 3 | Order status, tracking, modifications, cancellations |\n",
    "\n",
    "**Unanswerable Types (3 pairs):**\n",
    "\n",
    "| Type | Count | Description |\n",
    "|------|-------|-------------|\n",
    "| **competitor** | 1 | Questions about competitor pricing, products, or comparisons |\n",
    "| **personal_advice** | 1 | Questions asking for personal recommendations or opinions |\n",
    "| **future_events** | 1 | Questions about future sales, unreleased products, or predictions |\n",
    "\n",
    "**Data Structure:**\n",
    "```python\n",
    "{\n",
    "    'category': 'shipping',\n",
    "    'answerable': True,\n",
    "    'question': 'What is your shipping policy?',\n",
    "    'answer': 'We offer standard shipping (5-7 business days, free over $75)...'\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>â–¶ï¸ How to Run</b> (Click to expand)</summary>\n",
    "\n",
    "1. **Ensure Prerequisites Complete**: \n",
    "   - Run Objective 0 (Setup) first\n",
    "   - Run Objective 1 (System Prompt) - must complete successfully\n",
    "   - Verify `mistral_model`, `mistral_tokenizer`, and `system_prompt` exist\n",
    "\n",
    "2. **Run the Code Cell**: \n",
    "   - Execute the Objective 2 code cell\n",
    "   - Generation takes 2-3 minutes (21 LLM calls)\n",
    "   - Progress will be shown for each category\n",
    "\n",
    "3. **Verify Output**: \n",
    "   - Check console for: \"âœ… Generated 21 Q&A pairs\"\n",
    "   - Verify DataFrame created: \"âœ… DataFrame created: 21 rows Ã— 6 columns\"\n",
    "   - Check statistics displayed\n",
    "   - Verify file saved: `data/qa_database/qa_database.csv`\n",
    "\n",
    "**Expected Runtime:**\n",
    "- GPU: 2-3 minutes (21 generation calls)\n",
    "- CPU: 3-4 minutes (21 generation calls)\n",
    "\n",
    "**Troubleshooting:**\n",
    "- If generation fails, check that Objective 1 completed successfully\n",
    "- If parsing errors occur, the delimiter format may need adjustment\n",
    "- If file save fails, ensure `data/qa_database/` directory exists\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>âœ… Verification</b> (Click to expand)</summary>\n",
    "\n",
    "After running, use the verification function to check all requirements:\n",
    "\n",
    "```python\n",
    "# Call the verification function (created in Objective 2 code cell)\n",
    "verify_objective2()\n",
    "```\n",
    "\n",
    "**What the Function Checks:**\n",
    "- âœ… Variables exist: `qa_database`, `qa_df`\n",
    "- âœ… Correct count: 21 Q&A pairs total\n",
    "- âœ… Structure: All pairs have required keys (`category`, `answerable`, `question`, `answer`)\n",
    "- âœ… Distribution: 18 answerable + 3 unanswerable pairs\n",
    "- âœ… DataFrame: Correct number of rows and required columns\n",
    "- âœ… File exists: `data/qa_database/qa_database.csv` saved successfully\n",
    "\n",
    "**Function Output:**\n",
    "The function will print a summary with:\n",
    "- Total Q&A pairs count\n",
    "- Answerable vs unanswerable breakdown\n",
    "- DataFrame dimensions\n",
    "- File location confirmation\n",
    "\n",
    "**Note:** The `verify_objective2()` function is automatically created when you run the Objective 2 code cell. It performs all verification checks and provides a clear success/failure report.\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ”— Dependencies</b> (Click to expand)</summary>\n",
    "\n",
    "**This Objective:**\n",
    "- âœ… Requires Objective 0 (Setup) for `hf_token`\n",
    "- âœ… Requires Objective 1 (System Prompt) for `mistral_model`, `mistral_tokenizer`, `system_prompt`\n",
    "\n",
    "**Used By:**\n",
    "- Objective 3: Uses `qa_database` to build FAISS vector index\n",
    "- Objective 4: Uses `qa_database` as knowledge base for RAG retrieval\n",
    "- Objective 5: Uses `qa_database` for model evaluation\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ“š Learning Objectives Demonstrated</b> (Click to expand)</summary>\n",
    "\n",
    "1. **Zero-Shot Prompting**: Using instructions without examples to guide LLM output, leveraging the model's pre-training and context from system prompts\n",
    "2. **System Prompt Reuse**: Integrating previously created system prompts to maintain consistency across objectives and ensure domain alignment\n",
    "3. **LLM-Based Content Generation**: Using language models to create structured domain-specific content\n",
    "4. **Data Structure Design**: Designing flexible data formats (list + DataFrame) for different access patterns\n",
    "5. **Parsing LLM Output**: Reliable extraction of structured data from free-form LLM responses using delimiter-based parsing\n",
    "6. **Knowledge Base Construction**: Building domain-specific knowledge bases for RAG systems\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ’¡ Tips</b> (Click to expand)</summary>\n",
    "\n",
    "- **System Prompt Integration**: The system prompt from Objective 1 is automatically embedded in generation prompts - ensure Objective 1 completed successfully\n",
    "- **Zero-Shot vs Few-Shot**: Zero-shot is used here for efficiency; few-shot could improve consistency but requires example formatting\n",
    "- **Generation Time**: 21 LLM calls take 2-3 minutes - be patient\n",
    "- **Delimiter Choice**: `|||` is chosen because it's unlikely to appear in natural text\n",
    "- **Answerable Flag**: Essential for testing system's ability to decline unanswerable questions\n",
    "- **DataFrame Benefits**: Use `qa_df[qa_df['answerable'] == True]` for easy filtering\n",
    "- **Category Coverage**: Each category has 3 pairs to ensure sufficient coverage\n",
    "- **Unanswerable Testing**: The 3 unanswerable pairs test different types of out-of-scope questions\n",
    "- **Prompt Context**: The system prompt's first 1200 characters are included to provide business context without exceeding token limits\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Next Step:** Proceed to Objective 3 to build the FAISS vector database from the Q&A pairs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OBJECTIVE 2 (REVISED): GENERATE CUSTOM Q&A DATABASES \n",
    "# ============================================================================\n",
    "#\n",
    "#\n",
    "# NOTE: Requires Objective 1 (mistral_model, mistral_tokenizer, system_prompt)\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIG\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "TEMPERATURE = 0.7\n",
    "TOP_P = 0.9\n",
    "CONTEXT_CHARS = 900           # Smaller excerpt increases relevance\n",
    "DELIMITER = \"|||\"\n",
    "\n",
    "OUTPUT_DIR = \"data/qa_database\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# VALIDATION\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def validate_prerequisites():\n",
    "    required = [\"mistral_model\", \"mistral_tokenizer\", \"system_prompt\"]\n",
    "    missing = [r for r in required if r not in globals()]\n",
    "    if missing:\n",
    "        raise RuntimeError(\n",
    "            f\"âŒ Objective 2 requires Objective 1 first. Missing: {missing}\"\n",
    "        )\n",
    "    print(\"   âœ… Prerequisites validated\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CATEGORY DEFINITIONS\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "QA_CATEGORIES = [\n",
    "    (\"products\", \"types of products, solar panels, smart devices, eco-friendly items\"),\n",
    "    (\"shipping\", \"delivery times, shipping cost, free shipping threshold, tracking\"),\n",
    "    (\"returns\", \"return policy, refund window, 30-day policy, conditions\"),\n",
    "    (\"customer_service\", \"hours (Monâ€“Fri 9â€“6, Sat 10â€“4), email, phone support\"),\n",
    "    (\"warranty\", \"coverage periods 1â€“3 years, claims process\"),\n",
    "    (\"orders\", \"order status, modifying or cancelling orders, tracking numbers\"),\n",
    "]\n",
    "\n",
    "UNANSWERABLE_TYPES = [\n",
    "    (\"competitor\", \"questions about competitor prices or product comparisons\"),\n",
    "    (\"personal_advice\", \"questions asking for personal recommendations or opinions\"),\n",
    "    (\"future_events\", \"questions about upcoming sales or unreleased products\"),\n",
    "]\n",
    "\n",
    "PAIRS_PER_CATEGORY = 3\n",
    "UNANSWERABLE_PER_TYPE = 1\n",
    "\n",
    "ANSWERABLE_TOTAL = len(QA_CATEGORIES) * PAIRS_PER_CATEGORY\n",
    "UNANSWERABLE_TOTAL = len(UNANSWERABLE_TYPES) * UNANSWERABLE_PER_TYPE\n",
    "TOTAL_PAIRS = ANSWERABLE_TOTAL + UNANSWERABLE_TOTAL\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PROMPT TEMPLATES \n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "ANSWERABLE_PROMPT = \"\"\"\n",
    "You are generating REALISTIC customer service Q&A pairs for the ShopSmart e-commerce support assistant.\n",
    "\n",
    "Generate EXACTLY {num_pairs} Q&A pairs about the topic below.\n",
    "\n",
    "TOPIC FOCUS:\n",
    "{description}\n",
    "\n",
    "BUSINESS CONTEXT (from system prompt):\n",
    "{context}\n",
    "\n",
    "CRITICAL: You MUST output valid JSON only. No other text before or after.\n",
    "\n",
    "OUTPUT FORMAT (JSON array):\n",
    "[\n",
    "  {{\"question\": \"What is your shipping policy?\", \"answer\": \"We offer standard shipping (5-7 business days) for free on orders over $75. Express shipping (2-3 business days) is available for an additional $15. All orders are shipped with tracking numbers.\"}},\n",
    "  {{\"question\": \"Can I return a product if I'm not satisfied?\", \"answer\": \"Yes, you can return any unopened item in its original packaging within 30 days of delivery for a full refund. Simply contact our support team to initiate the return process.\"}},\n",
    "  {{\"question\": \"What are your customer service hours?\", \"answer\": \"Our customer service team is available Monday through Friday from 9 AM to 6 PM EST, and on Saturdays from 10 AM to 4 PM EST. You can reach us via email at support@greentechmarketplace.com or by phone.\"}}\n",
    "]\n",
    "\n",
    "CONTENT RULES:\n",
    "- Questions MUST sound like real customers asking natural questions.\n",
    "- Answers MUST be 2â€“3 sentences with concrete details (times, numbers, policies, contact info).\n",
    "- Stay entirely within ShopSmart policies from the business context.\n",
    "- DO NOT hallucinate unsupported information.\n",
    "\n",
    "OUTPUT: Return a valid JSON array with EXACTLY {num_pairs} objects, each with \"question\" and \"answer\" fields.\n",
    "\"\"\"\n",
    "\n",
    "UNANSWERABLE_PROMPT = \"\"\"\n",
    "Generate EXACTLY {num_pairs} UNANSWERABLE customer Q&A pairs.\n",
    "\n",
    "TOPIC TYPE OUT OF SCOPE:\n",
    "{description}\n",
    "\n",
    "CRITICAL: You MUST output valid JSON only. No other text before or after.\n",
    "\n",
    "OUTPUT FORMAT (JSON array):\n",
    "[\n",
    "  {{\"question\": \"What are your competitor's prices?\", \"answer\": \"I'm sorry, but I'm unable to provide information about competitor pricing as it's outside our knowledge base. However, I'd be happy to help you with questions about our own products, shipping options, or return policies.\"}},\n",
    "  {{\"question\": \"Can you recommend the best restaurant in New York?\", \"answer\": \"I apologize, but I cannot provide personal recommendations or advice about restaurants as that's beyond ShopSmart's scope. I can assist you with questions about our products, shipping, returns, or order tracking.\"}},\n",
    "  {{\"question\": \"When will you release new products next month?\", \"answer\": \"I'm unable to provide information about upcoming product releases or future events as that information isn't available in our knowledge base. However, I can help you with questions about our current product catalog, shipping options, or warranty information.\"}}\n",
    "]\n",
    "\n",
    "REFUSAL RULES:\n",
    "- Question MUST be outside ShopSmart's knowledge base (competitor info, personal advice, future events, etc.)\n",
    "- Answer MUST politely decline and explain you cannot provide that information\n",
    "- Answer MUST offer what you *can* help with (shipping, returns, products, warranty, orders, etc.)\n",
    "- Answer MUST be 2 sentences\n",
    "\n",
    "OUTPUT: Return a valid JSON array with EXACTLY {num_pairs} objects, each with \"question\" and \"answer\" fields.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# GENERATION FUNCTION\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def mistral_generate(prompt: str, max_tokens: int = 600) -> str:\n",
    "    tokenizer = mistral_tokenizer\n",
    "    model = mistral_model\n",
    "\n",
    "    formatted = f\"<s>[INST] {prompt} [/INST]\"\n",
    "\n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\")\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_p=TOP_P,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Extract generated tokens (skip prompt)\n",
    "    inp_len = inputs[\"input_ids\"].shape[1]\n",
    "    gen_tokens = outputs[0][inp_len:]\n",
    "    text = tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PARSER (ROBUST - IMPROVED)\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def parse_qa_lines(text: str, answerable: bool, debug: bool = False) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Parse Q&A pairs from model output.\n",
    "    Expected format: JSON array with objects containing \"question\" and \"answer\" fields.\n",
    "    \"\"\"\n",
    "    qa_list = []\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"      [DEBUG] Raw model output ({len(text)} chars):\")\n",
    "        print(f\"      {repr(text[:300])}...\")\n",
    "    \n",
    "    # Try to extract JSON from the text (might have extra text before/after)\n",
    "    text_clean = text.strip()\n",
    "    \n",
    "    # Find JSON array in the text (handle cases where model adds extra text)\n",
    "    json_start = text_clean.find('[')\n",
    "    json_end = text_clean.rfind(']') + 1\n",
    "    \n",
    "    if json_start == -1 or json_end == 0:\n",
    "        if debug:\n",
    "            print(f\"      [DEBUG] No JSON array found in output\")\n",
    "        return qa_list\n",
    "    \n",
    "    json_text = text_clean[json_start:json_end]\n",
    "    \n",
    "    try:\n",
    "        parsed_data = json.loads(json_text)\n",
    "        \n",
    "        if not isinstance(parsed_data, list):\n",
    "            if debug:\n",
    "                print(f\"      [DEBUG] JSON is not an array, got: {type(parsed_data)}\")\n",
    "            return qa_list\n",
    "        \n",
    "        for item in parsed_data:\n",
    "            if isinstance(item, dict) and \"question\" in item and \"answer\" in item:\n",
    "                q = item[\"question\"].strip()\n",
    "                a = item[\"answer\"].strip()\n",
    "                \n",
    "                # Validate content\n",
    "                if q and a and len(q) > 3 and len(a) > 10:\n",
    "                    qa_list.append({\n",
    "                        \"question\": q,\n",
    "                        \"answer\": a,\n",
    "                        \"answerable\": bool(answerable)\n",
    "                    })\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"      [DEBUG] Parsed {len(qa_list)} pairs from JSON\")\n",
    "            if len(qa_list) > 0:\n",
    "                print(f\"      [DEBUG] First parsed pair:\")\n",
    "                print(f\"        Q: {qa_list[0]['question'][:80]}...\")\n",
    "                print(f\"        A: {qa_list[0]['answer'][:80]}...\")\n",
    "    \n",
    "    except json.JSONDecodeError as e:\n",
    "        if debug:\n",
    "            print(f\"      [DEBUG] JSON decode error: {e}\")\n",
    "            print(f\"      [DEBUG] Attempted to parse: {json_text[:200]}...\")\n",
    "    \n",
    "    return qa_list\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# HIGH-LEVEL GENERATORS (WITH RETRY LOGIC)\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def generate_answerable(category: str, description: str, n: int, max_retries: int = 3) -> List[Dict]:\n",
    "    \"\"\"Generate answerable Q&A pairs with retry logic if parsing fails.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        prompt = ANSWERABLE_PROMPT.format(\n",
    "            num_pairs=n,\n",
    "            description=description,\n",
    "            context=system_prompt[:CONTEXT_CHARS]\n",
    "        )\n",
    "        raw = mistral_generate(prompt, max_tokens=800)  # Increased tokens for better generation\n",
    "        parsed = parse_qa_lines(raw, True, debug=(attempt == max_retries - 1))\n",
    "        \n",
    "        if len(parsed) >= n:\n",
    "            return parsed[:n]\n",
    "        elif len(parsed) > 0:\n",
    "            # If we got some pairs but not enough, return what we have\n",
    "            print(f\"      âš ï¸  Got {len(parsed)}/{n} pairs (attempt {attempt + 1})\")\n",
    "            if attempt < max_retries - 1:\n",
    "                continue\n",
    "            return parsed\n",
    "    \n",
    "    # If all retries failed, return empty list\n",
    "    print(f\"      âŒ Failed to generate pairs after {max_retries} attempts\")\n",
    "    return []\n",
    "\n",
    "\n",
    "def generate_unanswerable(category: str, description: str, n: int, max_retries: int = 3) -> List[Dict]:\n",
    "    \"\"\"Generate unanswerable Q&A pairs with retry logic if parsing fails.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        prompt = UNANSWERABLE_PROMPT.format(\n",
    "            num_pairs=n,\n",
    "            description=description\n",
    "        )\n",
    "        raw = mistral_generate(prompt, max_tokens=600)\n",
    "        parsed = parse_qa_lines(raw, False, debug=(attempt == max_retries - 1))\n",
    "        \n",
    "        if len(parsed) >= n:\n",
    "            return parsed[:n]\n",
    "        elif len(parsed) > 0:\n",
    "            print(f\"      âš ï¸  Got {len(parsed)}/{n} pairs (attempt {attempt + 1})\")\n",
    "            if attempt < max_retries - 1:\n",
    "                continue\n",
    "            return parsed\n",
    "    \n",
    "    print(f\"      âŒ Failed to generate pairs after {max_retries} attempts\")\n",
    "    return []\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# FULL DATABASE GENERATOR\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def generate_full_qa_database():\n",
    "    db = []\n",
    "\n",
    "    print(\"\\nğŸ“— Generating answerable Q&A...\")\n",
    "    for cat, desc in QA_CATEGORIES:\n",
    "        print(f\"   â†’ {cat}...\")\n",
    "        pairs = generate_answerable(cat, desc, PAIRS_PER_CATEGORY)\n",
    "        for p in pairs:\n",
    "            p[\"category\"] = cat\n",
    "        db.extend(pairs)\n",
    "\n",
    "    print(\"\\nğŸ“• Generating unanswerable Q&A...\")\n",
    "    for cat, desc in UNANSWERABLE_TYPES:\n",
    "        print(f\"   â†’ {cat}...\")\n",
    "        pairs = generate_unanswerable(cat, desc, UNANSWERABLE_PER_TYPE)\n",
    "        for p in pairs:\n",
    "            p[\"category\"] = cat\n",
    "        db.extend(pairs)\n",
    "\n",
    "    print(f\"\\nğŸ‰ Generated {len(db)} total pairs \"\n",
    "          f\"({ANSWERABLE_TOTAL} answerable, {UNANSWERABLE_TOTAL} unanswerable)\")\n",
    "\n",
    "    return db\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONVERSION / DISPLAY / SAVE â€” SAME AS ORIGINAL\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def qa_to_dataframe(qa_list: List[Dict]) -> pd.DataFrame:\n",
    "    df = pd.DataFrame(qa_list)\n",
    "    df[\"question_length\"] = df.question.str.len()\n",
    "    df[\"answer_length\"] = df.answer.str.len()\n",
    "    df[\"word_count\"] = df.answer.str.split().str.len()\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_qa_to_csv(qa_list, filename=\"qa_database.csv\"):\n",
    "    df = pd.DataFrame(qa_list)\n",
    "    path = os.path.join(OUTPUT_DIR, filename)\n",
    "    df.to_csv(path, index=False)\n",
    "    print(f\"   ğŸ’¾ Saved to {path}\")\n",
    "    return path\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# VERIFICATION FUNCTION\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def verify_objective2():\n",
    "    \"\"\"\n",
    "    Verify that Objective 2 completed successfully.\n",
    "    Checks all variables, counts, structure, and files.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"ğŸ” OBJECTIVE 2 VERIFICATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    errors = []\n",
    "    warnings = []\n",
    "    \n",
    "    # Check if variables exist\n",
    "    if 'qa_database' not in globals():\n",
    "        errors.append(\"âŒ qa_database not found\")\n",
    "    if 'qa_df' not in globals():\n",
    "        errors.append(\"âŒ qa_df not found\")\n",
    "    \n",
    "    if errors:\n",
    "        print(\"\\n\".join(errors))\n",
    "        print(\"=\"*70)\n",
    "        return False\n",
    "    \n",
    "    # Check count\n",
    "    actual_count = len(qa_database)\n",
    "    expected_count = TOTAL_PAIRS\n",
    "    \n",
    "    if actual_count != expected_count:\n",
    "        errors.append(f\"âŒ Wrong count: Expected {expected_count}, got {actual_count}\")\n",
    "    else:\n",
    "        print(f\"âœ… Count correct: {actual_count} pairs\")\n",
    "    \n",
    "    # Check structure\n",
    "    required_keys = [\"question\", \"answer\", \"answerable\", \"category\"]\n",
    "    for i, pair in enumerate(qa_database):\n",
    "        for key in required_keys:\n",
    "            if key not in pair:\n",
    "                errors.append(f\"âŒ Pair {i+1} missing key: {key}\")\n",
    "            elif pair[key] is None:\n",
    "                errors.append(f\"âŒ Pair {i+1} has None value for {key}\")\n",
    "            elif isinstance(pair[key], str) and len(pair[key].strip()) == 0:\n",
    "                errors.append(f\"âŒ Pair {i+1} has empty {key}\")\n",
    "            # Note: For boolean fields like \"answerable\", False is a valid value, not empty\n",
    "    \n",
    "    if not errors:\n",
    "        print(f\"âœ… Structure correct: All pairs have required keys\")\n",
    "    \n",
    "    # Check distribution\n",
    "    answerable_count = sum(1 for p in qa_database if p.get(\"answerable\") == True)\n",
    "    unanswerable_count = sum(1 for p in qa_database if p.get(\"answerable\") == False)\n",
    "    \n",
    "    if answerable_count != ANSWERABLE_TOTAL:\n",
    "        warnings.append(f\"âš ï¸  Answerable count: Expected {ANSWERABLE_TOTAL}, got {answerable_count}\")\n",
    "    else:\n",
    "        print(f\"âœ… Answerable pairs: {answerable_count}\")\n",
    "    \n",
    "    if unanswerable_count != UNANSWERABLE_TOTAL:\n",
    "        warnings.append(f\"âš ï¸  Unanswerable count: Expected {UNANSWERABLE_TOTAL}, got {unanswerable_count}\")\n",
    "    else:\n",
    "        print(f\"âœ… Unanswerable pairs: {unanswerable_count}\")\n",
    "    \n",
    "    # Check DataFrame\n",
    "    if len(qa_df) != actual_count:\n",
    "        errors.append(f\"âŒ DataFrame count mismatch: {len(qa_df)} != {actual_count}\")\n",
    "    else:\n",
    "        print(f\"âœ… DataFrame correct: {len(qa_df)} rows\")\n",
    "    \n",
    "    # Check file exists\n",
    "    csv_path = os.path.join(OUTPUT_DIR, \"qa_database.csv\")\n",
    "    if not os.path.exists(csv_path):\n",
    "        errors.append(f\"âŒ CSV file not found: {csv_path}\")\n",
    "    else:\n",
    "        print(f\"âœ… CSV file exists: {csv_path}\")\n",
    "        # Verify CSV content\n",
    "        try:\n",
    "            df_check = pd.read_csv(csv_path)\n",
    "            if len(df_check) != actual_count:\n",
    "                warnings.append(f\"âš ï¸  CSV row count: {len(df_check)} != {actual_count}\")\n",
    "        except Exception as e:\n",
    "            warnings.append(f\"âš ï¸  Could not verify CSV: {e}\")\n",
    "    \n",
    "    # Print results\n",
    "    if errors:\n",
    "        print(\"\\nâŒ VERIFICATION FAILED:\")\n",
    "        print(\"\\n\".join(errors))\n",
    "        if warnings:\n",
    "            print(\"\\nâš ï¸  WARNINGS:\")\n",
    "            print(\"\\n\".join(warnings))\n",
    "        print(\"=\"*70)\n",
    "        return False\n",
    "    else:\n",
    "        print(\"\\nâœ… Objective 2 Complete - All checks passed!\")\n",
    "        if warnings:\n",
    "            print(\"\\nâš ï¸  WARNINGS:\")\n",
    "            print(\"\\n\".join(warnings))\n",
    "        print(f\"   â€¢ Total Q&A pairs: {actual_count}\")\n",
    "        print(f\"   â€¢ Answerable: {answerable_count}\")\n",
    "        print(f\"   â€¢ Unanswerable: {unanswerable_count}\")\n",
    "        print(f\"   â€¢ DataFrame: {len(qa_df)} rows Ã— {len(qa_df.columns)} columns\")\n",
    "        print(f\"   â€¢ CSV file: {csv_path}\")\n",
    "        print(\"=\"*70)\n",
    "        return True\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# DISPLAY FUNCTION\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def display_qa_database(qa_list: List[Dict], max_display: int = None):\n",
    "    \"\"\"Display Q&A pairs in a readable format.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ“‹ GENERATED Q&A DATABASE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if max_display:\n",
    "        display_list = qa_list[:max_display]\n",
    "        print(f\"\\nShowing first {len(display_list)} of {len(qa_list)} pairs:\\n\")\n",
    "    else:\n",
    "        display_list = qa_list\n",
    "        print(f\"\\nAll {len(display_list)} pairs:\\n\")\n",
    "    \n",
    "    for i, pair in enumerate(display_list, 1):\n",
    "        answerable_str = \"âœ… Answerable\" if pair.get(\"answerable\") else \"âŒ Unanswerable\"\n",
    "        category = pair.get(\"category\", \"unknown\")\n",
    "        print(f\"\\n[{i}] {answerable_str} | Category: {category}\")\n",
    "        print(f\"    Q: {pair.get('question', 'N/A')}\")\n",
    "        print(f\"    A: {pair.get('answer', 'N/A')[:150]}{'...' if len(pair.get('answer', '')) > 150 else ''}\")\n",
    "    \n",
    "    if max_display and len(qa_list) > max_display:\n",
    "        print(f\"\\n... and {len(qa_list) - max_display} more pairs\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# EXECUTION ENTRYPOINT\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"OBJECTIVE 2 (REVISED): GENERATE CUSTOM Q&A DATABASE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "validate_prerequisites()\n",
    "\n",
    "print(\"\\nğŸ¤– Generating Q&A with strict Mistral prompts...\")\n",
    "qa_database = generate_full_qa_database()\n",
    "qa_df = qa_to_dataframe(qa_database)\n",
    "\n",
    "save_qa_to_csv(qa_database)\n",
    "\n",
    "# Display the generated Q&A pairs\n",
    "display_qa_database(qa_database)\n",
    "\n",
    "# Verify the results\n",
    "print(\"\\n\")\n",
    "verify_objective2()\n",
    "\n",
    "print(\"\\nDone! Q&A database is ready.\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective 3: Implement Vector Databases Using FAISS\n",
    "\n",
    "### ğŸ¯ Goal\n",
    "Build a FAISS vector database from the Q&A pairs to enable fast semantic search, converting text to embeddings and creating an index for efficient similarity retrieval in the RAG system.\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ“¥ Prerequisites</b> (Click to expand)</summary>\n",
    "\n",
    "| Item | Source | Required | Description |\n",
    "|------|--------|----------|-------------|\n",
    "| `qa_database` | Objective 2 | âœ… Yes | List of 21 Q&A pairs to convert to embeddings |\n",
    "| `system_prompt` | Objective 1 | âœ… Yes | System prompt (used for validation) |\n",
    "| Python packages | Setup cell | âœ… Yes | `faiss-cpu`, `sentence-transformers`, `numpy`, `pandas` |\n",
    "\n",
    "**Note:** Objective 2 must be completed first to provide the Q&A database.\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ”§ Core Concepts</b> (Click to expand)</summary>\n",
    "\n",
    "| Concept | Description |\n",
    "|--------|-------------|\n",
    "| **Embeddings** | Numerical vector representations of text that capture semantic meaning. Similar texts have similar embeddings (close in vector space) |\n",
    "| **Semantic Search** | Finding relevant documents by meaning rather than exact keyword matching. Enables finding \"shipping\" when querying \"delivery\" |\n",
    "| **FAISS (Facebook AI Similarity Search)** | Library for efficient similarity search in high-dimensional vector spaces. Searches millions of vectors in milliseconds |\n",
    "| **IndexFlatL2** | FAISS index type using L2 (Euclidean) distance for exact similarity search. Ideal for small-medium datasets (<100k vectors) |\n",
    "| **Sentence Transformers** | Pre-trained models that convert text to dense vector embeddings optimized for semantic similarity |\n",
    "| **Top-K Retrieval** | Retrieving the k most similar documents (e.g., top-3) based on embedding similarity scores |\n",
    "\n",
    "**Why This Matters:**\n",
    "Vector databases enable semantic search - finding relevant information even when exact keywords don't match. This is essential for RAG systems where user questions need to retrieve the most semantically similar context from the knowledge base.\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ“Š Design Choices</b> (Click to expand)</summary>\n",
    "\n",
    "| Choice | Selected | Rationale |\n",
    "|--------|----------|-----------|\n",
    "| **Embedding Model** | `all-MiniLM-L6-v2` | 384 dimensions, fast inference, good quality for small-medium datasets |\n",
    "| **FAISS Index Type** | IndexFlatL2 | Exact search with L2 distance, ideal for 21 Q&A pairs, no approximation needed |\n",
    "| **Embedding Strategy** | Question + Answer combined | Richer semantic representation by including both question and answer text |\n",
    "| **Top-K Retrieval** | 3 documents | Balances context richness with prompt length, sufficient for most queries |\n",
    "| **Distance Metric** | L2 (Euclidean) | Standard similarity measure, lower distance = more similar vectors |\n",
    "| **Data Type** | float32 | FAISS requirement, balances precision and memory usage |\n",
    "\n",
    "**Why This Approach:**\n",
    "- **all-MiniLM-L6-v2**: Lightweight, fast, sufficient quality for 21 pairs. Alternative models (e.g., all-mpnet-base-v2) offer higher accuracy but slower inference\n",
    "- **IndexFlatL2**: Exact search ensures highest quality results. For larger datasets (>100k), IndexIVFFlat or IndexHNSW would be better for speed\n",
    "- **Combined Q&A**: Including both question and answer in embeddings captures full semantic context, improving retrieval accuracy\n",
    "- **Top-3**: Provides enough context for LLM while keeping prompts manageable\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ”„ Process Flow</b> (Click to expand)</summary>\n",
    "\n",
    "**Left-to-Right Flow:**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Validate    â”‚â”€â”€â”€â–¶â”‚  Load        â”‚â”€â”€â”€â–¶â”‚  Generate    â”‚â”€â”€â”€â–¶â”‚  Create     â”‚â”€â”€â”€â–¶â”‚  Test       â”‚\n",
    "â”‚ Prerequisitesâ”‚    â”‚  Embedding   â”‚    â”‚  Embeddings â”‚    â”‚  FAISS      â”‚    â”‚  Retrieval  â”‚\n",
    "â”‚              â”‚    â”‚  Model       â”‚    â”‚  (384-dim)  â”‚    â”‚  Index      â”‚    â”‚             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                                                                        â”‚\n",
    "                                                                                        â–¼\n",
    "                                                                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                                                                                â”‚  Save Files  â”‚\n",
    "                                                                                â”‚  (Index +    â”‚\n",
    "                                                                                â”‚  Embeddings) â”‚\n",
    "                                                                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Detailed Steps:**\n",
    "1. **Validate Prerequisites**: Ensure Objective 2 completed (qa_database available with 21 pairs)\n",
    "2. **Load Embedding Model**: Download and load sentence-transformers model (`all-MiniLM-L6-v2`, 384 dimensions)\n",
    "3. **Generate Embeddings**: Convert each Q&A pair (question + answer) to 384-dimensional vector\n",
    "4. **Create FAISS Index**: Build IndexFlatL2 index and add all 21 embeddings\n",
    "5. **Test Retrieval**: Query the index with sample questions to verify semantic search works\n",
    "6. **Save Files**: Persist embeddings and FAISS index to disk for reuse\n",
    "\n",
    "**Embedding Process:**\n",
    "- Input: Q&A text (e.g., \"What is your shipping policy? We offer standard shipping...\")\n",
    "- Model: sentence-transformers/all-MiniLM-L6-v2\n",
    "- Output: 384-dimensional float32 vector\n",
    "- Result: 21 vectors (one per Q&A pair) ready for indexing\n",
    "\n",
    "**FAISS Index Creation:**\n",
    "- Dimension: 384 (matches embedding model output)\n",
    "- Type: IndexFlatL2 (exact search, L2 distance)\n",
    "- Vectors: 21 (one per Q&A pair)\n",
    "- Search Speed: Milliseconds for similarity queries\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ“¤ Outputs</b> (Click to expand)</summary>\n",
    "\n",
    "| Variable | Type | Description |\n",
    "|----------|------|-------------|\n",
    "| `embedding_model` | `SentenceTransformer` | Loaded sentence-transformers model (all-MiniLM-L6-v2) |\n",
    "| `qa_embeddings` | `np.ndarray` | Embedding vectors for all Q&A pairs, shape (21, 384) |\n",
    "| `faiss_index` | `faiss.IndexFlatL2` | FAISS index with 21 vectors indexed, ready for similarity search |\n",
    "| `embed_query()` | `function` | Function to convert query text to embedding vector |\n",
    "\n",
    "**Files Created:**\n",
    "| File | Location | Description |\n",
    "|------|----------|-------------|\n",
    "| `qa_embeddings.npy` | `data/vector_database/` | NumPy array of all Q&A embeddings (21 Ã— 384) |\n",
    "| `qa_index.faiss` | `data/vector_database/` | Serialized FAISS index for persistence |\n",
    "| `retrieval_test_results.csv` | `data/vector_database/` | Test query results with similarity scores |\n",
    "\n",
    "**Key Functions Created:**\n",
    "- `load_embedding_model()`: Loads sentence-transformers model\n",
    "- `generate_embeddings()`: Converts text list to embedding array\n",
    "- `create_faiss_index()`: Builds FAISS IndexFlatL2 from embeddings\n",
    "- `search_index()`: Finds top-k similar vectors by L2 distance\n",
    "- `retrieve_context()`: Complete RAG retrieval - query to relevant Q&A pairs\n",
    "- `embed_query()`: Converts query text to embedding vector\n",
    "- `format_context_for_llm()`: Formats retrieved Q&A pairs as context string\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ“‹ FAISS Index Details</b> (Click to expand)</summary>\n",
    "\n",
    "**Index Type: IndexFlatL2**\n",
    "\n",
    "| Property | Value | Description |\n",
    "|----------|-------|-------------|\n",
    "| **Distance Metric** | L2 (Euclidean) | Lower distance = more similar vectors |\n",
    "| **Search Type** | Exact | No approximation, highest quality results |\n",
    "| **Vectors Indexed** | 21 | One per Q&A pair |\n",
    "| **Embedding Dimension** | 384 | Matches all-MiniLM-L6-v2 output |\n",
    "| **Search Speed** | Milliseconds | Fast for small-medium datasets |\n",
    "| **Scalability** | <100k vectors | For larger datasets, use IndexIVFFlat or IndexHNSW |\n",
    "\n",
    "**Why IndexFlatL2:**\n",
    "- **Exact Search**: No approximation means highest quality results\n",
    "- **Simple**: Easy to implement and understand\n",
    "- **Sufficient**: Perfect for 21 Q&A pairs\n",
    "- **Fast Enough**: Millisecond search times for our dataset size\n",
    "\n",
    "**Alternative Index Types (for reference):**\n",
    "- **IndexIVFFlat**: Approximate search, faster for large datasets (>100k)\n",
    "- **IndexHNSW**: Graph-based, very fast approximate search for very large datasets\n",
    "- **IndexFlatIP**: Inner product (cosine similarity) instead of L2 distance\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>â–¶ï¸ How to Run</b> (Click to expand)</summary>\n",
    "\n",
    "1. **Ensure Prerequisites Complete**: \n",
    "   - Run Objective 0 (Setup) first\n",
    "   - Run Objective 1 (System Prompt)\n",
    "   - Run Objective 2 (Q&A Database) - must complete successfully\n",
    "   - Verify `qa_database` exists with 21 pairs\n",
    "\n",
    "2. **Run the Code Cell**: \n",
    "   - Execute the Objective 3 code cell\n",
    "   - Embedding model download: ~90MB (first run only)\n",
    "   - Embedding generation: <30 seconds for 21 pairs\n",
    "   - FAISS index creation: <1 second\n",
    "\n",
    "3. **Verify Output**: \n",
    "   - Check console for: \"âœ… FAISS index created\"\n",
    "   - Verify embeddings shape: (21, 384)\n",
    "   - Check index vectors: 21 indexed\n",
    "   - Verify test retrieval works\n",
    "   - Check files saved: `data/vector_database/qa_embeddings.npy`, `qa_index.faiss`\n",
    "\n",
    "**Expected Runtime:**\n",
    "- First run: 30-60 seconds (model download + embedding generation)\n",
    "- Subsequent runs: 10-20 seconds (model cached, embeddings regenerated)\n",
    "\n",
    "**Troubleshooting:**\n",
    "- If embedding generation fails, check that Objective 2 completed successfully\n",
    "- If FAISS index creation fails, verify embeddings are float32 dtype\n",
    "- If retrieval returns no results, check that index was created with correct dimension\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>âœ… Verification</b> (Click to expand)</summary>\n",
    "\n",
    "After running, use the verification function to check all requirements:\n",
    "\n",
    "```python\n",
    "# Call the verification function (created in Objective 3 code cell)\n",
    "verify_objective3()\n",
    "```\n",
    "\n",
    "**What the Function Checks:**\n",
    "- âœ… Variables exist: `embedding_model`, `qa_embeddings`, `faiss_index`, `embed_query`\n",
    "- âœ… Embeddings shape: (21, 384) - correct number of pairs and dimensions\n",
    "- âœ… FAISS index: 21 vectors indexed, correct dimension (384)\n",
    "- âœ… Files exist: `qa_embeddings.npy`, `qa_index.faiss` saved successfully\n",
    "- âœ… Retrieval function: `embed_query()` works correctly\n",
    "\n",
    "**Function Output:**\n",
    "The function will print a summary with:\n",
    "- Embedding model information\n",
    "- Embeddings shape and count\n",
    "- FAISS index details (vectors indexed, dimension)\n",
    "- File locations confirmation\n",
    "\n",
    "**Note:** The `verify_objective3()` function is automatically created when you run the Objective 3 code cell. It performs all verification checks and provides a clear success/failure report.\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ”— Dependencies</b> (Click to expand)</summary>\n",
    "\n",
    "**This Objective:**\n",
    "- âœ… Requires Objective 0 (Setup) for packages\n",
    "- âœ… Requires Objective 1 (System Prompt) for validation\n",
    "- âœ… Requires Objective 2 (Q&A Database) for `qa_database` (21 pairs)\n",
    "\n",
    "**Used By:**\n",
    "- Objective 4: Uses `faiss_index` and `embed_query()` for RAG retrieval\n",
    "- Objective 5: Uses `faiss_index` for model evaluation with RAGAS\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ“š Learning Objectives Demonstrated</b> (Click to expand)</summary>\n",
    "\n",
    "1. **Text Embeddings**: Converting text to numerical vectors that capture semantic meaning\n",
    "2. **Semantic Search**: Finding relevant documents by meaning rather than keyword matching\n",
    "3. **Vector Databases**: Using FAISS for efficient similarity search in high-dimensional spaces\n",
    "4. **Index Design**: Choosing appropriate index types (IndexFlatL2) for dataset size\n",
    "5. **RAG Retrieval**: Implementing the retrieval component of RAG systems\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ’¡ Tips</b> (Click to expand)</summary>\n",
    "\n",
    "- **Embedding Model**: all-MiniLM-L6-v2 is fast and sufficient for 21 pairs. For larger datasets, consider all-mpnet-base-v2 for better quality\n",
    "- **Combined Q&A**: Including both question and answer in embeddings improves retrieval accuracy\n",
    "- **FAISS Index Type**: IndexFlatL2 is perfect for small datasets. For >100k vectors, use IndexIVFFlat for speed\n",
    "- **Top-K Selection**: Top-3 provides good context balance. Adjust based on your use case\n",
    "- **Float32 Requirement**: FAISS requires float32 dtype - embeddings are automatically converted\n",
    "- **Index Persistence**: Saved index can be loaded later without regenerating embeddings\n",
    "- **Search Speed**: FAISS searches are extremely fast (milliseconds) even for larger datasets\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Next Step:** Proceed to Objective 4 to build the complete RAG pipeline using the FAISS index.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OBJECTIVE 3: IMPLEMENT VECTOR DATABASES USING FAISS\n",
    "# ============================================================================\n",
    "#\n",
    "# LEARNING OBJECTIVES DEMONSTRATED:\n",
    "#   1. Text Embeddings - Converting text to numerical vectors that capture semantic meaning\n",
    "#   2. Semantic Search - Finding relevant documents by meaning rather than keyword matching\n",
    "#   3. Vector Databases - Using FAISS for efficient similarity search in high-dimensional spaces\n",
    "#   4. Index Design - Choosing appropriate index types (IndexFlatL2) for dataset size\n",
    "#   5. RAG Retrieval - Implementing the retrieval component of RAG systems\n",
    "#\n",
    "# PREREQUISITES: Run Objective 1 and Objective 2 first\n",
    "#   - system_prompt (from Objective 1) - for validation\n",
    "#   - qa_database (from Objective 2) - 21 Q&A pairs to convert to embeddings\n",
    "#\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 1: IMPORTS & VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import faiss\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "except ImportError as e:\n",
    "    raise ImportError(f\"Missing: {e}. Run: pip install faiss-cpu sentence-transformers numpy pandas\")\n",
    "\n",
    "\n",
    "def validate_prerequisites():\n",
    "    \"\"\"Ensure Objective 1 and 2 were run first.\"\"\"\n",
    "    required = ['system_prompt', 'qa_database']\n",
    "    missing = [r for r in required if r not in globals()]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"Missing: {missing}. Run Objective 1 and 2 first.\")\n",
    "    print(\"âœ… Prerequisites validated\")\n",
    "    print(f\"   â€¢ System prompt: {len(globals()['system_prompt'])} chars\")\n",
    "    print(f\"   â€¢ Q&A database: {len(globals()['qa_database'])} pairs\")\n",
    "    return True\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 2: CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Output directory for FAISS index and embeddings\n",
    "OUTPUT_DIR = \"data/vector_database\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Embedding model - lightweight and efficient\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# Retrieval settings\n",
    "TOP_K = 3  # Number of similar documents to retrieve\n",
    "\n",
    "# Expected Q&A count (from Objective 2)\n",
    "EXPECTED_QA_COUNT = 21  # Total Q&A pairs: 18 answerable + 3 unanswerable\n",
    "EMBEDDING_DIM = 384  # Dimension for all-MiniLM-L6-v2 model\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 3: EMBEDDING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def load_embedding_model(model_name: str = EMBEDDING_MODEL) -> SentenceTransformer:\n",
    "    \"\"\"\n",
    "    Load sentence transformer model for generating embeddings.\n",
    "    \n",
    "    Model: all-MiniLM-L6-v2\n",
    "    - Dimensions: 384\n",
    "    - Speed: Fast (good for real-time applications)\n",
    "    - Quality: Good semantic understanding\n",
    "    - Memory: Low (suitable for CPU-based environments)\n",
    "    \n",
    "    Returns:\n",
    "        SentenceTransformer model ready for encoding\n",
    "    \"\"\"\n",
    "    print(f\"   Loading embedding model: {model_name}\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "    print(f\"   âœ… Model loaded (embedding dim: {model.get_sentence_embedding_dimension()})\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def generate_embeddings(texts: List[str], model: SentenceTransformer) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    if not texts:\n",
    "        raise ValueError(\"Texts list cannot be empty\")\n",
    "    \n",
    "    Generate embeddings for a list of texts.\n",
    "    \n",
    "    Converts text to 384-dimensional float32 vectors using sentence-transformers.\n",
    "    These embeddings capture semantic meaning, enabling similarity search.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of strings to embed\n",
    "        model: SentenceTransformer model\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of shape (n_texts, 384) with float32 dtype\n",
    "    \"\"\"\n",
    "    # Encode all texts - model handles batching internally\n",
    "    embeddings = model.encode(texts, show_progress_bar=True, convert_to_numpy=True)\n",
    "    \n",
    "    # FAISS requires float32 dtype\n",
    "    return embeddings.astype('float32')\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 4: FAISS INDEX FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def create_faiss_index(embeddings: np.ndarray) -> faiss.IndexFlatL2:\n",
    "    \"\"\"\n",
    "    Create FAISS index from embeddings.\n",
    "    \n",
    "    Uses IndexFlatL2 for exact search with L2 (Euclidean) distance.\n",
    "    Ideal for small-medium datasets (<100k vectors).\n",
    "    \n",
    "    Args:\n",
    "        embeddings: numpy array of shape (n_vectors, embedding_dim)\n",
    "    \n",
    "    Returns:\n",
    "        FAISS IndexFlatL2 index with all vectors indexed\n",
    "    \"\"\"\n",
    "    # Step 1: Get embedding dimension\n",
    "    if embeddings.size == 0:\n",
    "        raise ValueError(\"Embeddings array cannot be empty\")\n",
    "    \n",
    "    if len(embeddings.shape) != 2:\n",
    "        raise ValueError(f\"Embeddings must be 2D array, got shape {embeddings.shape}\")\n",
    "    \n",
    "    if embeddings.dtype != np.float32:\n",
    "        raise TypeError(f\"Embeddings must be float32, got {embeddings.dtype}\")\n",
    "    \n",
    "    dimension = embeddings.shape[1]\n",
    "    \n",
    "    # Step 2: Create FAISS index\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    \n",
    "    # Step 3: Add vectors to index\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    # Step 4: Return the populated index\n",
    "    return index\n",
    "\n",
    "\n",
    "def search_index(\n",
    "    query_embedding: np.ndarray,\n",
    "    index: faiss.IndexFlatL2,\n",
    "    top_k: int = TOP_K\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Search FAISS index for most similar vectors.\n",
    "    \n",
    "    Args:\n",
    "        query_embedding: Query vector (1, embedding_dim)\n",
    "        index: FAISS index\n",
    "        top_k: Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        distances: Distance scores (lower = more similar for L2)\n",
    "        indices: Indices of matched documents\n",
    "    \"\"\"\n",
    "    if len(query_embedding.shape) == 1:\n",
    "        query_embedding = query_embedding.reshape(1, -1)\n",
    "    \n",
    "    # Validate dimension match\n",
    "    if query_embedding.shape[1] != index.d:\n",
    "        raise ValueError(f\"Query embedding dimension {query_embedding.shape[1]} doesn't match index dimension {index.d}\")\n",
    "    \n",
    "    \n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    return distances[0], indices[0]\n",
    "\n",
    "\n",
    "def retrieve_context(\n",
    "    query: str,\n",
    "    model: SentenceTransformer,\n",
    "    index: faiss.IndexFlatL2,\n",
    "    qa_database: List[Dict],\n",
    "    top_k: int = TOP_K\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve most relevant Q&A pairs for a query.\n",
    "    \n",
    "    This is the core RAG retrieval function:\n",
    "    1. Convert query to embedding\n",
    "    2. Search FAISS index for similar embeddings\n",
    "    3. Return corresponding Q&A pairs as context\n",
    "    \n",
    "    Args:\n",
    "        query: User's question\n",
    "        model: Embedding model\n",
    "        index: FAISS index\n",
    "        qa_database: Original Q&A database\n",
    "        top_k: Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of relevant Q&A pairs with distance scores\n",
    "    \"\"\"\n",
    "    if not query or not query.strip():\n",
    "        raise ValueError(\"Query cannot be empty\")\n",
    "    \n",
    "    if not qa_database:\n",
    "        raise ValueError(\"Q&A database cannot be empty\")\n",
    "    \n",
    "    # Generate query embedding\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True).astype('float32')\n",
    "    \n",
    "    # Search index\n",
    "    distances, indices = search_index(query_embedding, index, top_k)\n",
    "    \n",
    "    # Get corresponding Q&A pairs\n",
    "    results = []\n",
    "    for dist, idx in zip(distances, indices):\n",
    "        if idx < len(qa_database):  # Safety check\n",
    "            qa = qa_database[idx].copy()\n",
    "            qa['distance'] = float(dist)\n",
    "            qa['similarity_score'] = 1 / (1 + float(dist))  # Convert distance to similarity\n",
    "            results.append(qa)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def format_context_for_llm(retrieved_qa: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Format retrieved Q&A pairs as context for LLM.\n",
    "    \n",
    "    This context will be injected into the prompt for RAG.\n",
    "    \"\"\"\n",
    "    if not retrieved_qa:\n",
    "        return \"No relevant information found in knowledge base.\"\n",
    "    \n",
    "    context_parts = []\n",
    "    for i, qa in enumerate(retrieved_qa, 1):\n",
    "        context_parts.append(f\"[Context {i}]\")\n",
    "        context_parts.append(f\"Q: {qa['question']}\")\n",
    "        context_parts.append(f\"A: {qa['answer']}\")\n",
    "        context_parts.append(\"\")\n",
    "    \n",
    "    return \"\\n\".join(context_parts)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 5: DISPLAY & STORAGE FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def display_retrieval_results(query: str, results: List[Dict]):\n",
    "    \"\"\"Display retrieval results in a formatted way.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"ğŸ” RETRIEVAL RESULTS FOR: \\\"{query}\\\"\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for i, qa in enumerate(results, 1):\n",
    "        similarity = qa.get('similarity_score', 0) * 100\n",
    "        answerable = 'ğŸ“—' if qa.get('answerable', True) else 'ğŸ“•'\n",
    "        \n",
    "        print(f\"\\n{answerable} Result {i} (Similarity: {similarity:.1f}%)\")\n",
    "        print(f\"   Category: {qa.get('category', 'N/A')}\")\n",
    "        print(f\"   Question: {qa.get('question', 'N/A')}\")\n",
    "        print(f\"   Answer: {qa.get('answer', 'N/A')[:100]}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "\n",
    "def save_embeddings(embeddings: np.ndarray, filename: str = \"qa_embeddings.npy\"):\n",
    "    \"\"\"Save embeddings to numpy file.\"\"\"\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    np.save(filepath, embeddings)\n",
    "    print(f\"âœ… Embeddings saved to: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def save_faiss_index(index: faiss.IndexFlatL2, filename: str = \"qa_index.faiss\"):\n",
    "    \"\"\"Save FAISS index to file.\"\"\"\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    faiss.write_index(index, filepath)\n",
    "    print(f\"âœ… FAISS index saved to: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def save_retrieval_test_results(test_results: List[Dict], filename: str = \"retrieval_test_results.csv\"):\n",
    "    \"\"\"Save retrieval test results to CSV.\"\"\"\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    \n",
    "    # Flatten results for CSV\n",
    "    rows = []\n",
    "    for result in test_results:\n",
    "        for i, retrieved in enumerate(result['retrieved'], 1):\n",
    "            rows.append({\n",
    "                'query': result['query'],\n",
    "                'rank': i,\n",
    "                'category': retrieved.get('category', ''),\n",
    "                'answerable': retrieved.get('answerable', True),\n",
    "                'question': retrieved.get('question', ''),\n",
    "                'answer': retrieved.get('answer', '')[:200],\n",
    "                'similarity_score': retrieved.get('similarity_score', 0)\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(filepath, index=False)\n",
    "    print(f\"âœ… Retrieval test results saved to: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 6: VERIFICATION FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def verify_objective3():\n",
    "    \"\"\"\n",
    "    Verify that Objective 3 completed successfully.\n",
    "    Checks all variables, embeddings, FAISS index, and files.\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"ğŸ” OBJECTIVE 3 VERIFICATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    errors = []\n",
    "    \n",
    "    # Check if variables exist\n",
    "    if 'embedding_model' not in globals():\n",
    "        errors.append(\"âŒ embedding_model not found\")\n",
    "    if 'qa_embeddings' not in globals():\n",
    "        errors.append(\"âŒ qa_embeddings not found\")\n",
    "    if 'faiss_index' not in globals():\n",
    "        errors.append(\"âŒ faiss_index not found\")\n",
    "    if 'embed_query' not in globals():\n",
    "        errors.append(\"âŒ embed_query function not found\")\n",
    "    \n",
    "    if errors:\n",
    "        print(\"\\n\".join(errors))\n",
    "        print(\"=\"*70)\n",
    "        return False\n",
    "    \n",
    "    # Check embeddings shape\n",
    "    if qa_embeddings.shape[0] != EXPECTED_QA_COUNT:\n",
    "        errors.append(f\"âŒ Expected EXPECTED_QA_COUNT embeddings, got {qa_embeddings.shape[0]}\")\n",
    "    if qa_embeddings.shape[1] != EMBEDDING_DIM:\n",
    "        errors.append(f\"âŒ Expected EMBEDDING_DIM dimensions, got {qa_embeddings.shape[1]}\")\n",
    "    if qa_embeddings.dtype != 'float32':\n",
    "        errors.append(f\"âŒ Expected float32 dtype, got {qa_embeddings.dtype}\")\n",
    "    \n",
    "    # Check FAISS index\n",
    "    if faiss_index.ntotal != EXPECTED_QA_COUNT:\n",
    "        errors.append(f\"âŒ Expected EXPECTED_QA_COUNT vectors in index, got {faiss_index.ntotal}\")\n",
    "    if faiss_index.d != EMBEDDING_DIM:\n",
    "        errors.append(f\"âŒ Expected EMBEDDING_DIM dimensions in index, got {faiss_index.d}\")\n",
    "    \n",
    "    # Check files exist\n",
    "    if not os.path.exists(\"data/vector_database/qa_embeddings.npy\"):\n",
    "        errors.append(\"âŒ qa_embeddings.npy not found\")\n",
    "    if not os.path.exists(\"data/vector_database/qa_index.faiss\"):\n",
    "        errors.append(\"âŒ qa_index.faiss not found\")\n",
    "    \n",
    "    # Test embed_query function\n",
    "    try:\n",
    "        test_embedding = embed_query(\"test query\")\n",
    "        if test_embedding.shape != (EMBEDDING_DIM,):\n",
    "            errors.append(f\"âŒ embed_query() returned wrong shape: {test_embedding.shape}\")\n",
    "    except Exception as e:\n",
    "        errors.append(f\"âŒ embed_query() test failed: {e}\")\n",
    "    \n",
    "    # Print results\n",
    "    if errors:\n",
    "        print(\"\\nâŒ VERIFICATION FAILED:\")\n",
    "        print(\"\\n\".join(errors))\n",
    "        print(\"=\"*70)\n",
    "        return False\n",
    "    else:\n",
    "        print(\"\\nâœ… Objective 3 Complete - All variables and files verified\")\n",
    "        print(f\"   â€¢ Embedding Model: {embedding_model.get_sentence_embedding_dimension()} dimensions\")\n",
    "        print(f\"   â€¢ Embeddings: {qa_embeddings.shape[0]} vectors Ã— {qa_embeddings.shape[1]} dimensions\")\n",
    "        print(f\"   â€¢ FAISS Index: {faiss_index.ntotal} vectors indexed\")\n",
    "        print(f\"   â€¢ embed_query(): Ready\")\n",
    "        print(f\"   â€¢ Files: Saved to data/vector_database/\")\n",
    "        print(\"=\"*70)\n",
    "        return True\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 7: EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"   OBJECTIVE 3: IMPLEMENT VECTOR DATABASE USING FAISS\")\n",
    "print(\"   Building Semantic Search for RAG System\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# --- Step 1: Validate Prerequisites ---\n",
    "print(\"\\nğŸ” STEP 1: Validate Prerequisites\")\n",
    "print(\"-\"*70)\n",
    "validate_prerequisites()\n",
    "\n",
    "qa_database = globals()['qa_database']\n",
    "\n",
    "# --- Step 2: Load Embedding Model ---\n",
    "print(\"\\nğŸ¤– STEP 2: Load Embedding Model\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "embedding_model = load_embedding_model()\n",
    "globals()['embedding_model'] = embedding_model\n",
    "\n",
    "# --- Step 3: Generate Embeddings for Q&A Database ---\n",
    "print(\"\\nğŸ“Š STEP 3: Generate Embeddings for Q&A Database\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Combine question and answer for richer embeddings\n",
    "qa_texts = [f\"{qa['question']} {qa['answer']}\" for qa in qa_database]\n",
    "print(f\"   Generating embeddings for {len(qa_texts)} Q&A pairs...\")\n",
    "\n",
    "qa_embeddings = generate_embeddings(qa_texts, embedding_model)\n",
    "globals()['qa_embeddings'] = qa_embeddings\n",
    "\n",
    "print(f\"   âœ… Embeddings shape: {qa_embeddings.shape}\")\n",
    "\n",
    "# --- Step 4: Create FAISS Index ---\n",
    "print(\"\\nğŸ—„ï¸  STEP 4: Create FAISS Index\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Create FAISS index and add embeddings\n",
    "faiss_index = create_faiss_index(qa_embeddings)\n",
    "globals()['faiss_index'] = faiss_index\n",
    "\n",
    "# Create embed_query function for RAG pipeline (used in Objective 4)\n",
    "def embed_query(query: str) -> np.ndarray:\n",
    "    \"\"\"Convert query text to embedding vector for FAISS search.\"\"\"\n",
    "    return embedding_model.encode([query], convert_to_numpy=True).astype('float32')[0]\n",
    "\n",
    "globals()['embed_query'] = embed_query\n",
    "\n",
    "print(f\"   âœ… FAISS index created\")\n",
    "print(f\"   â€¢ Index type: IndexFlatL2 (exact search)\")\n",
    "print(f\"   â€¢ Vectors indexed: {faiss_index.ntotal}\")\n",
    "print(f\"   â€¢ Embedding dimension: {qa_embeddings.shape[1]}\")\n",
    "\n",
    "# --- Step 5: Test Retrieval ---\n",
    "print(\"\\nğŸ§ª STEP 5: Test Retrieval with Sample Queries\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "test_queries = [\n",
    "    \"How long does shipping take?\",\n",
    "    \"Can I return a product?\",\n",
    "    \"What are your business hours?\",\n",
    "    \"Do you price match with competitors?\",  # Unanswerable\n",
    "]\n",
    "\n",
    "test_results = []\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n   Testing: \\\"{query}\\\"\")\n",
    "    \n",
    "    retrieved = retrieve_context(\n",
    "        query=query,\n",
    "        model=embedding_model,\n",
    "        index=faiss_index,\n",
    "        qa_database=qa_database,\n",
    "        top_k=TOP_K\n",
    "    )\n",
    "    \n",
    "    test_results.append({\n",
    "        'query': query,\n",
    "        'retrieved': retrieved\n",
    "    })\n",
    "    \n",
    "    display_retrieval_results(query, retrieved)\n",
    "\n",
    "# --- Step 6: Show Formatted Context for LLM ---\n",
    "print(\"\\nğŸ“ STEP 6: Example - Formatted Context for LLM\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "sample_query = \"What is your return policy?\"\n",
    "sample_retrieved = retrieve_context(sample_query, embedding_model, faiss_index, qa_database, TOP_K)\n",
    "formatted_context = format_context_for_llm(sample_retrieved)\n",
    "\n",
    "print(f\"Query: \\\"{sample_query}\\\"\\n\")\n",
    "print(\"Formatted Context for LLM:\")\n",
    "print(\"-\"*70)\n",
    "print(formatted_context)\n",
    "print(\"-\"*70)\n",
    "\n",
    "# --- Step 7: Save All Artifacts ---\n",
    "print(\"\\nğŸ’¾ STEP 7: Save Files to data/vector_database/\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "save_embeddings(qa_embeddings)\n",
    "save_faiss_index(faiss_index)\n",
    "save_retrieval_test_results(test_results)\n",
    "\n",
    "# --- Step 8: Verify Objective 3 ---\n",
    "print(\"\\nâœ… STEP 8: Verify Objective 3\")\n",
    "print(\"-\"*70)\n",
    "verify_objective3()\n",
    "\n",
    "# --- Summary ---\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… OBJECTIVE 3 COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\"\"\n",
    "Key Concepts Demonstrated:\n",
    "  1. Embeddings - Text to vector conversion using sentence-transformers\n",
    "  2. FAISS Index - Efficient similarity search with IndexFlatL2\n",
    "  3. RAG Retrieval - Finding relevant context for user queries\n",
    "\n",
    "ğŸ“¦ FILES SAVED (for submission):\n",
    "  â€¢ {OUTPUT_DIR}/qa_embeddings.npy - Embedding vectors\n",
    "  â€¢ {OUTPUT_DIR}/qa_index.faiss - FAISS index\n",
    "  â€¢ {OUTPUT_DIR}/retrieval_test_results.csv - Test results\n",
    "\n",
    "ğŸ“¦ GLOBAL VARIABLES:\n",
    "  â€¢ embedding_model: SentenceTransformer model\n",
    "  â€¢ qa_embeddings: numpy array ({qa_embeddings.shape})\n",
    "  â€¢ faiss_index: FAISS IndexFlatL2 ({faiss_index.ntotal} vectors)\n",
    "\n",
    "ğŸ”œ READY FOR OBJECTIVE 4: RAG Pipeline Integration\n",
    "\"\"\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective 4: RAG Pipeline Integration\n",
    "\n",
    "### ğŸ¯ Goal\n",
    "\n",
    "Build a complete Retrieval-Augmented Generation (RAG) pipeline that combines semantic search (FAISS) with the Mistral LLM to answer questions using the custom knowledge base. The pipeline will retrieve relevant context from the Q&A database and generate accurate, context-aware responses.\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ“¥ Prerequisites</b> (Click to expand)</summary>\n",
    "\n",
    "| Item | Source | Required | Description |\n",
    "|------|--------|----------|-------------|\n",
    "| `mistral_model`, `mistral_tokenizer` | Objective 1 | âœ… Yes | Mistral model for answer generation |\n",
    "| `system_prompt` | Objective 1 | âœ… Yes | System prompt for context-aware responses |\n",
    "| `qa_database` | Objective 2 | âœ… Yes | Q&A pairs as knowledge base |\n",
    "| `embedding_model` | Objective 3 | âœ… Yes | SentenceTransformer for query embeddings |\n",
    "| `faiss_index` | Objective 3 | âœ… Yes | FAISS index for semantic search |\n",
    "| `embed_query()` | Objective 3 | âœ… Yes | Function to convert text to embeddings |\n",
    "| GPU (recommended) | System | âš ï¸ Optional | Faster inference for Mistral model |\n",
    "\n",
    "**Note:** This objective requires all previous objectives (1-3) to be completed successfully.\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ”§ Core Concepts</b> (Click to expand)</summary>\n",
    "\n",
    "| Concept | Description |\n",
    "|--------|-------------|\n",
    "| **RAG Pipeline** | Complete flow: Query â†’ Embed â†’ Retrieve â†’ Augment â†’ Generate |\n",
    "| **Semantic Retrieval** | Using FAISS to find most relevant Q&A pairs based on embedding similarity |\n",
    "| **Context Augmentation** | Formatting retrieved documents as context for the LLM |\n",
    "| **Prompt Engineering** | Combining system prompt, retrieved context, and user question |\n",
    "| **Response Generation** | Using Mistral to generate answers based on retrieved context |\n",
    "\n",
    "**Why RAG:**\n",
    "RAG combines the accuracy of retrieval (finding exact relevant information) with the fluency of generation (natural language responses). This ensures answers are grounded in the knowledge base while maintaining conversational quality.\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ“Š Design Choices</b> (Click to expand)</summary>\n",
    "\n",
    "| Choice | Selected | Rationale |\n",
    "|--------|----------|-----------|\n",
    "| **Retrieval Method** | FAISS semantic search | Fast, accurate similarity search using embeddings |\n",
    "| **Top-K Retrieval** | 3 documents | Balances context richness with prompt length |\n",
    "| **Context Format** | Structured Q&A pairs | Clear, readable format for LLM processing |\n",
    "| **Generation Model** | Mistral-7B-Instruct | Same model from Objective 1, ensures consistency |\n",
    "| **Pipeline Functions** | Modular design | `rag_query()`, `search_faiss()`, `format_context()`, `build_prompt()` |\n",
    "\n",
    "**Why This Approach:**\n",
    "- **Modular Functions**: Easy to test, debug, and modify individual components\n",
    "- **Top-K = 3**: Sufficient context without overwhelming the prompt or introducing noise\n",
    "- **Structured Context**: Clear formatting helps LLM understand and use retrieved information\n",
    "- **Reuse Components**: Leverages all previous objectives for maximum efficiency\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ”„ Process Flow</b> (Click to expand)</summary>\n",
    "\n",
    "**Complete RAG Pipeline Flow:**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   User       â”‚â”€â”€â”€â–¶â”‚   Embed      â”‚â”€â”€â”€â–¶â”‚   Retrieve   â”‚â”€â”€â”€â–¶â”‚   Augment    â”‚â”€â”€â”€â–¶â”‚   Generate   â”‚\n",
    "â”‚   Question   â”‚    â”‚   Query      â”‚    â”‚   Context    â”‚    â”‚   Prompt     â”‚    â”‚   Answer     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                         â”‚                     â”‚                     â”‚                     â”‚\n",
    "                    embed_query()        search_faiss()      format_context()    generate_response()\n",
    "                    (from Obj 3)         (new function)      (new function)      (uses Obj 1)\n",
    "```\n",
    "\n",
    "**Detailed Steps:**\n",
    "1. **Query Embedding**: Convert user question to 384-dim vector using `embed_query()` from Objective 3\n",
    "2. **Semantic Search**: Use `search_faiss()` to find top-3 most similar Q&A pairs from knowledge base\n",
    "3. **Context Formatting**: Format retrieved Q&A pairs as readable context string\n",
    "4. **Prompt Building**: Combine system prompt, retrieved context, and user question\n",
    "5. **Answer Generation**: Use Mistral model to generate response based on augmented prompt\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ“¦ Outputs</b> (Click to expand)</summary>\n",
    "\n",
    "**Functions Created:**\n",
    "- `rag_query(query)` - Main pipeline function (query â†’ answer)\n",
    "- `search_faiss(embedding, top_k=3)` - FAISS similarity search\n",
    "- `format_context(retrieved_qa)` - Format Q&A pairs as context\n",
    "- `build_prompt(question, context)` - Combine system prompt + context + question\n",
    "- `generate_response(prompt)` - Generate answer using Mistral\n",
    "- `RAGResult` - Dataclass for pipeline results\n",
    "\n",
    "**Files Saved:**\n",
    "- `data/rag_pipeline/rag_test_results.csv` - Test query results\n",
    "- `data/rag_pipeline/pipeline_config.txt` - Pipeline configuration\n",
    "\n",
    "**Global Variables:**\n",
    "- `rag_query()` - Available for Objective 5 evaluation\n",
    "- `search_faiss()` - Available for Objective 5 evaluation\n",
    "- `format_context()` - Available for Objective 5 evaluation\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>âœ… Verification</b> (Click to expand)</summary>\n",
    "\n",
    "After completing Objective 4, verify:\n",
    "\n",
    "1. **Functions Exist:**\n",
    "   - âœ… `rag_query()` returns `RAGResult` with answer and retrieved context\n",
    "   - âœ… `search_faiss()` finds top-k similar documents\n",
    "   - âœ… `format_context()` formats Q&A pairs correctly\n",
    "   - âœ… `build_prompt()` combines all components\n",
    "\n",
    "2. **Test Results:**\n",
    "   - âœ… Answerable questions get relevant context\n",
    "   - âœ… Unanswerable questions handled appropriately\n",
    "   - âœ… Answers are context-aware and accurate\n",
    "\n",
    "3. **Files Created:**\n",
    "   - âœ… `data/rag_pipeline/rag_test_results.csv` exists\n",
    "   - âœ… `data/rag_pipeline/pipeline_config.txt` exists\n",
    "\n",
    "**Note:** The `verify_objective4()` function is automatically created when you run the Objective 4 code cell. It performs all verification checks and provides a clear success/failure report.\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ”— Dependencies</b> (Click to expand)</summary>\n",
    "\n",
    "**This Objective:**\n",
    "- âœ… Requires Objective 1 (System Prompt) for `mistral_model`, `mistral_tokenizer`, `system_prompt`\n",
    "- âœ… Requires Objective 2 (Q&A Database) for `qa_database`\n",
    "- âœ… Requires Objective 3 (Vector Database) for `embedding_model`, `faiss_index`, `embed_query()`\n",
    "\n",
    "**Used By:**\n",
    "- Objective 5: Uses `rag_query()`, `search_faiss()`, `format_context()` for model evaluation\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ“š Learning Objectives Demonstrated</b> (Click to expand)</summary>\n",
    "\n",
    "1. **RAG Pipeline Design**: Building end-to-end retrieval-augmented generation systems\n",
    "2. **Component Integration**: Combining multiple objectives into a unified pipeline\n",
    "3. **Semantic Search Application**: Using FAISS for real-world question answering\n",
    "4. **Context Management**: Formatting and managing retrieved context for LLM generation\n",
    "5. **Prompt Engineering**: Designing effective prompts that combine system instructions, context, and queries\n",
    "6. **Modular Architecture**: Creating reusable, testable pipeline components\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ’¡ Tips</b> (Click to expand)</summary>\n",
    "\n",
    "**Best Practices:**\n",
    "- Test with both answerable and unanswerable questions\n",
    "- Monitor retrieved context quality - if irrelevant, adjust top_k\n",
    "- Keep context formatting consistent for reliable LLM processing\n",
    "- Use the verification function to ensure all components work\n",
    "\n",
    "**Common Issues:**\n",
    "- **Irrelevant context**: Try adjusting top_k or check embedding quality\n",
    "- **Hallucinated answers**: Ensure context is properly formatted and included in prompt\n",
    "- **Slow generation**: Consider using GPU or reducing max_new_tokens\n",
    "\n",
    "**Performance Tips:**\n",
    "- Cache embeddings for frequently asked questions\n",
    "- Batch process multiple queries if evaluating many questions\n",
    "- Monitor token usage to stay within model limits\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OBJECTIVE 4: BUILD COMPLETE RAG PIPELINE\n",
    "# ============================================================================\n",
    "#\n",
    "# PIPELINE COMPONENTS:\n",
    "#   1. Query Processing - Convert user question to embedding\n",
    "#   2. Retrieval - Use FAISS to find top-k most similar Q&A pairs\n",
    "#   3. Augmentation - Combine user question with retrieved context\n",
    "#   4. Generation - Use Mistral to generate answer from augmented context\n",
    "#\n",
    "# WHY THIS ARCHITECTURE:\n",
    "#   - RAG combines retrieval (accurate, up-to-date info) with generation (natural responses)\n",
    "#   - Grounds answers in knowledge base, reducing hallucinations\n",
    "#   - Allows easy updates to knowledge base without retraining models\n",
    "#\n",
    "# 100% REUSE FROM PREVIOUS OBJECTIVES:\n",
    "#   - system_prompt, mistral_tokenizer, mistral_model (Objective 1)\n",
    "#   - qa_database (Objective 2)\n",
    "#   - embedding_model, faiss_index, embed_query (Objective 3)\n",
    "#\n",
    "# PREREQUISITES: Run Objectives 1, 2, and 3 first\n",
    "#\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 1: IMPORTS & VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "from typing import List, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "except ImportError as e:\n",
    "    raise ImportError(f\"Missing: {e}. Run: pip install numpy pandas torch\")\n",
    "\n",
    "\n",
    "def validate_prerequisites():\n",
    "    \"\"\"Ensure Objectives 1, 2, and 3 were run first.\"\"\"\n",
    "    required = {\n",
    "        'Objective 1': ['system_prompt', 'mistral_tokenizer', 'mistral_model'],\n",
    "        'Objective 2': ['qa_database'],\n",
    "        'Objective 3': ['embedding_model', 'faiss_index', 'embed_query']\n",
    "    }\n",
    "    \n",
    "    all_missing = []\n",
    "    for objective, items in required.items():\n",
    "        missing = [item for item in items if item not in globals()]\n",
    "        if missing:\n",
    "            all_missing.append(f\"{objective}: {missing}\")\n",
    "    \n",
    "    if all_missing:\n",
    "        raise RuntimeError(f\"Missing prerequisites:\\n\" + \"\\n\".join(all_missing))\n",
    "    \n",
    "    print(\"âœ… All prerequisites validated\")\n",
    "    print(f\"   â€¢ System prompt: {len(globals()['system_prompt'])} chars\")\n",
    "    print(f\"   â€¢ Q&A database: {len(globals()['qa_database'])} pairs\")\n",
    "    print(f\"   â€¢ FAISS index: {globals()['faiss_index'].ntotal} vectors\")\n",
    "    print(f\"   â€¢ embed_query(): Ready (from Objective 3)\")\n",
    "    return True\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 2: CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = \"data/rag_pipeline\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# RAG CONFIGURATION PARAMETERS EXPLAINED\n",
    "# ============================================================================\n",
    "#\n",
    "# top_k: Number of documents to retrieve from FAISS\n",
    "#   - Higher value (5-10): More context, better coverage, but may include noise\n",
    "#   - Lower value (1-3): More focused, less noise, but may miss relevant info\n",
    "#   - Default 3: Good balance for small knowledge bases (21 Q&A pairs)\n",
    "#\n",
    "# max_new_tokens: Maximum tokens the model can generate in response\n",
    "#   - Higher value (500+): Longer, more detailed responses\n",
    "#   - Lower value (100-200): Concise responses, faster generation\n",
    "#   - Default 300: Allows comprehensive answers without being verbose\n",
    "#\n",
    "# temperature: Controls randomness/creativity in generation (0.0 - 1.0)\n",
    "#   - 0.0: Deterministic, always picks most likely token (factual tasks)\n",
    "#   - 0.5-0.7: Balanced creativity and coherence (recommended for QA)\n",
    "#   - 1.0+: More random/creative (creative writing, brainstorming)\n",
    "#   - Default 0.7: Allows natural variation while staying on-topic\n",
    "#\n",
    "# similarity_threshold: Minimum similarity score to include a document (0.0 - 1.0)\n",
    "#   - Higher value (0.5+): Only very relevant documents, may return few/none\n",
    "#   - Lower value (0.1-0.3): More documents included, may have lower relevance\n",
    "#   - Default 0.3: Filters out clearly irrelevant results while being inclusive\n",
    "#\n",
    "# ============================================================================\n",
    "\n",
    "RAG_CONFIG = {\n",
    "    \"top_k\": 3,                    # Number of documents to retrieve\n",
    "    \"max_new_tokens\": 300,         # Max tokens for generation\n",
    "    \"temperature\": 0.7,            # Generation temperature\n",
    "    \"similarity_threshold\": 0.3,   # Minimum similarity score (0-1)\n",
    "}\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 3: RAG RESULT DATA CLASS\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class RAGResult:\n",
    "    \"\"\"Container for RAG pipeline results.\"\"\"\n",
    "    query: str\n",
    "    response: str\n",
    "    retrieved_context: List[Dict]\n",
    "    success: bool\n",
    "    error_message: Optional[str] = None\n",
    "\n",
    "# Export RAGResult globally\n",
    "globals()['RAGResult'] = RAGResult\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 4: RAG PIPELINE CORE FUNCTIONS\n",
    "# ============================================================================\n",
    "#\n",
    "# These functions form the complete RAG pipeline:\n",
    "#   1. search_faiss()       - Search FAISS for similar Q&A pairs\n",
    "#   2. format_context()     - Format retrieved Q&A as context string\n",
    "#   3. build_prompt()       - Build augmented prompt\n",
    "#   4. generate_response()  - Generate response with Mistral\n",
    "#\n",
    "# NOTE: embed_query() is REUSED from Objective 3 (not redefined here)\n",
    "#\n",
    "# ============================================================================\n",
    "\n",
    "def search_faiss(query_embedding: np.ndarray, top_k: int = None) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    STEP 2: Search FAISS index for similar Q&A pairs.\n",
    "    \n",
    "    Reuses: faiss_index from Objective 3, qa_database from Objective 2\n",
    "    \n",
    "    Args:\n",
    "        query_embedding: Query vector from embed_query()\n",
    "        top_k: Number of results to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        List of Q&A dicts with similarity scores\n",
    "    \"\"\"\n",
    "    if top_k is None:\n",
    "        top_k = RAG_CONFIG[\"top_k\"]\n",
    "    \n",
    "    faiss_index = globals()['faiss_index']\n",
    "    qa_database = globals()['qa_database']\n",
    "    \n",
    "    # Ensure proper shape for FAISS search\n",
    "    if len(query_embedding.shape) == 1:\n",
    "        query_embedding = query_embedding.reshape(1, -1)\n",
    "    \n",
    "    # Search FAISS index\n",
    "    distances, indices = faiss_index.search(query_embedding, top_k)\n",
    "    \n",
    "    # Get Q&A pairs with similarity scores\n",
    "    results = []\n",
    "    for dist, idx in zip(distances[0], indices[0]):\n",
    "        if idx < len(qa_database):\n",
    "            similarity = 1 / (1 + float(dist))  # Convert distance to similarity\n",
    "            if similarity >= RAG_CONFIG[\"similarity_threshold\"]:\n",
    "                qa = qa_database[idx].copy()\n",
    "                qa['similarity_score'] = similarity\n",
    "                qa['distance'] = float(dist)\n",
    "                results.append(qa)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def format_context(retrieved_qa: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    STEP 3: Format retrieved Q&A pairs as context string.\n",
    "    \n",
    "    Args:\n",
    "        retrieved_qa: List of Q&A dicts from search_faiss()\n",
    "    \n",
    "    Returns:\n",
    "        Formatted context string for prompt\n",
    "    \"\"\"\n",
    "    if not retrieved_qa:\n",
    "        return \"No relevant information found in knowledge base.\"\n",
    "    \n",
    "    context_parts = [\"RELEVANT INFORMATION FROM KNOWLEDGE BASE:\", \"-\" * 40]\n",
    "    \n",
    "    for i, qa in enumerate(retrieved_qa, 1):\n",
    "        similarity_pct = qa.get('similarity_score', 0) * 100\n",
    "        context_parts.append(f\"\\n[Source {i}] (Relevance: {similarity_pct:.0f}%)\")\n",
    "        context_parts.append(f\"Q: {qa['question']}\")\n",
    "        context_parts.append(f\"A: {qa['answer']}\")\n",
    "    \n",
    "    context_parts.append(\"-\" * 40)\n",
    "    return \"\\n\".join(context_parts)\n",
    "\n",
    "\n",
    "def build_prompt(query: str, context: str) -> str:\n",
    "    \"\"\"\n",
    "    STEP 4: Build augmented prompt combining system prompt, context, and query.\n",
    "    \n",
    "    Reuses: system_prompt from Objective 1\n",
    "    \n",
    "    Args:\n",
    "        query: User's question\n",
    "        context: Formatted context from format_context()\n",
    "    \n",
    "    Returns:\n",
    "        Complete augmented prompt\n",
    "    \"\"\"\n",
    "    system_prompt = globals()['system_prompt']\n",
    "    \n",
    "    augmented_prompt = f\"\"\"{system_prompt}\n",
    "\n",
    "{context}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Answer using ONLY the information provided above\n",
    "- If information is not available, politely say so\n",
    "- Be helpful, accurate, and concise\n",
    "\n",
    "CUSTOMER QUESTION: {query}\n",
    "\n",
    "ASSISTANT RESPONSE:\"\"\"\n",
    "    \n",
    "    return augmented_prompt\n",
    "\n",
    "\n",
    "def generate_response(augmented_prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    STEP 5: Generate response with Mistral model.\n",
    "    \n",
    "    Reuses: mistral_tokenizer, mistral_model from Objective 1\n",
    "    \n",
    "    Args:\n",
    "        augmented_prompt: Complete prompt from build_prompt()\n",
    "    \n",
    "    Returns:\n",
    "        Generated response string\n",
    "    \"\"\"\n",
    "    tokenizer = globals()['mistral_tokenizer']\n",
    "    model = globals()['mistral_model']\n",
    "    \n",
    "    # Format for Mistral Instruct\n",
    "    formatted = f\"<s>[INST] {augmented_prompt} [/INST]\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\", truncation=True, max_length=4096)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=RAG_CONFIG[\"max_new_tokens\"],\n",
    "            temperature=RAG_CONFIG[\"temperature\"],\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode only new tokens\n",
    "    input_length = inputs['input_ids'].shape[1]\n",
    "    response = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True).strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 5: COMPLETE RAG PIPELINE FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def rag_query(query: str, top_k: int = None, verbose: bool = True) -> RAGResult:\n",
    "    \"\"\"\n",
    "    Complete RAG Pipeline: Query â†’ Retrieve â†’ Augment â†’ Generate\n",
    "    \n",
    "    This is the main entry point for the RAG system.\n",
    "    Orchestrates all pipeline functions in sequence.\n",
    "    \n",
    "    Args:\n",
    "        query: User's question\n",
    "        top_k: Number of documents to retrieve (default: from config)\n",
    "        verbose: Print step-by-step progress\n",
    "    \n",
    "    Returns:\n",
    "        RAGResult with response and metadata\n",
    "    \n",
    "    Example:\n",
    "        result = rag_query(\"What is your return policy?\")\n",
    "        print(result.response)\n",
    "    \"\"\"\n",
    "    if top_k is None:\n",
    "        top_k = RAG_CONFIG[\"top_k\"]\n",
    "    \n",
    "    # Get embed_query from Objective 3\n",
    "    embed_query_func = globals()['embed_query']\n",
    "    \n",
    "    try:\n",
    "        # ============================================================\n",
    "        # STEP 1: QUERY PROCESSING - Convert to embedding\n",
    "        # ============================================================\n",
    "        if verbose:\n",
    "            print(f\"   Step 1: embed_query() - Converting query to embedding...\")\n",
    "        query_embedding = embed_query_func(query)\n",
    "        \n",
    "        # Ensure proper shape\n",
    "        if len(query_embedding.shape) == 1:\n",
    "            query_embedding = query_embedding.reshape(1, -1)\n",
    "        \n",
    "        # ============================================================\n",
    "        # STEP 2: RETRIEVAL - Search FAISS for similar Q&A\n",
    "        # ============================================================\n",
    "        if verbose:\n",
    "            print(f\"   Step 2: search_faiss() - Searching for similar Q&A pairs...\")\n",
    "        retrieved_qa = search_faiss(query_embedding, top_k)\n",
    "        if verbose:\n",
    "            print(f\"           â†’ Found {len(retrieved_qa)} relevant documents\")\n",
    "        \n",
    "        # ============================================================\n",
    "        # STEP 3: AUGMENTATION (Part 1) - Format context\n",
    "        # ============================================================\n",
    "        if verbose:\n",
    "            print(f\"   Step 3: format_context() - Formatting retrieved context...\")\n",
    "        context = format_context(retrieved_qa)\n",
    "        \n",
    "        # ============================================================\n",
    "        # STEP 4: AUGMENTATION (Part 2) - Build prompt\n",
    "        # ============================================================\n",
    "        if verbose:\n",
    "            print(f\"   Step 4: build_prompt() - Building augmented prompt...\")\n",
    "        augmented_prompt = build_prompt(query, context)\n",
    "        \n",
    "        # ============================================================\n",
    "        # STEP 5: GENERATION - Generate response with Mistral\n",
    "        # ============================================================\n",
    "        if verbose:\n",
    "            print(f\"   Step 5: generate_response() - Generating response with Mistral...\")\n",
    "        response = generate_response(augmented_prompt)\n",
    "        if verbose:\n",
    "            print(f\"           â†’ Response generated: {len(response)} chars\")\n",
    "        \n",
    "        return RAGResult(\n",
    "            query=query,\n",
    "            response=response,\n",
    "            retrieved_context=retrieved_qa,\n",
    "            success=True\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        return RAGResult(\n",
    "            query=query,\n",
    "            response=\"I apologize, but I encountered an error processing your request.\",\n",
    "            retrieved_context=[],\n",
    "            success=False,\n",
    "            error_message=str(e)\n",
    "        )\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 6: DISPLAY & STORAGE FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def display_rag_result(result: RAGResult):\n",
    "    \"\"\"Display RAG result in formatted way.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ¤– RAG PIPELINE RESULT\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nğŸ“¥ USER QUERY:\")\n",
    "    print(f\"   {result.query}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“š RETRIEVED CONTEXT ({len(result.retrieved_context)} sources):\")\n",
    "    for i, ctx in enumerate(result.retrieved_context, 1):\n",
    "        similarity = ctx.get('similarity_score', 0) * 100\n",
    "        answerable = 'ğŸ“—' if ctx.get('answerable', True) else 'ğŸ“•'\n",
    "        print(f\"   {answerable} [{i}] {ctx.get('category', 'N/A')} (Similarity: {similarity:.0f}%)\")\n",
    "        print(f\"       Q: {ctx['question'][:60]}...\")\n",
    "    \n",
    "    print(f\"\\nğŸ“¤ GENERATED RESPONSE:\")\n",
    "    print(\"-\"*70)\n",
    "    print(result.response)\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    if not result.success:\n",
    "        print(f\"\\nâš ï¸  Error: {result.error_message}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "\n",
    "\n",
    "def save_rag_results(results: List[RAGResult], filename: str = \"rag_test_results.csv\"):\n",
    "    \"\"\"Save RAG test results to CSV.\"\"\"\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    \n",
    "    rows = []\n",
    "    for result in results:\n",
    "        rows.append({\n",
    "            'query': result.query,\n",
    "            'response': result.response[:500],\n",
    "            'num_sources': len(result.retrieved_context),\n",
    "            'top_source_similarity': result.retrieved_context[0]['similarity_score'] if result.retrieved_context else 0,\n",
    "            'success': result.success,\n",
    "            'error': result.error_message\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(filepath, index=False)\n",
    "    print(f\"âœ… RAG results saved to: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def save_pipeline_config(filename: str = \"pipeline_config.txt\"):\n",
    "    \"\"\"Save pipeline configuration.\"\"\"\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    \n",
    "    config_text = f\"\"\"RAG PIPELINE CONFIGURATION\n",
    "==========================\n",
    "\n",
    "Retrieval Settings:\n",
    "- Top-K: {RAG_CONFIG['top_k']}\n",
    "- Similarity Threshold: {RAG_CONFIG['similarity_threshold']}\n",
    "\n",
    "Generation Settings:\n",
    "- Max New Tokens: {RAG_CONFIG['max_new_tokens']}\n",
    "- Temperature: {RAG_CONFIG['temperature']}\n",
    "\n",
    "Components:\n",
    "- Embedding Model: sentence-transformers/all-MiniLM-L6-v2\n",
    "- Vector Store: FAISS IndexFlatL2\n",
    "- LLM: Mistral-7B-Instruct-v0.3\n",
    "- Q&A Database: {len(globals().get('qa_database', []))} pairs\n",
    "\"\"\"\n",
    "    \n",
    "    with open(filepath, 'w') as f:\n",
    "        f.write(config_text)\n",
    "    \n",
    "    print(f\"âœ… Pipeline config saved to: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 7: VERIFICATION FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def verify_objective4():\n",
    "    \"\"\"\n",
    "    Verify that Objective 4 completed successfully.\n",
    "    Checks all functions, files, and test results.\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"ğŸ” OBJECTIVE 4 VERIFICATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    errors = []\n",
    "    \n",
    "    # Check required functions exist\n",
    "    required_functions = ['rag_query', 'search_faiss', 'format_context', \n",
    "                          'build_prompt', 'generate_response', 'embed_query']\n",
    "    for func_name in required_functions:\n",
    "        if func_name not in globals() or not callable(globals()[func_name]):\n",
    "            errors.append(f\"âŒ Function '{func_name}' not found\")\n",
    "    \n",
    "    # Check RAGResult dataclass\n",
    "    if 'RAGResult' not in globals():\n",
    "        errors.append(\"âŒ RAGResult dataclass not found\")\n",
    "    \n",
    "    # Check files exist\n",
    "    if not os.path.exists(\"data/rag_pipeline/rag_test_results.csv\"):\n",
    "        errors.append(\"âŒ rag_test_results.csv not found\")\n",
    "    if not os.path.exists(\"data/rag_pipeline/pipeline_config.txt\"):\n",
    "        errors.append(\"âŒ pipeline_config.txt not found\")\n",
    "    \n",
    "    # Check test results\n",
    "    if 'rag_results' not in globals():\n",
    "        errors.append(\"âŒ rag_results not found\")\n",
    "    elif len(globals()['rag_results']) != 10:\n",
    "        errors.append(f\"âŒ Expected 10 test results, got {len(globals()['rag_results'])}\")\n",
    "    \n",
    "    # Test rag_query function\n",
    "    try:\n",
    "        test_result = rag_query(\"test query\", verbose=False)\n",
    "        if not isinstance(test_result, RAGResult):\n",
    "            errors.append(\"âŒ rag_query() does not return RAGResult\")\n",
    "    except Exception as e:\n",
    "        errors.append(f\"âŒ rag_query() test failed: {e}\")\n",
    "    \n",
    "    # Print results\n",
    "    if errors:\n",
    "        print(\"\\nâŒ VERIFICATION FAILED:\")\n",
    "        print(\"\\n\".join(errors))\n",
    "        print(\"=\"*70)\n",
    "        return False\n",
    "    else:\n",
    "        print(\"\\nâœ… Objective 4 Complete - All components verified\")\n",
    "        print(f\"   â€¢ Pipeline Functions: {len(required_functions)} verified\")\n",
    "        print(f\"   â€¢ RAGResult: Defined\")\n",
    "        print(f\"   â€¢ Test Results: {len(globals()['rag_results'])} queries\")\n",
    "        print(f\"   â€¢ Files: Saved to data/rag_pipeline/\")\n",
    "        print(\"=\"*70)\n",
    "        return True\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 8: EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"   OBJECTIVE 4: BUILD COMPLETE RAG PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# --- Step 1: Validate Prerequisites ---\n",
    "print(\"\\nğŸ” STEP 1: Validate Prerequisites\")\n",
    "print(\"-\"*70)\n",
    "validate_prerequisites()\n",
    "\n",
    "# --- Step 2: Show Pipeline Architecture ---\n",
    "print(\"\\nğŸ“ STEP 2: RAG Pipeline Architecture\")\n",
    "print(\"-\"*70)\n",
    "print(\"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                      RAG PIPELINE FLOW                          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  User Query                                                     â”‚\n",
    "â”‚      â”‚                                                          â”‚\n",
    "â”‚      â–¼                                                          â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚\n",
    "â”‚  â”‚ 1. embed_query()â”‚  Convert query to embedding (from Obj 3)   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                            â”‚\n",
    "â”‚           â–¼                                                     â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚\n",
    "â”‚  â”‚ 2. search_faiss()â”‚  Find similar Q&A pairs                   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                            â”‚\n",
    "â”‚           â–¼                                                     â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚\n",
    "â”‚  â”‚ 3. format_context()â”‚  Format as context string               â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚\n",
    "â”‚           â–¼                                                     â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚\n",
    "â”‚  â”‚ 4. build_prompt()â”‚  Combine system + context + query         â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                            â”‚\n",
    "â”‚           â–¼                                                     â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚\n",
    "â”‚  â”‚ 5. generate_response()â”‚  Generate with Mistral               â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚\n",
    "â”‚           â–¼                                                     â”‚\n",
    "â”‚      Response                                                   â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\")\n",
    "\n",
    "# --- Step 3: Test RAG Pipeline with Answerable Questions ---\n",
    "print(\"\\nğŸ§ª STEP 3: Test with ANSWERABLE Questions\")\n",
    "print(\"-\"*70)\n",
    "print(\"   These questions CAN be answered from our knowledge base\")\n",
    "\n",
    "answerable_questions = [\n",
    "    \"What is your return policy?\",\n",
    "    \"How long does shipping take?\",\n",
    "    \"What are your customer service hours?\",\n",
    "    \"Do you offer warranty on products?\",\n",
    "    \"How can I track my order?\",\n",
    "]\n",
    "\n",
    "answerable_results = []\n",
    "\n",
    "for query in answerable_questions:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸ“— ANSWERABLE: \\\"{query}\\\"\")\n",
    "    print('='*70)\n",
    "    \n",
    "    result = rag_query(query, verbose=True)\n",
    "    answerable_results.append(result)\n",
    "    display_rag_result(result)\n",
    "\n",
    "# --- Step 4: Test RAG Pipeline with Unanswerable Questions ---\n",
    "print(\"\\nğŸ§ª STEP 4: Test with UNANSWERABLE Questions\")\n",
    "print(\"-\"*70)\n",
    "print(\"   These questions CANNOT be answered from our knowledge base\")\n",
    "print(\"   Testing system limitations and graceful handling\")\n",
    "\n",
    "unanswerable_questions = [\n",
    "    \"How do your prices compare to Amazon?\",\n",
    "    \"Should I buy solar panels for my house?\",\n",
    "    \"Will you have a Black Friday sale this year?\",\n",
    "    \"What is the CEO's email address?\",\n",
    "    \"Can you recommend a good restaurant nearby?\",\n",
    "]\n",
    "\n",
    "unanswerable_results = []\n",
    "\n",
    "for query in unanswerable_questions:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸ“• UNANSWERABLE: \\\"{query}\\\"\")\n",
    "    print('='*70)\n",
    "    \n",
    "    result = rag_query(query, verbose=True)\n",
    "    unanswerable_results.append(result)\n",
    "    display_rag_result(result)\n",
    "\n",
    "# --- Step 5: Save Results ---\n",
    "print(\"\\nğŸ’¾ STEP 5: Save Results\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "all_results = answerable_results + unanswerable_results\n",
    "save_rag_results(all_results)\n",
    "save_pipeline_config()\n",
    "\n",
    "# Store globally for reuse\n",
    "globals()['rag_query'] = rag_query\n",
    "globals()['rag_results'] = all_results\n",
    "\n",
    "# Export core functions globally\n",
    "globals()['search_faiss'] = search_faiss\n",
    "globals()['format_context'] = format_context\n",
    "globals()['build_prompt'] = build_prompt\n",
    "globals()['generate_response'] = generate_response\n",
    "globals()['verify_objective4'] = verify_objective4\n",
    "\n",
    "# --- Step 6: Verify Objective 4 ---\n",
    "print(\"\\nâœ… STEP 6: Verify Objective 4\")\n",
    "print(\"-\"*70)\n",
    "verify_objective4()\n",
    "\n",
    "# --- Summary ---\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… OBJECTIVE 4 COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "answerable_success = sum(1 for r in answerable_results if r.success)\n",
    "unanswerable_success = sum(1 for r in unanswerable_results if r.success)\n",
    "\n",
    "print(f\"\"\"\n",
    "RAG Pipeline Components:\n",
    "  1. Query Processing: embed_query() - Convert to embedding (from Objective 3)\n",
    "  2. Retrieval: search_faiss() - FAISS similarity search (top-{RAG_CONFIG['top_k']})\n",
    "  3. Augmentation: format_context() + build_prompt()\n",
    "  4. Generation: generate_response() - Mistral-7B response\n",
    "\n",
    "Test Results:\n",
    "  ğŸ“— Answerable Questions: {answerable_success}/{len(answerable_results)} successful\n",
    "  ğŸ“• Unanswerable Questions: {unanswerable_success}/{len(unanswerable_results)} successful\n",
    "\n",
    "ğŸ“¦ FILES SAVED:\n",
    "  â€¢ {OUTPUT_DIR}/rag_test_results.csv\n",
    "  â€¢ {OUTPUT_DIR}/pipeline_config.txt\n",
    "\n",
    "ğŸ“¦ GLOBAL FUNCTIONS:\n",
    "  â€¢ rag_query(query) - Complete RAG pipeline\n",
    "  â€¢ search_faiss(), format_context(), build_prompt(), generate_response()\n",
    "  â€¢ embed_query() - Reused from Objective 3\n",
    "  â€¢ verify_objective4() - Verification function\n",
    "\n",
    "ğŸ”œ READY FOR OBJECTIVE 5: Model Experimentation and Ranking\n",
    "\"\"\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective 5: Model Evaluation & Ranking\n",
    "\n",
    "### ğŸ¯ Goal\n",
    "\n",
    "Evaluate 6 question-answering models using the RAG pipeline, compare their performance using **5 evaluation metrics**, and rank models to identify the best performer based on weighted scoring.\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ“¥ Prerequisites</b> (Click to expand)</summary>\n",
    "\n",
    "| Item | Source | Required | Description |\n",
    "|------|--------|----------|-------------|\n",
    "| `qa_database` | Objective 2 | âœ… Yes | Ground truth answers for comparison |\n",
    "| `embed_query()` | Objective 3 | âœ… Yes | For semantic similarity calculation |\n",
    "| `rag_query()` | Objective 4 | âœ… Yes | Complete RAG pipeline for dynamic context |\n",
    "| `search_faiss()`, `format_context()` | Objective 4 | âœ… Yes | RAG pipeline components |\n",
    "\n",
    "**Note:** Requires Objectives 1-4 completed. Achieves **100% component reuse**.\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ“Š The 5 Evaluation Metrics</b> (Click to expand)</summary>\n",
    "\n",
    "### 1. Accuracy (F1 Score) - Weight: 25%\n",
    "**What it measures:** Token-level overlap between model answer and ground truth\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)\n",
    "```\n",
    "\n",
    "**Calculation:**\n",
    "- Tokenize both answer and expected answer\n",
    "- Calculate precision: (matching tokens) / (total tokens in answer)\n",
    "- Calculate recall: (matching tokens) / (total tokens in expected)\n",
    "- Compute F1 score (0-1 range)\n",
    "\n",
    "**Only computed for:** Answerable questions (unanswerable have no expected answer)\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Confidence Handling - Weight: 20%\n",
    "**What it measures:** Whether model appropriately indicates uncertainty\n",
    "\n",
    "**Scoring Logic:**\n",
    "\n",
    "**For Answerable Questions:**\n",
    "- Higher raw confidence â†’ Higher score\n",
    "- Score = raw_confidence (0-1)\n",
    "\n",
    "**For Unanswerable Questions:**\n",
    "- Model abstains (\"I don't know\", \"unknown\", \"cannot answer\") â†’ **Score: 1.0** âœ…\n",
    "- Low confidence (< 0.3) â†’ **Score: 0.8** âœ…\n",
    "- Medium confidence (0.3-0.5) â†’ **Score: 0.5** âš ï¸\n",
    "- High confidence (> 0.5) â†’ **Score: 0.2** âŒ (overconfident)\n",
    "\n",
    "**Computed for:** ALL questions (both answerable and unanswerable)\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Quality (Semantic Similarity) - Weight: 25%\n",
    "**What it measures:** Semantic meaning similarity using embeddings\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "Quality = cosine_similarity(embed_query(answer), embed_query(expected))\n",
    "```\n",
    "\n",
    "**Calculation:**\n",
    "- Uses `embed_query()` from Objective 3 (component reuse!)\n",
    "- Embed both answer and expected answer\n",
    "- Calculate cosine similarity between embeddings\n",
    "- Range: 0-1 (1 = identical meaning)\n",
    "\n",
    "**Only computed for:** Answerable questions\n",
    "\n",
    "**Key Insight:** Reuses `embed_query()` from Objective 3 - same function used in RAG retrieval!\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Speed - Weight: 15%\n",
    "**What it measures:** Response time performance\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "Speed = 1 - (response_time_ms / max_acceptable_time_ms)\n",
    "```\n",
    "\n",
    "**Calculation:**\n",
    "- Measure time from query to answer (milliseconds)\n",
    "- Normalize against max acceptable time (e.g., 5000ms)\n",
    "- Faster responses = higher score\n",
    "- Range: 0-1 (1 = instant, 0 = very slow)\n",
    "\n",
    "**Computed for:** ALL questions\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Robustness - Weight: 15%\n",
    "**What it measures:** Edge case handling and error recovery\n",
    "\n",
    "**Scoring Logic:**\n",
    "- **Error during inference** â†’ Score: 0.0 âŒ\n",
    "- **Answer too short** (< 2 words) â†’ Score: 0.2 âš ï¸\n",
    "- **Answer too long** (> 100 words) â†’ Score: 0.5 âš ï¸ (possible hallucination)\n",
    "- **Unanswerable + abstains** â†’ Score: 1.0 âœ… (correct behavior)\n",
    "- **Unanswerable + long answer** (> 50 words) â†’ Score: 0.2 âŒ (hallucinating)\n",
    "- **Normal answer** â†’ Score: 1.0 âœ…\n",
    "\n",
    "**Computed for:** ALL questions\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ† Ranking Calculation</b> (Click to expand)</summary>\n",
    "\n",
    "### Step 1: Calculate Per-Question Metrics\n",
    "For each question, compute all 5 metrics:\n",
    "- Accuracy (F1) - if answerable\n",
    "- Confidence - always\n",
    "- Quality (Semantic) - if answerable\n",
    "- Speed - always\n",
    "- Robustness - always\n",
    "\n",
    "### Step 2: Aggregate Per Model\n",
    "For each model, average each metric across all questions:\n",
    "```\n",
    "avg_accuracy = mean(accuracy_scores for answerable questions)\n",
    "avg_confidence = mean(confidence_scores for all questions)\n",
    "avg_quality = mean(quality_scores for answerable questions)\n",
    "avg_speed = mean(speed_scores for all questions)\n",
    "avg_robustness = mean(robustness_scores for all questions)\n",
    "```\n",
    "\n",
    "### Step 3: Calculate Final Score\n",
    "Weighted combination of all 5 metrics:\n",
    "\n",
    "```\n",
    "Final Score = (Accuracy Ã— 0.25) + \n",
    "              (Confidence Ã— 0.20) + \n",
    "              (Quality Ã— 0.25) + \n",
    "              (Speed Ã— 0.15) + \n",
    "              (Robustness Ã— 0.15)\n",
    "```\n",
    "\n",
    "**Weight Distribution:**\n",
    "- **Accuracy + Quality = 50%** (correctness and meaning)\n",
    "- **Confidence = 20%** (uncertainty handling)\n",
    "- **Speed + Robustness = 30%** (performance and reliability)\n",
    "\n",
    "### Step 4: Rank Models\n",
    "Sort all models by Final Score (descending):\n",
    "- Rank 1 = Highest Final Score (Best Model)\n",
    "- Rank 2 = Second highest\n",
    "- ... and so on\n",
    "\n",
    "### Example Calculation:\n",
    "\n",
    "**Model: RoBERTa-SQuAD2**\n",
    "- Accuracy: 0.847\n",
    "- Confidence: 0.756\n",
    "- Quality: 0.891\n",
    "- Speed: 0.823\n",
    "- Robustness: 0.950\n",
    "\n",
    "**Final Score:**\n",
    "```\n",
    "(0.847 Ã— 0.25) + (0.756 Ã— 0.20) + (0.891 Ã— 0.25) + (0.823 Ã— 0.15) + (0.950 Ã— 0.15)\n",
    "= 0.212 + 0.151 + 0.223 + 0.123 + 0.143\n",
    "= 0.852\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ¤– Models Evaluated</b> (Click to expand)</summary>\n",
    "\n",
    "| Rank | Model Name | Model ID | Type | Size |\n",
    "|------|-----------|----------|------|------|\n",
    "| - | T5-QA-Generative | consciousAI/question-answering-generative-t5-v1-base-s-q-c | text2text-generation | base |\n",
    "| - | RoBERTa-SQuAD2 | deepset/roberta-base-squad2 | question-answering | base |\n",
    "| - | BERT-Large-SQuAD | google-bert/bert-large-cased-whole-word-masking-finetuned-squad | question-answering | large |\n",
    "| - | DistilBERT-SQuAD | distilbert-base-uncased-distilled-squad | question-answering | small |\n",
    "| - | BERT-Tiny-SQuAD | mrm8488/bert-tiny-finetuned-squadv2 | question-answering | tiny |\n",
    "| - | MiniLM-SQuAD | deepset/minilm-uncased-squad2 | question-answering | small |\n",
    "\n",
    "**Note:** Rankings determined by Final Score after evaluation.\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ“‹ Test Questions</b> (Click to expand)</summary>\n",
    "\n",
    "**Answerable (5 questions):**\n",
    "1. \"What is your return policy?\"\n",
    "2. \"How long does shipping take?\"\n",
    "3. \"What are your customer service hours?\"\n",
    "4. \"Do you offer warranty on products?\"\n",
    "5. \"How can I track my order?\"\n",
    "\n",
    "**Unanswerable (3 questions):**\n",
    "1. \"What is the CEO's phone number?\"\n",
    "2. \"Will you have a Black Friday sale?\"\n",
    "3. \"How do prices compare to Amazon?\"\n",
    "\n",
    "**Why this split:**\n",
    "- Answerable questions test: Accuracy, Quality\n",
    "- Unanswerable questions test: Confidence Handling, Robustness\n",
    "- All questions test: Speed\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ”„ Evaluation Process</b> (Click to expand)</summary>\n",
    "\n",
    "**For Each Model:**\n",
    "1. Load model from HuggingFace\n",
    "2. For each test question:\n",
    "   - Get RAG context using `rag_query()` (reuses Obj 4)\n",
    "   - Run model inference with retrieved context\n",
    "   - Calculate all 5 metrics\n",
    "   - Store results\n",
    "\n",
    "**For Each Question:**\n",
    "- **Context Retrieval:** `rag_query()` â†’ uses `embed_query()` (Obj 3) + `search_faiss()` (Obj 4)\n",
    "- **Answer Generation:** Model inference with context\n",
    "- **Metric Calculation:**\n",
    "  - Accuracy: F1 score vs ground truth\n",
    "  - Confidence: Extract from model output\n",
    "  - Quality: `embed_query()` for semantic similarity (reuses Obj 3)\n",
    "  - Speed: Measure response time\n",
    "  - Robustness: Check answer length, errors, abstention\n",
    "\n",
    "**After All Models:**\n",
    "- Aggregate metrics per model\n",
    "- Calculate Final Scores\n",
    "- Rank models\n",
    "- Save to `model_rankings.csv`\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ“¤ Outputs</b> (Click to expand)</summary>\n",
    "\n",
    "**Files Created:**\n",
    "- `model_rankings.csv` - Final rankings with all metrics\n",
    "\n",
    "**Global Variables:**\n",
    "- `rankings_df` - DataFrame with model rankings\n",
    "- `model_evaluations` - List of ModelMetrics objects\n",
    "\n",
    "**CSV Columns:**\n",
    "| Rank | Model | Accuracy | Confidence | Quality | Speed | Robustness | Final_Score |\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>ğŸ”— Component Reuse</b> (Click to expand)</summary>\n",
    "\n",
    "**100% Reuse from Previous Objectives:**\n",
    "\n",
    "| Component | From | Used For |\n",
    "|-----------|------|----------|\n",
    "| `rag_query()` | Obj 4 | Dynamic context retrieval per question |\n",
    "| `embed_query()` | Obj 3 | **Semantic similarity metric** (Quality) |\n",
    "| `search_faiss()` | Obj 4 | Via `rag_query()` |\n",
    "| `format_context()` | Obj 4 | Via `rag_query()` |\n",
    "| `qa_database` | Obj 2 | Ground truth answers |\n",
    "| `faiss_index` | Obj 3 | Via `search_faiss()` |\n",
    "\n",
    "**Key Insight:** The `embed_query()` function is reused for BOTH:\n",
    "1. RAG retrieval (Objective 4)\n",
    "2. Semantic similarity calculation (Objective 5)\n",
    "\n",
    "This demonstrates true modular design with zero code duplication.\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><b>âœ… Verification</b> (Click to expand)</summary>\n",
    "\n",
    "Run `verify_objective5()` to check:\n",
    "- âœ… `rankings_df` exists with 6 models\n",
    "- âœ… `model_rankings.csv` file created\n",
    "- âœ… All 5 metrics present (Accuracy, Confidence, Quality, Speed, Robustness)\n",
    "- âœ… Final_Score calculated\n",
    "- âœ… Models ranked by Final_Score (descending)\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "**Next Step:** Proceed to Objective 6 for system analysis and recommendations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸš€ OBJECTIVE 5: MODEL EVALUATION (FIXED VERSION)\n",
      "======================================================================\n",
      "======================================================================\n",
      "âœ… ALL PREREQUISITES VALIDATED\n",
      "======================================================================\n",
      "   Obj 2: qa_database (21 Q&A pairs)\n",
      "   Obj 3: embedding_model (384D)\n",
      "   Obj 3: faiss_index (21 vectors)\n",
      "   Obj 4: rag_query(), search_faiss(), format_context()\n",
      "======================================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š EVALUATING: T5-QA-Generative\n",
      "============================================================\n",
      "   âœ… Model loaded: consciousAI/question-answering-generative-t5-v1-base-s-q-c\n",
      "   âœ… [ANS] What is your return policy?...\n",
      "      Sources: 3 | F1=0.44 Conf=0.20 Qual=0.91 Spd=0.82 Rob=0.30\n",
      "   âœ… [ANS] How long does shipping take?...\n",
      "      Sources: 3 | F1=0.67 Conf=0.20 Qual=0.61 Spd=0.88 Rob=0.30\n",
      "   âœ… [ANS] What are your customer service hours?...\n",
      "      Sources: 3 | F1=0.67 Conf=0.20 Qual=0.71 Spd=0.66 Rob=0.30\n",
      "   âœ… [ANS] Do you offer warranty on products?...\n",
      "      Sources: 3 | F1=0.20 Conf=0.20 Qual=0.20 Spd=0.67 Rob=0.30\n",
      "   âœ… [ANS] How can I track my order?...\n",
      "      Sources: 3 | F1=0.33 Conf=0.20 Qual=0.32 Spd=0.80 Rob=0.30\n",
      "   âœ… [UNK] What is the CEO's phone number?...\n",
      "      Sources: 3 | F1=0.00 Conf=1.00 Qual=0.00 Spd=0.70 Rob=1.00\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# OBJECTIVE 5: MODEL EVALUATION & RANKING (FIXED VERSION)\n",
    "# ============================================================================\n",
    "#\n",
    "# FIXES APPLIED:\n",
    "# 1. Confidence handling logic corrected for unanswerable questions\n",
    "# 2. Better context formatting for extractive QA models\n",
    "# 3. Improved F1 calculation with partial matching\n",
    "# 4. Added debug output to diagnose issues\n",
    "#\n",
    "# ============================================================================\n",
    "\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 1: DATA CLASSES\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class QuestionMetrics:\n",
    "    \"\"\"Metrics for a single question evaluation.\"\"\"\n",
    "    question: str\n",
    "    answer: str\n",
    "    expected: str\n",
    "    context: str\n",
    "    is_answerable: bool\n",
    "    num_sources: int\n",
    "    accuracy: float\n",
    "    confidence: float\n",
    "    quality: float\n",
    "    speed: float\n",
    "    robustness: float\n",
    "    raw_confidence: Optional[float] = None\n",
    "    response_time_ms: float = 0.0\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelMetrics:\n",
    "    \"\"\"Aggregated metrics for one model.\"\"\"\n",
    "    name: str\n",
    "    model_id: str\n",
    "    accuracy: float\n",
    "    confidence: float\n",
    "    quality: float\n",
    "    speed: float\n",
    "    robustness: float\n",
    "    final_score: float\n",
    "    rank: int = 0\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 2: CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "MODELS_CONFIG = [\n",
    "    (\"T5-QA-Generative\", \"consciousAI/question-answering-generative-t5-v1-base-s-q-c\", \"text2text-generation\"),\n",
    "    (\"RoBERTa-SQuAD2\", \"deepset/roberta-base-squad2\", \"question-answering\"),\n",
    "    (\"BERT-Large-SQuAD\", \"google-bert/bert-large-cased-whole-word-masking-finetuned-squad\", \"question-answering\"),\n",
    "    (\"DistilBERT-SQuAD\", \"distilbert-base-uncased-distilled-squad\", \"question-answering\"),\n",
    "    (\"BERT-Tiny-SQuAD\", \"mrm8488/bert-tiny-finetuned-squadv2\", \"question-answering\"),\n",
    "    (\"MiniLM-SQuAD\", \"deepset/minilm-uncased-squad2\", \"question-answering\"),\n",
    "]\n",
    "\n",
    "# Test questions - these should match your qa_database content!\n",
    "TEST_QUESTIONS = [\n",
    "    # (question, expected_answer, is_answerable)\n",
    "    (\"What is your return policy?\", \"30-day return policy with full refund\", True),\n",
    "    (\"How long does shipping take?\", \"Standard shipping takes 5-7 business days\", True),\n",
    "    (\"What are your customer service hours?\", \"Monday to Friday 9am to 5pm\", True),\n",
    "    (\"Do you offer warranty on products?\", \"1 year warranty on all electronics\", True),\n",
    "    (\"How can I track my order?\", \"Use tracking number on our website\", True),\n",
    "    # Unanswerable questions\n",
    "    (\"What is the CEO's phone number?\", \"\", False),\n",
    "    (\"Will you have a Black Friday sale?\", \"\", False),\n",
    "    (\"How do prices compare to Amazon?\", \"\", False),\n",
    "]\n",
    "\n",
    "METRIC_WEIGHTS = {\n",
    "    'accuracy': 0.25,\n",
    "    'confidence': 0.20,\n",
    "    'quality': 0.25,\n",
    "    'speed': 0.15,\n",
    "    'robustness': 0.15\n",
    "}\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 3: PREREQUISITE VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "def validate_prerequisites() -> bool:\n",
    "    \"\"\"Verify ALL required components from Objectives 1-4 exist.\"\"\"\n",
    "    requirements = {\n",
    "        'Objective 1': ['mistral_model', 'mistral_tokenizer', 'system_prompt'],\n",
    "        'Objective 2': ['qa_database'],\n",
    "        'Objective 3': ['embedding_model', 'faiss_index', 'embed_query', 'qa_embeddings'],\n",
    "        'Objective 4': ['rag_query', 'search_faiss', 'format_context'],\n",
    "    }\n",
    "    \n",
    "    missing = []\n",
    "    for obj, items in requirements.items():\n",
    "        for item in items:\n",
    "            if item not in globals():\n",
    "                missing.append(f\"{obj}: {item}\")\n",
    "            elif item in ['embed_query', 'rag_query', 'search_faiss', 'format_context']:\n",
    "                if not callable(globals()[item]):\n",
    "                    missing.append(f\"{obj}: {item} (not callable)\")\n",
    "    \n",
    "    if missing:\n",
    "        print(\"âŒ MISSING PREREQUISITES - Run Objectives 1-4 first:\")\n",
    "        for m in missing:\n",
    "            print(f\"   â€¢ {m}\")\n",
    "        return False\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"âœ… ALL PREREQUISITES VALIDATED\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"   Obj 2: qa_database ({len(globals()['qa_database'])} Q&A pairs)\")\n",
    "    print(f\"   Obj 3: embedding_model ({globals()['embedding_model'].get_sentence_embedding_dimension()}D)\")\n",
    "    print(f\"   Obj 3: faiss_index ({globals()['faiss_index'].ntotal} vectors)\")\n",
    "    print(f\"   Obj 4: rag_query(), search_faiss(), format_context()\")\n",
    "    print(\"=\"*70)\n",
    "    return True\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 4: CONTEXT RETRIEVAL (REUSES OBJECTIVE 4)\n",
    "# ============================================================================\n",
    "\n",
    "def get_rag_context(question: str, debug: bool = False) -> Tuple[str, int, List[dict]]:\n",
    "    \"\"\"\n",
    "    Get context using RAG pipeline from Objective 4.\n",
    "    \n",
    "    Returns:\n",
    "        (context_string, num_sources, raw_results)\n",
    "    \"\"\"\n",
    "    rag_query_fn = globals()['rag_query']\n",
    "    format_context_fn = globals()['format_context']\n",
    "    \n",
    "    result = rag_query_fn(question, verbose=False)\n",
    "    \n",
    "    if result.success and result.retrieved_context:\n",
    "        # Format context in a way extractive QA models understand\n",
    "        # Extractive models need the answer to be PRESENT in the context\n",
    "        context_parts = []\n",
    "        for ctx in result.retrieved_context:\n",
    "            # Include both question and answer from retrieved pairs\n",
    "            q = ctx.get('question', '')\n",
    "            a = ctx.get('answer', '')\n",
    "            context_parts.append(f\"Q: {q} A: {a}\")\n",
    "        \n",
    "        context = \" \".join(context_parts)\n",
    "        num_sources = len(result.retrieved_context)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"      [DEBUG] Retrieved {num_sources} sources\")\n",
    "            print(f\"      [DEBUG] Context preview: {context[:200]}...\")\n",
    "        \n",
    "        return context, num_sources, result.retrieved_context\n",
    "    else:\n",
    "        return \"No relevant information found.\", 0, []\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 5: METRIC CALCULATOR (FIXED VERSION)\n",
    "# ============================================================================\n",
    "\n",
    "class MetricCalculator:\n",
    "    \"\"\"\n",
    "    Calculate all 5 evaluation metrics with FIXED confidence handling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.embedding_model = globals()['embedding_model']\n",
    "        self.embed_query = globals()['embed_query']\n",
    "        self.abstention_phrases = [\n",
    "            'unknown', 'not sure', 'cannot answer', 'no information',\n",
    "            'not mentioned', \"don't know\", 'unclear', 'not found',\n",
    "            'i do not', 'i cannot', 'unable to', 'no answer',\n",
    "            'not available', 'not specified', 'empty', ''\n",
    "        ]\n",
    "    \n",
    "    def _normalize(self, text: str) -> str:\n",
    "        \"\"\"Normalize text for comparison.\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        text = text.lower().strip()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        return re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    def _is_abstaining(self, answer: str) -> bool:\n",
    "        \"\"\"Check if model is abstaining from answering.\"\"\"\n",
    "        if not answer or len(answer.strip()) < 3:\n",
    "            return True\n",
    "        answer_lower = answer.lower()\n",
    "        return any(p in answer_lower for p in self.abstention_phrases)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # METRIC 1: ACCURACY (F1 Score) - IMPROVED\n",
    "    # -------------------------------------------------------------------------\n",
    "    def calc_accuracy(self, prediction: str, ground_truth: str) -> float:\n",
    "        \"\"\"\n",
    "        F1 Score with improved tokenization.\n",
    "        Also checks if key terms from ground truth appear in prediction.\n",
    "        \"\"\"\n",
    "        pred_norm = self._normalize(prediction)\n",
    "        truth_norm = self._normalize(ground_truth)\n",
    "        \n",
    "        if not pred_norm or not truth_norm:\n",
    "            return 0.0\n",
    "        \n",
    "        pred_tokens = set(pred_norm.split())\n",
    "        truth_tokens = set(truth_norm.split())\n",
    "        \n",
    "        # Remove common stopwords for better matching\n",
    "        stopwords = {'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', \n",
    "                     'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will',\n",
    "                     'would', 'could', 'should', 'may', 'might', 'must', 'shall',\n",
    "                     'to', 'of', 'in', 'for', 'on', 'with', 'at', 'by', 'from',\n",
    "                     'as', 'into', 'through', 'during', 'before', 'after', 'and',\n",
    "                     'but', 'or', 'nor', 'so', 'yet', 'both', 'either', 'neither',\n",
    "                     'not', 'only', 'own', 'same', 'than', 'too', 'very', 'just',\n",
    "                     'our', 'your', 'we', 'you', 'they', 'it', 'its'}\n",
    "        \n",
    "        pred_tokens = pred_tokens - stopwords\n",
    "        truth_tokens = truth_tokens - stopwords\n",
    "        \n",
    "        if not pred_tokens or not truth_tokens:\n",
    "            return 0.0\n",
    "        \n",
    "        common = pred_tokens & truth_tokens\n",
    "        if not common:\n",
    "            # Check for partial matches (e.g., \"30-day\" vs \"30 day\")\n",
    "            for pt in pred_tokens:\n",
    "                for tt in truth_tokens:\n",
    "                    if pt in tt or tt in pt:\n",
    "                        common.add(pt)\n",
    "        \n",
    "        if not common:\n",
    "            return 0.0\n",
    "        \n",
    "        precision = len(common) / len(pred_tokens) if pred_tokens else 0\n",
    "        recall = len(common) / len(truth_tokens) if truth_tokens else 0\n",
    "        \n",
    "        if precision + recall == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # METRIC 2: CONFIDENCE HANDLING - FIXED!\n",
    "    # -------------------------------------------------------------------------\n",
    "    def calc_confidence(self, raw_conf: Optional[float], answer: str, \n",
    "                        is_answerable: bool, context_has_answer: bool = True) -> float:\n",
    "        \"\"\"\n",
    "        FIXED confidence scoring:\n",
    "        \n",
    "        For ANSWERABLE questions:\n",
    "            - High confidence + correct answer = GOOD (return raw_conf)\n",
    "            - Low confidence = BAD (model uncertain when it should know)\n",
    "        \n",
    "        For UNANSWERABLE questions:\n",
    "            - Low confidence = GOOD (model knows it can't answer)\n",
    "            - Abstaining = BEST (model refuses to guess)\n",
    "            - High confidence = BAD (model is hallucinating)\n",
    "        \"\"\"\n",
    "        is_abstaining = self._is_abstaining(answer)\n",
    "        conf = raw_conf if raw_conf is not None else 0.5\n",
    "        \n",
    "        if is_answerable:\n",
    "            # For answerable questions, we want HIGH confidence\n",
    "            if is_abstaining:\n",
    "                return 0.2  # Bad: abstaining when answer exists\n",
    "            return conf  # Return actual confidence\n",
    "        else:\n",
    "            # For unanswerable questions, we want LOW confidence or abstention\n",
    "            if is_abstaining:\n",
    "                return 1.0  # Best: model knows it can't answer\n",
    "            elif conf < 0.2:\n",
    "                return 0.9  # Great: very low confidence\n",
    "            elif conf < 0.4:\n",
    "                return 0.7  # Good: low confidence\n",
    "            elif conf < 0.6:\n",
    "                return 0.4  # Mediocre: medium confidence on unanswerable\n",
    "            else:\n",
    "                return 0.1  # Bad: high confidence on unanswerable (hallucinating)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # METRIC 3: QUALITY (Semantic Similarity)\n",
    "    # -------------------------------------------------------------------------\n",
    "    def calc_quality(self, prediction: str, ground_truth: str) -> float:\n",
    "        \"\"\"Semantic similarity using embed_query() from Objective 3.\"\"\"\n",
    "        if not prediction or not ground_truth:\n",
    "            return 0.0\n",
    "        \n",
    "        if len(prediction.strip()) < 3 or len(ground_truth.strip()) < 3:\n",
    "            return 0.0\n",
    "        \n",
    "        try:\n",
    "            pred_emb = self.embed_query(prediction)\n",
    "            truth_emb = self.embed_query(ground_truth)\n",
    "            \n",
    "            similarity = np.dot(pred_emb, truth_emb) / (\n",
    "                np.linalg.norm(pred_emb) * np.linalg.norm(truth_emb)\n",
    "            )\n",
    "            return max(0, float(similarity))\n",
    "        except Exception as e:\n",
    "            print(f\"      [WARN] Quality calc failed: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # METRIC 4: SPEED\n",
    "    # -------------------------------------------------------------------------\n",
    "    def calc_speed(self, response_time_ms: float, max_ms: float = 2000) -> float:\n",
    "        \"\"\"Speed score: faster = higher.\"\"\"\n",
    "        return max(0, min(1, 1 - (response_time_ms / max_ms)))\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # METRIC 5: ROBUSTNESS - IMPROVED\n",
    "    # -------------------------------------------------------------------------\n",
    "    def calc_robustness(self, answer: str, confidence: Optional[float],\n",
    "                        is_answerable: bool, had_error: bool) -> float:\n",
    "        \"\"\"\n",
    "        Robustness scoring with better edge case handling.\n",
    "        \"\"\"\n",
    "        if had_error:\n",
    "            return 0.0\n",
    "        \n",
    "        is_abstaining = self._is_abstaining(answer)\n",
    "        answer_len = len(answer.split()) if answer else 0\n",
    "        conf = confidence if confidence is not None else 0.5\n",
    "        \n",
    "        if is_answerable:\n",
    "            # For answerable: we want actual answers, not too short/long\n",
    "            if is_abstaining:\n",
    "                return 0.3  # Bad: abstaining when answer exists\n",
    "            elif answer_len < 2:\n",
    "                return 0.4  # Too short\n",
    "            elif answer_len > 100:\n",
    "                return 0.5  # Possible over-generation\n",
    "            elif answer_len > 50:\n",
    "                return 0.8  # Slightly long but okay\n",
    "            return 1.0  # Good answer length\n",
    "        else:\n",
    "            # For unanswerable: we want abstention or short uncertain answers\n",
    "            if is_abstaining:\n",
    "                return 1.0  # Best: knows it can't answer\n",
    "            elif answer_len < 5 and conf < 0.5:\n",
    "                return 0.9  # Good: short answer with low confidence\n",
    "            elif answer_len < 10:\n",
    "                return 0.7  # Okay: brief answer\n",
    "            elif answer_len > 30:\n",
    "                return 0.2  # Bad: long answer to unanswerable = hallucination\n",
    "            return 0.5\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 6: QUESTION EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_question(\n",
    "    model_pipeline,\n",
    "    task_type: str,\n",
    "    question: str,\n",
    "    expected: str,\n",
    "    is_answerable: bool,\n",
    "    calculator: MetricCalculator,\n",
    "    debug: bool = False\n",
    ") -> QuestionMetrics:\n",
    "    \"\"\"Evaluate ONE question with ALL 5 metrics.\"\"\"\n",
    "    \n",
    "    # STEP 1: Get context via RAG\n",
    "    context, num_sources, raw_results = get_rag_context(question, debug=debug)\n",
    "    \n",
    "    # Check if expected answer is in context (for extractive models)\n",
    "    context_has_answer = expected.lower() in context.lower() if expected else False\n",
    "    \n",
    "    # STEP 2: Model inference\n",
    "    start_time = time.time()\n",
    "    had_error = False\n",
    "    raw_confidence = None\n",
    "    \n",
    "    try:\n",
    "        if task_type == \"text2text-generation\":\n",
    "            # T5-style: question + context as input\n",
    "            input_text = f\"question: {question} context: {context}\"\n",
    "            output = model_pipeline(input_text, max_length=100, min_length=5)\n",
    "            answer = output[0]['generated_text'].strip()\n",
    "            raw_confidence = 0.7  # T5 doesn't provide confidence\n",
    "        else:\n",
    "            # Extractive QA: question + context\n",
    "            output = model_pipeline(question=question, context=context)\n",
    "            answer = output['answer'].strip()\n",
    "            raw_confidence = output.get('score', 0.5)\n",
    "    except Exception as e:\n",
    "        answer = f\"Error: {str(e)[:50]}\"\n",
    "        had_error = True\n",
    "        raw_confidence = 0.0\n",
    "    \n",
    "    response_time_ms = (time.time() - start_time) * 1000\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"      [DEBUG] Answer: {answer[:100]}...\")\n",
    "        print(f\"      [DEBUG] Raw confidence: {raw_confidence:.3f}\")\n",
    "        print(f\"      [DEBUG] Expected: {expected[:100]}...\")\n",
    "    \n",
    "    # STEP 3: Calculate all 5 metrics\n",
    "    accuracy = calculator.calc_accuracy(answer, expected) if is_answerable else 0.0\n",
    "    confidence = calculator.calc_confidence(raw_confidence, answer, is_answerable, context_has_answer)\n",
    "    quality = calculator.calc_quality(answer, expected) if is_answerable else 0.0\n",
    "    speed = calculator.calc_speed(response_time_ms)\n",
    "    robustness = calculator.calc_robustness(answer, raw_confidence, is_answerable, had_error)\n",
    "    \n",
    "    return QuestionMetrics(\n",
    "        question=question, answer=answer, expected=expected,\n",
    "        context=context[:500], is_answerable=is_answerable, num_sources=num_sources,\n",
    "        accuracy=accuracy, confidence=confidence, quality=quality,\n",
    "        speed=speed, robustness=robustness,\n",
    "        raw_confidence=raw_confidence, response_time_ms=response_time_ms\n",
    "    )\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 7: MODEL EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_model(\n",
    "    name: str,\n",
    "    model_id: str,\n",
    "    task_type: str,\n",
    "    questions: List[tuple],\n",
    "    calculator: MetricCalculator,\n",
    "    weights: Dict[str, float],\n",
    "    debug: bool = False\n",
    ") -> ModelMetrics:\n",
    "    \"\"\"Evaluate ONE model on ALL questions.\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ“Š EVALUATING: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load model\n",
    "    try:\n",
    "        model_pipeline = pipeline(task_type, model=model_id)\n",
    "        print(f\"   âœ… Model loaded: {model_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Failed to load: {e}\")\n",
    "        return ModelMetrics(name=name, model_id=model_id,\n",
    "                           accuracy=0, confidence=0, quality=0,\n",
    "                           speed=0, robustness=0, final_score=0)\n",
    "    \n",
    "    # Evaluate each question\n",
    "    results = []\n",
    "    for q, expected, is_ans in questions:\n",
    "        result = evaluate_question(model_pipeline, task_type, q, expected, is_ans, calculator, debug)\n",
    "        results.append(result)\n",
    "        \n",
    "        status = \"âœ…\" if \"Error\" not in result.answer else \"âŒ\"\n",
    "        ans_type = \"ANS\" if is_ans else \"UNK\"\n",
    "        print(f\"   {status} [{ans_type}] {q[:40]}...\")\n",
    "        print(f\"      Sources: {result.num_sources} | F1={result.accuracy:.2f} Conf={result.confidence:.2f} \"\n",
    "              f\"Qual={result.quality:.2f} Spd={result.speed:.2f} Rob={result.robustness:.2f}\")\n",
    "        if debug:\n",
    "            print(f\"      Answer: {result.answer[:80]}...\")\n",
    "    \n",
    "    # Aggregate metrics\n",
    "    answerable = [r for r in results if r.is_answerable]\n",
    "    unanswerable = [r for r in results if not r.is_answerable]\n",
    "    \n",
    "    # Accuracy and Quality: only from answerable questions\n",
    "    avg_accuracy = np.mean([r.accuracy for r in answerable]) if answerable else 0\n",
    "    avg_quality = np.mean([r.quality for r in answerable]) if answerable else 0\n",
    "    \n",
    "    # Confidence, Speed, Robustness: from ALL questions\n",
    "    avg_confidence = np.mean([r.confidence for r in results])\n",
    "    avg_speed = np.mean([r.speed for r in results])\n",
    "    avg_robustness = np.mean([r.robustness for r in results])\n",
    "    \n",
    "    # Weighted final score\n",
    "    final_score = (\n",
    "        avg_accuracy * weights['accuracy'] +\n",
    "        avg_confidence * weights['confidence'] +\n",
    "        avg_quality * weights['quality'] +\n",
    "        avg_speed * weights['speed'] +\n",
    "        avg_robustness * weights['robustness']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n   ğŸ“ˆ AVERAGES: Acc={avg_accuracy:.2f} Conf={avg_confidence:.2f} \"\n",
    "          f\"Qual={avg_quality:.2f} Spd={avg_speed:.2f} Rob={avg_robustness:.2f}\")\n",
    "    print(f\"   ğŸ“ˆ FINAL SCORE: {final_score:.3f}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del model_pipeline\n",
    "    \n",
    "    return ModelMetrics(\n",
    "        name=name, model_id=model_id,\n",
    "        accuracy=avg_accuracy, confidence=avg_confidence,\n",
    "        quality=avg_quality, speed=avg_speed,\n",
    "        robustness=avg_robustness, final_score=final_score\n",
    "    )\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 8: MAIN EVALUATION FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def run_evaluation(debug: bool = False) -> Tuple[pd.DataFrame, List[ModelMetrics]]:\n",
    "    \"\"\"\n",
    "    Run complete model evaluation.\n",
    "    \n",
    "    Args:\n",
    "        debug: If True, print detailed debug info for each question\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸš€ OBJECTIVE 5: MODEL EVALUATION (FIXED VERSION)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Validate prerequisites\n",
    "    if not validate_prerequisites():\n",
    "        raise RuntimeError(\"Prerequisites not met. Run Objectives 1-4 first.\")\n",
    "    \n",
    "    # Initialize calculator\n",
    "    calculator = MetricCalculator()\n",
    "    \n",
    "    # Evaluate all models\n",
    "    results = []\n",
    "    for name, model_id, task_type in MODELS_CONFIG:\n",
    "        result = evaluate_model(name, model_id, task_type,\n",
    "                               TEST_QUESTIONS, calculator, METRIC_WEIGHTS, debug)\n",
    "        results.append(result)\n",
    "    \n",
    "    # Rank by final score\n",
    "    results.sort(key=lambda x: x.final_score, reverse=True)\n",
    "    for i, r in enumerate(results, 1):\n",
    "        r.rank = i\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame([{\n",
    "        'Rank': r.rank,\n",
    "        'Model': r.name,\n",
    "        'Accuracy': round(r.accuracy, 3),\n",
    "        'Confidence': round(r.confidence, 3),\n",
    "        'Quality': round(r.quality, 3),\n",
    "        'Speed': round(r.speed, 3),\n",
    "        'Robustness': round(r.robustness, 3),\n",
    "        'Final_Score': round(r.final_score, 3)\n",
    "    } for r in results])\n",
    "    \n",
    "    return df, results\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 9: VERIFICATION\n",
    "# ============================================================================\n",
    "\n",
    "def verify_objective5() -> bool:\n",
    "    \"\"\"Verify Objective 5 completed successfully.\"\"\"\n",
    "    \n",
    "    checks = {\n",
    "        'rankings_df exists': 'rankings_df' in globals(),\n",
    "        'model_evaluations exists': 'model_evaluations' in globals(),\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        import os\n",
    "        checks['CSV file created'] = os.path.exists('model_rankings.csv')\n",
    "    except:\n",
    "        checks['CSV file created'] = False\n",
    "    \n",
    "    if 'rankings_df' in globals():\n",
    "        df = globals()['rankings_df']\n",
    "        checks['Has 6 models'] = len(df) == 6\n",
    "        checks['Has all columns'] = all(c in df.columns for c in \n",
    "            ['Rank', 'Model', 'Accuracy', 'Confidence', 'Quality', 'Speed', 'Robustness', 'Final_Score'])\n",
    "        \n",
    "        # Check if scores are reasonable\n",
    "        avg_accuracy = df['Accuracy'].mean()\n",
    "        checks['Accuracy reasonable (>0.3)'] = avg_accuracy > 0.3\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ğŸ“‹ OBJECTIVE 5 VERIFICATION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    all_passed = True\n",
    "    for check, passed in checks.items():\n",
    "        status = \"âœ…\" if passed else \"âŒ\"\n",
    "        print(f\"   {status} {check}\")\n",
    "        if not passed:\n",
    "            all_passed = False\n",
    "    \n",
    "    return all_passed\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 10: EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Run evaluation (set debug=True to see detailed output)\n",
    "    rankings_df, model_evaluations = run_evaluation(debug=False)\n",
    "    \n",
    "    # Store in globals\n",
    "    globals()['rankings_df'] = rankings_df\n",
    "    globals()['model_evaluations'] = model_evaluations\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ† FINAL MODEL RANKINGS\")\n",
    "    print(\"=\"*70)\n",
    "    print(rankings_df.to_string(index=False))\n",
    "    \n",
    "    # Save to CSV\n",
    "    rankings_df.to_csv('model_rankings.csv', index=False)\n",
    "    print(f\"\\nâœ… Saved to: model_rankings.csv\")\n",
    "    \n",
    "    # Winner\n",
    "    winner = rankings_df.iloc[0]\n",
    "    print(f\"\\nğŸ¥‡ WINNER: {winner['Model']} (Score: {winner['Final_Score']})\")\n",
    "    \n",
    "    # Verify\n",
    "    verify_objective5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective 6: Analyze System Limitations\n",
    "\n",
    "### Design Choices & Rationale\n",
    "\n",
    "### Analysis Framework\n",
    "\n",
    "| Category | What We Analyze |\n",
    "|----------|-----------------|\n",
    "| **System Limitations** | What can/cannot be answered, failure modes |\n",
    "| **Real-World Applications** | Deployment scenarios, business value |\n",
    "| **Scalability** | Performance at scale, bottlenecks |\n",
    "| **Accuracy & Reliability** | Error rates, confidence calibration |\n",
    "| **User Experience** | Interaction design, response quality |\n",
    "| **Deployment** | Infrastructure, costs, maintenance |\n",
    "| **Improvements** | Future enhancements, optimizations |\n",
    "\n",
    "### Metrics to Compute\n",
    "\n",
    "| Metric | Description | Source |\n",
    "|--------|-------------|--------|\n",
    "| Answerable Accuracy | % correct on answerable questions | Objective 5 results |\n",
    "| Unanswerable Detection | % correctly identified as unanswerable | Objective 5 results |\n",
    "| Avg Response Time | Mean time per query | Objective 5 results |\n",
    "| Knowledge Coverage | % of topics covered in KB | Q&A database analysis |\n",
    "| Retrieval Precision | % relevant docs in top-k | FAISS search analysis |\n",
    "| Confidence Calibration | Correlation: confidence vs correctness | Model evaluation |\n",
    "\n",
    "### Reuse from Previous Objectives\n",
    "\n",
    "| Component | Source | Used For |\n",
    "|-----------|--------|----------|\n",
    "| `model_evaluations` | Objective 5 | Performance metrics |\n",
    "| `qa_database` | Objective 2 | Knowledge coverage |\n",
    "| `rag_results` | Objective 4 | RAG pipeline analysis |\n",
    "| `faiss_index` | Objective 3 | Retrieval analysis |\n",
    "\n",
    "### Output Files\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `system_analysis.txt` | Complete analysis report |\n",
    "| `limitations_summary.csv` | Quantitative metrics |\n",
    "| `recommendations.txt` | Improvement recommendations |\n",
    "\n",
    "### Analysis Sections\n",
    "\n",
    "**1. System Limitations**\n",
    "- Questions answerable vs not answerable\n",
    "- Failure mode classification\n",
    "- Edge cases identified\n",
    "\n",
    "**2. Real-World Applications**\n",
    "- Customer service chatbot\n",
    "- Internal FAQ system\n",
    "- Knowledge base assistant\n",
    "- Help desk automation\n",
    "\n",
    "**3. Scalability Analysis**\n",
    "- Current: 21 Q&A pairs\n",
    "- Projected: 1K, 10K, 100K entries\n",
    "- Bottlenecks: embedding time, search latency\n",
    "\n",
    "**4. Accuracy & Reliability**\n",
    "- Best model performance\n",
    "- Error rate breakdown\n",
    "- Confidence score analysis\n",
    "\n",
    "**5. User Experience**\n",
    "- Response time requirements\n",
    "- Answer quality expectations\n",
    "- Interaction patterns\n",
    "\n",
    "**6. Deployment Considerations**\n",
    "- Infrastructure: GPU vs CPU\n",
    "- Costs: Model hosting, API calls\n",
    "- Maintenance: KB updates, model updates\n",
    "\n",
    "**7. Improvement Opportunities**\n",
    "- More Q&A pairs\n",
    "- Domain-specific fine-tuning\n",
    "- Better retrieval (hybrid search)\n",
    "- Response generation improvements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OBJECTIVE 6: ANALYZE SYSTEM LIMITATIONS\n",
    "# ============================================================================\n",
    "#\n",
    "# ANALYSIS FRAMEWORK:\n",
    "#   1. System Limitations - What can/cannot be answered, failure modes\n",
    "#   2. Real-World Applications - Deployment scenarios, business value\n",
    "#   3. Scalability - Performance at scale, bottlenecks\n",
    "#   4. Accuracy & Reliability - Error rates, confidence calibration\n",
    "#   5. User Experience - Interaction design, response quality\n",
    "#   6. Deployment - Infrastructure, costs, maintenance\n",
    "#   7. Improvements - Future enhancements, optimizations\n",
    "#\n",
    "# REUSES FROM PREVIOUS OBJECTIVES:\n",
    "#   - model_evaluations (Objective 5)\n",
    "#   - qa_database (Objective 2)\n",
    "#   - rag_results (Objective 4)\n",
    "#   - faiss_index (Objective 3)\n",
    "#\n",
    "# PREREQUISITES: Run Objectives 1-5 first\n",
    "#\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 1: IMPORTS & VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "from typing import List, Dict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "except ImportError as e:\n",
    "    raise ImportError(f\"Missing: {e}. Run: pip install pandas numpy\")\n",
    "\n",
    "\n",
    "def validate_prerequisites():\n",
    "    \"\"\"Ensure Objectives 1-5 were run first.\"\"\"\n",
    "    required = {\n",
    "        'Objective 2': ['qa_database'],\n",
    "        'Objective 3': ['faiss_index'],\n",
    "        'Objective 4': ['rag_results'],\n",
    "        'Objective 5': ['model_evaluations']\n",
    "    }\n",
    "    \n",
    "    all_missing = []\n",
    "    for objective, items in required.items():\n",
    "        missing = [item for item in items if item not in globals()]\n",
    "        if missing:\n",
    "            all_missing.append(f\"{objective}: {missing}\")\n",
    "    \n",
    "    if all_missing:\n",
    "        raise RuntimeError(f\"Missing prerequisites:\\n\" + \"\\n\".join(all_missing))\n",
    "    \n",
    "    print(\"âœ… All prerequisites validated\")\n",
    "    return True\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 2: CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "OUTPUT_DIR = \"data/system_analysis\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 3: DATA CLASSES\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class SystemMetrics:\n",
    "    \"\"\"Container for system analysis metrics.\"\"\"\n",
    "    # Knowledge Base\n",
    "    total_qa_pairs: int = 0\n",
    "    categories_covered: int = 0\n",
    "    avg_answer_length: float = 0.0\n",
    "    \n",
    "    # Performance\n",
    "    best_model: str = \"\"\n",
    "    best_model_score: float = 0.0\n",
    "    avg_response_time: float = 0.0\n",
    "    fastest_model: str = \"\"\n",
    "    fastest_time: float = 0.0\n",
    "    \n",
    "    # Accuracy\n",
    "    answerable_accuracy: float = 0.0\n",
    "    unanswerable_detection: float = 0.0\n",
    "    avg_confidence_gap: float = 0.0\n",
    "    \n",
    "    # Retrieval\n",
    "    faiss_index_size: int = 0\n",
    "    embedding_dimension: int = 0\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 4: ANALYSIS FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_knowledge_base(qa_database: List[Dict]) -> Dict:\n",
    "    \"\"\"Analyze the Q&A knowledge base.\"\"\"\n",
    "    \n",
    "    # Basic stats\n",
    "    total = len(qa_database)\n",
    "    \n",
    "    # Category analysis\n",
    "    categories = {}\n",
    "    for qa in qa_database:\n",
    "        cat = qa.get('category', 'unknown')\n",
    "        categories[cat] = categories.get(cat, 0) + 1\n",
    "    \n",
    "    # Answer length analysis\n",
    "    answer_lengths = [len(qa['answer']) for qa in qa_database]\n",
    "    avg_length = sum(answer_lengths) / len(answer_lengths) if answer_lengths else 0\n",
    "    \n",
    "    # Answerable vs unanswerable\n",
    "    answerable = sum(1 for qa in qa_database if qa.get('answerable', True))\n",
    "    unanswerable = total - answerable\n",
    "    \n",
    "    return {\n",
    "        'total_pairs': total,\n",
    "        'categories': categories,\n",
    "        'num_categories': len(categories),\n",
    "        'avg_answer_length': avg_length,\n",
    "        'min_answer_length': min(answer_lengths) if answer_lengths else 0,\n",
    "        'max_answer_length': max(answer_lengths) if answer_lengths else 0,\n",
    "        'answerable_count': answerable,\n",
    "        'unanswerable_count': unanswerable\n",
    "    }\n",
    "\n",
    "\n",
    "def analyze_model_performance(evaluations: List) -> Dict:\n",
    "    \"\"\"Analyze model evaluation results.\"\"\"\n",
    "    \n",
    "    if not evaluations:\n",
    "        return {}\n",
    "    \n",
    "    # Find best model by average score\n",
    "    best = max(evaluations, \n",
    "               key=lambda e: (e.scores.manual_weighted + e.scores.llm_weighted) / 2)\n",
    "    best_avg = (best.scores.manual_weighted + best.scores.llm_weighted) / 2\n",
    "    \n",
    "    # Find fastest model\n",
    "    valid_times = [(e, e.avg_response_time) for e in evaluations if e.avg_response_time > 0]\n",
    "    fastest = min(valid_times, key=lambda x: x[1]) if valid_times else (None, 0)\n",
    "    \n",
    "    # Average metrics across all models\n",
    "    avg_manual = sum(e.scores.manual_weighted for e in evaluations) / len(evaluations)\n",
    "    avg_llm = sum(e.scores.llm_weighted for e in evaluations) / len(evaluations)\n",
    "    avg_time = sum(e.avg_response_time for e in evaluations) / len(evaluations)\n",
    "    avg_conf_gap = sum(e.confidence_gap for e in evaluations) / len(evaluations)\n",
    "    \n",
    "    # Model rankings\n",
    "    ranked = sorted(evaluations, \n",
    "                   key=lambda e: (e.scores.manual_weighted + e.scores.llm_weighted) / 2,\n",
    "                   reverse=True)\n",
    "    \n",
    "    return {\n",
    "        'best_model': best.model_name,\n",
    "        'best_model_score': best_avg,\n",
    "        'best_model_size': best.model_size,\n",
    "        'fastest_model': fastest[0].model_name if fastest[0] else 'N/A',\n",
    "        'fastest_time': fastest[1],\n",
    "        'avg_manual_score': avg_manual,\n",
    "        'avg_llm_score': avg_llm,\n",
    "        'avg_response_time': avg_time,\n",
    "        'avg_confidence_gap': avg_conf_gap,\n",
    "        'rankings': [(e.model_name, (e.scores.manual_weighted + e.scores.llm_weighted) / 2) \n",
    "                     for e in ranked]\n",
    "    }\n",
    "\n",
    "\n",
    "def analyze_retrieval_system(faiss_index) -> Dict:\n",
    "    \"\"\"Analyze FAISS retrieval system.\"\"\"\n",
    "    \n",
    "    return {\n",
    "        'index_size': faiss_index.ntotal,\n",
    "        'dimension': faiss_index.d,\n",
    "        'index_type': type(faiss_index).__name__\n",
    "    }\n",
    "\n",
    "\n",
    "def identify_failure_modes(evaluations: List) -> List[Dict]:\n",
    "    \"\"\"Identify common failure modes from model evaluations.\"\"\"\n",
    "    \n",
    "    failures = []\n",
    "    \n",
    "    for e in evaluations:\n",
    "        for r in e.responses:\n",
    "            if not r.success:\n",
    "                failures.append({\n",
    "                    'model': r.model_name,\n",
    "                    'question': r.question,\n",
    "                    'error': r.error_message,\n",
    "                    'type': 'execution_error'\n",
    "                })\n",
    "            elif r.is_answerable and len(r.answer.strip()) < 10:\n",
    "                failures.append({\n",
    "                    'model': r.model_name,\n",
    "                    'question': r.question,\n",
    "                    'answer': r.answer,\n",
    "                    'type': 'incomplete_answer'\n",
    "                })\n",
    "            elif not r.is_answerable and r.confidence > 0.7:\n",
    "                failures.append({\n",
    "                    'model': r.model_name,\n",
    "                    'question': r.question,\n",
    "                    'answer': r.answer,\n",
    "                    'confidence': r.confidence,\n",
    "                    'type': 'false_confidence'\n",
    "                })\n",
    "    \n",
    "    return failures\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 5: REPORT GENERATION\n",
    "# ============================================================================\n",
    "\n",
    "def generate_limitations_report(kb_analysis: Dict, perf_analysis: Dict, \n",
    "                                retrieval_analysis: Dict, failures: List) -> str:\n",
    "    \"\"\"Generate the system limitations section.\"\"\"\n",
    "    \n",
    "    report = \"\"\"\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "                        1. SYSTEM LIMITATIONS\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "WHAT THE SYSTEM CAN ANSWER:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  âœ… Questions directly covered in the Q&A database\n",
    "  âœ… Questions semantically similar to database entries\n",
    "  âœ… Factual questions about products, policies, procedures\n",
    "  \n",
    "  Coverage: {total} Q&A pairs across {cats} categories\n",
    "  Answerable questions in database: {ans}\n",
    "  Unanswerable examples in database: {unans}\n",
    "\n",
    "WHAT THE SYSTEM CANNOT ANSWER:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  âŒ Questions requiring reasoning beyond retrieved context\n",
    "  âŒ Questions about topics not in knowledge base\n",
    "  âŒ Questions requiring real-time or external data\n",
    "  âŒ Complex multi-step questions\n",
    "  âŒ Subjective or opinion-based questions\n",
    "\n",
    "FAILURE MODES IDENTIFIED:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  Total failures detected: {fail_count}\n",
    "  \n",
    "  â€¢ Incomplete Answers: Model returns fragment instead of full answer\n",
    "    Example: Returning \"30\" instead of \"30-day return policy\"\n",
    "    \n",
    "  â€¢ False Confidence: High confidence on unanswerable questions\n",
    "    Risk: Model may hallucinate answers\n",
    "    \n",
    "  â€¢ Execution Errors: Model fails to generate response\n",
    "    Cause: Context too long, model limitations\n",
    "    \n",
    "  â€¢ Retrieval Misses: Wrong context retrieved for question\n",
    "    Cause: Semantic gap between query and database\n",
    "\n",
    "EDGE CASES:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  â€¢ Ambiguous questions (multiple valid interpretations)\n",
    "  â€¢ Questions with typos or grammatical errors\n",
    "  â€¢ Questions in different languages\n",
    "  â€¢ Very long or complex questions\n",
    "  â€¢ Questions combining multiple topics\n",
    "\n",
    "\"\"\".format(\n",
    "        total=kb_analysis['total_pairs'],\n",
    "        cats=kb_analysis['num_categories'],\n",
    "        ans=kb_analysis['answerable_count'],\n",
    "        unans=kb_analysis['unanswerable_count'],\n",
    "        fail_count=len(failures)\n",
    "    )\n",
    "    \n",
    "    return report\n",
    "\n",
    "\n",
    "def generate_applications_report() -> str:\n",
    "    \"\"\"Generate the real-world applications section.\"\"\"\n",
    "    \n",
    "    return \"\"\"\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "                    2. REAL-WORLD APPLICATIONS\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "SUITABLE DEPLOYMENT SCENARIOS:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "  ğŸ“ CUSTOMER SERVICE CHATBOT\n",
    "     â€¢ First-line support for common questions\n",
    "     â€¢ Reduce load on human agents\n",
    "     â€¢ 24/7 availability\n",
    "     â€¢ Consistent responses\n",
    "     \n",
    "  ğŸ“š INTERNAL FAQ SYSTEM\n",
    "     â€¢ Employee self-service portal\n",
    "     â€¢ HR policy questions\n",
    "     â€¢ IT helpdesk automation\n",
    "     â€¢ Onboarding assistance\n",
    "     \n",
    "  ğŸ¢ KNOWLEDGE BASE ASSISTANT\n",
    "     â€¢ Product documentation search\n",
    "     â€¢ Technical support queries\n",
    "     â€¢ Training material access\n",
    "     â€¢ Process documentation\n",
    "     \n",
    "  ğŸ« HELP DESK AUTOMATION\n",
    "     â€¢ Ticket deflection\n",
    "     â€¢ Auto-response for common issues\n",
    "     â€¢ Escalation routing\n",
    "     â€¢ Response suggestions for agents\n",
    "\n",
    "BUSINESS VALUE:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  â€¢ Reduced response time: Instant answers vs waiting for human\n",
    "  â€¢ Cost savings: Fewer human agents needed for basic queries\n",
    "  â€¢ Consistency: Same answer every time for same question\n",
    "  â€¢ Scalability: Handle more queries without linear cost increase\n",
    "  â€¢ Analytics: Track common questions and gaps\n",
    "\n",
    "NOT SUITABLE FOR:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  âŒ Medical diagnosis or advice\n",
    "  âŒ Legal guidance\n",
    "  âŒ Financial recommendations\n",
    "  âŒ Emergency situations\n",
    "  âŒ Highly personalized queries requiring account access\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generate_scalability_report(kb_analysis: Dict, retrieval_analysis: Dict) -> str:\n",
    "    \"\"\"Generate the scalability analysis section.\"\"\"\n",
    "    \n",
    "    return \"\"\"\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "                      3. SCALABILITY ANALYSIS\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "CURRENT SYSTEM SCALE:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  â€¢ Q&A Pairs: {total}\n",
    "  â€¢ FAISS Index Size: {index_size} vectors\n",
    "  â€¢ Embedding Dimension: {dim}\n",
    "  â€¢ Index Type: {index_type}\n",
    "\n",
    "PROJECTED PERFORMANCE AT SCALE:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  \n",
    "  â”‚ Scale      â”‚ Entries â”‚ Index Size â”‚ Search Time â”‚ Notes           â”‚\n",
    "  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "  â”‚ Current    â”‚ {total:<7} â”‚ ~1 MB      â”‚ <10ms       â”‚ âœ… Excellent    â”‚\n",
    "  â”‚ Small      â”‚ 1,000   â”‚ ~5 MB      â”‚ <20ms       â”‚ âœ… Good         â”‚\n",
    "  â”‚ Medium     â”‚ 10,000  â”‚ ~50 MB     â”‚ <50ms       â”‚ âœ… Acceptable   â”‚\n",
    "  â”‚ Large      â”‚ 100,000 â”‚ ~500 MB    â”‚ <100ms      â”‚ âš ï¸ May need IVF â”‚\n",
    "  â”‚ Very Large â”‚ 1M+     â”‚ ~5 GB      â”‚ Varies      â”‚ âŒ Need HNSW    â”‚\n",
    "\n",
    "BOTTLENECKS:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  1. EMBEDDING GENERATION\n",
    "     â€¢ Current: ~50ms per query\n",
    "     â€¢ At scale: Batch processing recommended\n",
    "     â€¢ Solution: GPU acceleration, caching\n",
    "     \n",
    "  2. FAISS SEARCH\n",
    "     â€¢ Current: IndexFlatL2 (exact search)\n",
    "     â€¢ At scale: Need approximate search (IVF, HNSW)\n",
    "     â€¢ Trade-off: Speed vs accuracy\n",
    "     \n",
    "  3. LLM GENERATION\n",
    "     â€¢ Current: ~1-2s per response\n",
    "     â€¢ At scale: Main bottleneck\n",
    "     â€¢ Solution: Smaller models, caching, batching\n",
    "\n",
    "RECOMMENDATIONS FOR SCALE:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  â€¢ 1K-10K: Keep current setup, add caching\n",
    "  â€¢ 10K-100K: Switch to FAISS IVF index\n",
    "  â€¢ 100K+: Use HNSW, consider vector database (Pinecone, Weaviate)\n",
    "  â€¢ 1M+: Distributed architecture, sharding\n",
    "\n",
    "\"\"\".format(\n",
    "        total=kb_analysis['total_pairs'],\n",
    "        index_size=retrieval_analysis['index_size'],\n",
    "        dim=retrieval_analysis['dimension'],\n",
    "        index_type=retrieval_analysis['index_type']\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_accuracy_report(perf_analysis: Dict) -> str:\n",
    "    \"\"\"Generate the accuracy and reliability section.\"\"\"\n",
    "    \n",
    "    rankings_str = \"\\n\".join([f\"     {i+1}. {name}: {score:.1%}\" \n",
    "                              for i, (name, score) in enumerate(perf_analysis['rankings'])])\n",
    "    \n",
    "    return \"\"\"\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "                    4. ACCURACY & RELIABILITY\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "OVERALL PERFORMANCE:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  â€¢ Best Model: {best_model}\n",
    "  â€¢ Best Score: {best_score:.1%}\n",
    "  â€¢ Model Size: {best_size}\n",
    "  \n",
    "  â€¢ Average Manual Score: {avg_manual:.1%}\n",
    "  â€¢ Average LLM-Judge Score: {avg_llm:.1%}\n",
    "  â€¢ Average Confidence Gap: {avg_gap:+.1%}\n",
    "\n",
    "MODEL RANKINGS:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "{rankings}\n",
    "\n",
    "RELIABILITY FACTORS:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  \n",
    "  âœ… HIGH RELIABILITY:\n",
    "     â€¢ Questions with exact matches in database\n",
    "     â€¢ High confidence scores (>0.7) on answerable questions\n",
    "     â€¢ Common, frequently asked questions\n",
    "     \n",
    "  âš ï¸ MEDIUM RELIABILITY:\n",
    "     â€¢ Semantically similar but not exact matches\n",
    "     â€¢ Medium confidence scores (0.4-0.7)\n",
    "     â€¢ Paraphrased questions\n",
    "     \n",
    "  âŒ LOW RELIABILITY:\n",
    "     â€¢ Low confidence scores (<0.4)\n",
    "     â€¢ Out-of-scope questions\n",
    "     â€¢ Ambiguous or complex queries\n",
    "\n",
    "CONFIDENCE CALIBRATION:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  â€¢ Ideal: High confidence = correct, Low confidence = uncertain\n",
    "  â€¢ Current Gap: {avg_gap:+.1%} (answerable vs unanswerable)\n",
    "  â€¢ Assessment: {\"Good\" if perf_analysis['avg_confidence_gap'] > 0.1 else \"Needs improvement\"}\n",
    "  \n",
    "  Recommendation: Use confidence threshold of 0.5 for production\n",
    "  Below threshold: Escalate to human or ask for clarification\n",
    "\n",
    "\"\"\".format(\n",
    "        best_model=perf_analysis['best_model'],\n",
    "        best_score=perf_analysis['best_model_score'],\n",
    "        best_size=perf_analysis['best_model_size'],\n",
    "        avg_manual=perf_analysis['avg_manual_score'],\n",
    "        avg_llm=perf_analysis['avg_llm_score'],\n",
    "        avg_gap=perf_analysis['avg_confidence_gap'],\n",
    "        rankings=rankings_str\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_ux_report(perf_analysis: Dict) -> str:\n",
    "    \"\"\"Generate the user experience section.\"\"\"\n",
    "    \n",
    "    return \"\"\"\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "                       5. USER EXPERIENCE\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "RESPONSE TIME ANALYSIS:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  â€¢ Average Response Time: {avg_time:.2f}s\n",
    "  â€¢ Fastest Model: {fastest} ({fastest_time:.3f}s)\n",
    "  \n",
    "  â”‚ Latency    â”‚ User Perception    â”‚ Recommendation      â”‚\n",
    "  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "  â”‚ <0.5s      â”‚ Instant            â”‚ âœ… Excellent        â”‚\n",
    "  â”‚ 0.5-1s     â”‚ Fast               â”‚ âœ… Good             â”‚\n",
    "  â”‚ 1-2s       â”‚ Acceptable         â”‚ âš ï¸ Show loading     â”‚\n",
    "  â”‚ 2-5s       â”‚ Slow               â”‚ âŒ Optimize needed  â”‚\n",
    "  â”‚ >5s        â”‚ Frustrating        â”‚ âŒ Major issue      â”‚\n",
    "\n",
    "INTERACTION DESIGN RECOMMENDATIONS:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  \n",
    "  1. LOADING INDICATORS\n",
    "     â€¢ Show typing indicator while generating\n",
    "     â€¢ Display \"Searching...\" during retrieval\n",
    "     â€¢ Progress feedback for long queries\n",
    "     \n",
    "  2. RESPONSE FORMATTING\n",
    "     â€¢ Clear, concise answers\n",
    "     â€¢ Bullet points for lists\n",
    "     â€¢ Links to full documentation\n",
    "     â€¢ \"Was this helpful?\" feedback\n",
    "     \n",
    "  3. FALLBACK HANDLING\n",
    "     â€¢ \"I'm not sure, let me connect you with a human\"\n",
    "     â€¢ Suggest related questions\n",
    "     â€¢ Offer to rephrase or clarify\n",
    "     \n",
    "  4. CONFIDENCE DISPLAY\n",
    "     â€¢ Don't show raw confidence scores to users\n",
    "     â€¢ Use phrases: \"I'm confident that...\" vs \"I think...\"\n",
    "     â€¢ Offer to escalate for low-confidence answers\n",
    "\n",
    "ACCESSIBILITY CONSIDERATIONS:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  â€¢ Screen reader compatibility\n",
    "  â€¢ Keyboard navigation\n",
    "  â€¢ Clear error messages\n",
    "  â€¢ Multiple input methods (text, voice)\n",
    "\n",
    "\"\"\".format(\n",
    "        avg_time=perf_analysis['avg_response_time'],\n",
    "        fastest=perf_analysis['fastest_model'],\n",
    "        fastest_time=perf_analysis['fastest_time']\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_deployment_report() -> str:\n",
    "    \"\"\"Generate the deployment considerations section.\"\"\"\n",
    "    \n",
    "    return \"\"\"\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "                   6. DEPLOYMENT CONSIDERATIONS\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "INFRASTRUCTURE REQUIREMENTS:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "  MINIMUM (Development/Testing):\n",
    "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "  â”‚ â€¢ CPU: 4 cores                                                  â”‚\n",
    "  â”‚ â€¢ RAM: 16 GB                                                    â”‚\n",
    "  â”‚ â€¢ Storage: 10 GB                                                â”‚\n",
    "  â”‚ â€¢ GPU: Optional (CPU inference possible)                        â”‚\n",
    "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "  \n",
    "  RECOMMENDED (Production):\n",
    "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "  â”‚ â€¢ CPU: 8+ cores                                                 â”‚\n",
    "  â”‚ â€¢ RAM: 32 GB                                                    â”‚\n",
    "  â”‚ â€¢ Storage: 50 GB SSD                                            â”‚\n",
    "  â”‚ â€¢ GPU: NVIDIA T4 or better (for fast inference)                 â”‚\n",
    "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "COST ESTIMATES (Monthly):\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  \n",
    "  â”‚ Deployment      â”‚ Compute    â”‚ Storage â”‚ Total    â”‚\n",
    "  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "  â”‚ CPU-only (AWS)  â”‚ $150-300   â”‚ $10     â”‚ ~$200    â”‚\n",
    "  â”‚ GPU (AWS g4dn)  â”‚ $400-800   â”‚ $10     â”‚ ~$500    â”‚\n",
    "  â”‚ Serverless      â”‚ Pay/query  â”‚ $10     â”‚ Variable â”‚\n",
    "  â”‚ On-premise      â”‚ One-time   â”‚ -       â”‚ ~$5000   â”‚\n",
    "\n",
    "MAINTENANCE REQUIREMENTS:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  \n",
    "  DAILY:\n",
    "  â€¢ Monitor response times and error rates\n",
    "  â€¢ Check system health and logs\n",
    "  \n",
    "  WEEKLY:\n",
    "  â€¢ Review low-confidence responses\n",
    "  â€¢ Analyze unanswered questions\n",
    "  â€¢ Update metrics dashboard\n",
    "  \n",
    "  MONTHLY:\n",
    "  â€¢ Add new Q&A pairs based on gaps\n",
    "  â€¢ Retrain/update embeddings if KB changes significantly\n",
    "  â€¢ Performance benchmarking\n",
    "  \n",
    "  QUARTERLY:\n",
    "  â€¢ Evaluate newer models\n",
    "  â€¢ User satisfaction survey\n",
    "  â€¢ ROI analysis\n",
    "\n",
    "MONITORING & LOGGING:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  â€¢ Query logs: What users are asking\n",
    "  â€¢ Response logs: What the system answered\n",
    "  â€¢ Latency metrics: Response times\n",
    "  â€¢ Error tracking: Failed queries\n",
    "  â€¢ Confidence distribution: Score patterns\n",
    "  â€¢ User feedback: Thumbs up/down\n",
    "\n",
    "SECURITY CONSIDERATIONS:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  â€¢ Input sanitization (prevent prompt injection)\n",
    "  â€¢ Rate limiting (prevent abuse)\n",
    "  â€¢ Data encryption (at rest and in transit)\n",
    "  â€¢ Access control (who can query, who can update KB)\n",
    "  â€¢ Audit logging (compliance requirements)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generate_improvements_report() -> str:\n",
    "    \"\"\"Generate the improvement opportunities section.\"\"\"\n",
    "    \n",
    "    return \"\"\"\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "                  7. IMPROVEMENT OPPORTUNITIES\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "SHORT-TERM IMPROVEMENTS (1-2 weeks):\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  \n",
    "  1. EXPAND KNOWLEDGE BASE\n",
    "     â€¢ Add more Q&A pairs (target: 100+)\n",
    "     â€¢ Cover more edge cases\n",
    "     â€¢ Add variations of common questions\n",
    "     \n",
    "  2. IMPROVE PROMPTS\n",
    "     â€¢ Refine system prompt for better responses\n",
    "     â€¢ Add few-shot examples for edge cases\n",
    "     â€¢ Better handling of \"I don't know\"\n",
    "     \n",
    "  3. CACHING\n",
    "     â€¢ Cache frequent queries\n",
    "     â€¢ Cache embeddings for common questions\n",
    "     â€¢ Reduce redundant LLM calls\n",
    "\n",
    "MEDIUM-TERM IMPROVEMENTS (1-3 months):\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  \n",
    "  1. HYBRID SEARCH\n",
    "     â€¢ Combine semantic search with keyword search\n",
    "     â€¢ BM25 + FAISS for better retrieval\n",
    "     â€¢ Reranking with cross-encoder\n",
    "     \n",
    "  2. FINE-TUNING\n",
    "     â€¢ Fine-tune embedding model on domain data\n",
    "     â€¢ Fine-tune QA model on company-specific Q&A\n",
    "     â€¢ Domain adaptation for better accuracy\n",
    "     \n",
    "  3. MULTI-TURN CONVERSATION\n",
    "     â€¢ Context tracking across turns\n",
    "     â€¢ Follow-up question handling\n",
    "     â€¢ Conversation memory\n",
    "\n",
    "LONG-TERM IMPROVEMENTS (3-6 months):\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  \n",
    "  1. ADVANCED RETRIEVAL\n",
    "     â€¢ Query expansion and rewriting\n",
    "     â€¢ Multi-index search (products, policies, FAQs)\n",
    "     â€¢ Knowledge graph integration\n",
    "     \n",
    "  2. ACTIVE LEARNING\n",
    "     â€¢ Learn from user feedback\n",
    "     â€¢ Automatic KB updates from conversations\n",
    "     â€¢ Continuous model improvement\n",
    "     \n",
    "  3. MULTI-MODAL\n",
    "     â€¢ Support for images (product photos)\n",
    "     â€¢ Voice input/output\n",
    "     â€¢ Document understanding\n",
    "\n",
    "METRICS TO TRACK FOR IMPROVEMENT:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  â€¢ Answer accuracy (human evaluation)\n",
    "  â€¢ User satisfaction (feedback scores)\n",
    "  â€¢ Resolution rate (% questions answered without escalation)\n",
    "  â€¢ Time to resolution\n",
    "  â€¢ Knowledge base coverage (% of queries with relevant KB entry)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 6: SAVE FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def save_analysis_report(full_report: str):\n",
    "    \"\"\"Save the complete analysis report.\"\"\"\n",
    "    filepath = os.path.join(OUTPUT_DIR, \"system_analysis.txt\")\n",
    "    with open(filepath, 'w') as f:\n",
    "        f.write(full_report)\n",
    "    print(f\"âœ… Saved: {filepath}\")\n",
    "\n",
    "\n",
    "def save_metrics_csv(kb_analysis: Dict, perf_analysis: Dict, retrieval_analysis: Dict):\n",
    "    \"\"\"Save metrics to CSV.\"\"\"\n",
    "    metrics = {\n",
    "        'metric': [],\n",
    "        'value': []\n",
    "    }\n",
    "    \n",
    "    # KB metrics\n",
    "    metrics['metric'].append('Total Q&A Pairs')\n",
    "    metrics['value'].append(kb_analysis['total_pairs'])\n",
    "    metrics['metric'].append('Categories')\n",
    "    metrics['value'].append(kb_analysis['num_categories'])\n",
    "    metrics['metric'].append('Avg Answer Length')\n",
    "    metrics['value'].append(f\"{kb_analysis['avg_answer_length']:.0f}\")\n",
    "    \n",
    "    # Performance metrics\n",
    "    metrics['metric'].append('Best Model')\n",
    "    metrics['value'].append(perf_analysis['best_model'])\n",
    "    metrics['metric'].append('Best Score')\n",
    "    metrics['value'].append(f\"{perf_analysis['best_model_score']:.1%}\")\n",
    "    metrics['metric'].append('Avg Response Time')\n",
    "    metrics['value'].append(f\"{perf_analysis['avg_response_time']:.3f}s\")\n",
    "    metrics['metric'].append('Fastest Model')\n",
    "    metrics['value'].append(perf_analysis['fastest_model'])\n",
    "    \n",
    "    # Retrieval metrics\n",
    "    metrics['metric'].append('FAISS Index Size')\n",
    "    metrics['value'].append(retrieval_analysis['index_size'])\n",
    "    metrics['metric'].append('Embedding Dimension')\n",
    "    metrics['value'].append(retrieval_analysis['dimension'])\n",
    "    \n",
    "    df = pd.DataFrame(metrics)\n",
    "    filepath = os.path.join(OUTPUT_DIR, \"metrics_summary.csv\")\n",
    "    df.to_csv(filepath, index=False)\n",
    "    print(f\"âœ… Saved: {filepath}\")\n",
    "\n",
    "\n",
    "def save_recommendations(recommendations: str):\n",
    "    \"\"\"Save recommendations to file.\"\"\"\n",
    "    filepath = os.path.join(OUTPUT_DIR, \"recommendations.txt\")\n",
    "    with open(filepath, 'w') as f:\n",
    "        f.write(recommendations)\n",
    "    print(f\"âœ… Saved: {filepath}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 7: EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"         OBJECTIVE 6: ANALYZE SYSTEM LIMITATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Step 1: Validate\n",
    "print(\"\\nğŸ” STEP 1: Validate Prerequisites\")\n",
    "print(\"â”€\"*70)\n",
    "validate_prerequisites()\n",
    "\n",
    "# Get data from previous objectives\n",
    "qa_database = globals()['qa_database']\n",
    "faiss_index = globals()['faiss_index']\n",
    "model_evaluations = globals()['model_evaluations']\n",
    "\n",
    "# Step 2: Analyze Components\n",
    "print(\"\\nğŸ“Š STEP 2: Analyze System Components\")\n",
    "print(\"â”€\"*70)\n",
    "\n",
    "print(\"   Analyzing knowledge base...\")\n",
    "kb_analysis = analyze_knowledge_base(qa_database)\n",
    "print(f\"   âœ“ {kb_analysis['total_pairs']} Q&A pairs, {kb_analysis['num_categories']} categories\")\n",
    "\n",
    "print(\"   Analyzing model performance...\")\n",
    "perf_analysis = analyze_model_performance(model_evaluations)\n",
    "print(f\"   âœ“ Best model: {perf_analysis['best_model']} ({perf_analysis['best_model_score']:.1%})\")\n",
    "\n",
    "print(\"   Analyzing retrieval system...\")\n",
    "retrieval_analysis = analyze_retrieval_system(faiss_index)\n",
    "print(f\"   âœ“ FAISS index: {retrieval_analysis['index_size']} vectors\")\n",
    "\n",
    "print(\"   Identifying failure modes...\")\n",
    "failures = identify_failure_modes(model_evaluations)\n",
    "print(f\"   âœ“ {len(failures)} failure cases identified\")\n",
    "\n",
    "# Step 3: Generate Reports\n",
    "print(\"\\nğŸ“ STEP 3: Generate Analysis Reports\")\n",
    "print(\"â”€\"*70)\n",
    "\n",
    "# Generate all sections\n",
    "limitations = generate_limitations_report(kb_analysis, perf_analysis, retrieval_analysis, failures)\n",
    "applications = generate_applications_report()\n",
    "scalability = generate_scalability_report(kb_analysis, retrieval_analysis)\n",
    "accuracy = generate_accuracy_report(perf_analysis)\n",
    "ux = generate_ux_report(perf_analysis)\n",
    "deployment = generate_deployment_report()\n",
    "improvements = generate_improvements_report()\n",
    "\n",
    "# Combine into full report\n",
    "full_report = f\"\"\"\n",
    "{'='*70}\n",
    "           SYSTEM ANALYSIS REPORT - RAG CUSTOMER SERVICE BOT\n",
    "{'='*70}\n",
    "\n",
    "Generated from Objectives 1-5 results.\n",
    "This report analyzes system limitations, applications, and improvements.\n",
    "\n",
    "{limitations}\n",
    "{applications}\n",
    "{scalability}\n",
    "{accuracy}\n",
    "{ux}\n",
    "{deployment}\n",
    "{improvements}\n",
    "\n",
    "{'='*70}\n",
    "                         END OF REPORT\n",
    "{'='*70}\n",
    "\"\"\"\n",
    "\n",
    "# Step 4: Display Summary\n",
    "print(\"\\nğŸ“ˆ STEP 4: Analysis Summary\")\n",
    "print(\"â”€\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "  KNOWLEDGE BASE:\n",
    "  â€¢ Total Q&A Pairs: {kb_analysis['total_pairs']}\n",
    "  â€¢ Categories: {kb_analysis['num_categories']}\n",
    "  â€¢ Avg Answer Length: {kb_analysis['avg_answer_length']:.0f} chars\n",
    "  \n",
    "  PERFORMANCE:\n",
    "  â€¢ Best Model: {perf_analysis['best_model']}\n",
    "  â€¢ Best Score: {perf_analysis['best_model_score']:.1%}\n",
    "  â€¢ Avg Response Time: {perf_analysis['avg_response_time']:.3f}s\n",
    "  â€¢ Fastest: {perf_analysis['fastest_model']} ({perf_analysis['fastest_time']:.3f}s)\n",
    "  \n",
    "  RETRIEVAL:\n",
    "  â€¢ FAISS Index: {retrieval_analysis['index_size']} vectors\n",
    "  â€¢ Dimension: {retrieval_analysis['dimension']}\n",
    "  \n",
    "  ISSUES:\n",
    "  â€¢ Failure Cases: {len(failures)}\n",
    "\"\"\")\n",
    "\n",
    "# Step 5: Save Results\n",
    "print(\"\\nğŸ’¾ STEP 5: Save Results\")\n",
    "print(\"â”€\"*70)\n",
    "\n",
    "save_analysis_report(full_report)\n",
    "save_metrics_csv(kb_analysis, perf_analysis, retrieval_analysis)\n",
    "save_recommendations(improvements)\n",
    "\n",
    "# Store globally\n",
    "globals()['system_analysis'] = {\n",
    "    'kb_analysis': kb_analysis,\n",
    "    'perf_analysis': perf_analysis,\n",
    "    'retrieval_analysis': retrieval_analysis,\n",
    "    'failures': failures\n",
    "}\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"                 âœ… OBJECTIVE 6 COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "   Analysis Sections Generated:\n",
    "   1. System Limitations\n",
    "   2. Real-World Applications\n",
    "   3. Scalability Analysis\n",
    "   4. Accuracy & Reliability\n",
    "   5. User Experience\n",
    "   6. Deployment Considerations\n",
    "   7. Improvement Opportunities\n",
    "   \n",
    "   ğŸ“¦ FILES:\n",
    "      â€¢ {OUTPUT_DIR}/system_analysis.txt\n",
    "      â€¢ {OUTPUT_DIR}/metrics_summary.csv\n",
    "      â€¢ {OUTPUT_DIR}/recommendations.txt\n",
    "   \n",
    "   ğŸ“ KEY FINDINGS:\n",
    "      â€¢ Best Model: {perf_analysis['best_model']}\n",
    "      â€¢ System ready for: Small-scale deployment\n",
    "      â€¢ Main limitation: Knowledge base size ({kb_analysis['total_pairs']} pairs)\n",
    "      â€¢ Priority improvement: Expand Q&A database\n",
    "\"\"\")\n",
    "print(\"=\"*70)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
