============================================================
OBJECTIVE 5: MODEL EVALUATION SUMMARY
============================================================

Date: 2025-11-29 14:18:35
Scoring: ScorePack (BERTScore DeBERTa-large-mnli)

REUSES FROM PREVIOUS OBJECTIVES:
  â€¢ Objective 3: embed_query() for quality metric
  â€¢ Objective 4: rag_query() for context retrieval

METRIC WEIGHTS:
  accuracy: 25%
  quality: 25%
  confidence: 20%
  speed: 15%
  robustness: 15%

============================================================
RANKINGS:
============================================================

 Rank            Model  Accuracy  Quality  Confidence  Speed  Robustness  Final_Score
    1   RoBERTa-SQuAD2     0.425    0.631       0.526  0.935        0.85        0.637
    2 BERT-Large-SQuAD     0.428    0.675       0.526  0.805        0.85        0.629
    3     MiniLM-SQuAD     0.232    0.407       0.458  0.972        0.85        0.525
    4 DistilBERT-SQuAD     0.243    0.400       0.451  0.966        0.85        0.523
    5 T5-QA-Generative     0.154    0.416       0.512  0.386        0.85        0.430
    6    DynamicRAG-8B     0.049    0.389       0.341  0.278        0.85        0.347

ðŸ¥‡ WINNER: RoBERTa-SQuAD2 (0.637)
