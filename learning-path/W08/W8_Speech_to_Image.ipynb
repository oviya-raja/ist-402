{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/oviya-raja/ist-402/blob/main/learning-path/W08/W8_Speech_to_Image.ipynb)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Speech-to-Image Generator\n",
        "\n",
        "## Overview\n",
        "This notebook implements an end-to-end multimodal pipeline that converts spoken descriptions into AI-generated images.\n",
        "\n",
        "## Architecture\n",
        "1. **Speech-to-Text**: Transcribe audio using OpenAI Whisper\n",
        "2. **Text-to-Image**: Generate images from text using Stable Diffusion v1.5\n",
        "\n",
        "## Pipeline Flow\n",
        "```\n",
        "Audio File ‚Üí Whisper (Transcription) ‚Üí Text Prompt ‚Üí \n",
        "Stable Diffusion (Generation) ‚Üí Generated Image\n",
        "```\n",
        "\n",
        "Alternative: Direct text input bypasses transcription stage.\n",
        "\n",
        "## Features\n",
        "- **Dual Input Methods**: Upload audio files OR type text directly\n",
        "- **High-Quality Transcription**: OpenAI Whisper for accurate speech recognition\n",
        "- **Creative Image Generation**: Stable Diffusion v1.5 for diverse image creation\n",
        "- **Adjustable Settings**: Control quality (inference steps) and prompt adherence (guidance scale)\n",
        "- **User-Friendly Interface**: Clear progress indicators and image download\n",
        "\n",
        "## Usage\n",
        "1. Run the cell below to install dependencies and launch the app\n",
        "2. Choose input method:\n",
        "   - **Audio Tab**: Upload audio file (WAV, MP3, M4A, FLAC) and transcribe\n",
        "   - **Text Tab**: Type your image description directly\n",
        "3. Adjust quality settings (optional)\n",
        "4. Click \"Generate Image\" to create your artwork\n",
        "\n",
        "## Technical Stack\n",
        "- **Speech Recognition**: OpenAI Whisper (tiny variant)\n",
        "- **Image Generation**: Stable Diffusion v1.5 (runwayml)\n",
        "- **UI Framework**: Streamlit\n",
        "- **Deep Learning**: PyTorch, Transformers, Diffusers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTgyjtqo7ggh",
        "outputId": "cc6bb072-36fc-4556-f5b0-55e8805b7f27"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "#  Audio-to-Image Generator ‚Äî TESTED & WORKING\n",
        "#  Run this entire cell in Google Colab\n",
        "# =====================================================\n",
        "# This cell sets up the environment, installs dependencies, and launches the app\n",
        "\n",
        "# ==================== STEP 1: Clean Environment ====================\n",
        "# Kill any existing Streamlit processes to avoid conflicts\n",
        "print(\"üßπ Cleaning up...\")\n",
        "import os\n",
        "import subprocess\n",
        "try:\n",
        "    subprocess.run([\"pkill\", \"-f\", \"streamlit\"], capture_output=True, timeout=5)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# ==================== STEP 2: Install Packages ====================\n",
        "# Install required packages for speech recognition and image generation\n",
        "# Using latest compatible versions to avoid import errors\n",
        "print(\"üì¶ Installing packages (2-3 minutes)...\")\n",
        "%pip install -q \"transformers>=4.35.0\" \"diffusers>=0.24.0\" accelerate streamlit soundfile torch torchvision pyngrok requests==2.32.4\n",
        "\n",
        "print(\"‚úÖ Packages installed!\")\n",
        "\n",
        "# ==================== STEP 4: Create Streamlit App ====================\n",
        "app_code = '''\n",
        "import streamlit as st\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import time\n",
        "\n",
        "# Config\n",
        "st.set_page_config(page_title=\"üéôÔ∏è Audio-to-Image\", layout=\"centered\")\n",
        "\n",
        "# ==================== Load Models ====================\n",
        "@st.cache_resource\n",
        "def load_models():\n",
        "    \"\"\"\n",
        "    Load both Whisper (speech-to-text) and Stable Diffusion (text-to-image) models.\n",
        "    Models are cached to avoid reloading on every interaction.\n",
        "    First run takes 3-5 minutes to download models.\n",
        "    \"\"\"\n",
        "    st.info(\"Loading AI models... (first run takes 3-5 minutes)\")\n",
        "\n",
        "    # Whisper for speech-to-text\n",
        "    # Using 'tiny' variant for speed; larger variants (base, small, medium) offer better accuracy\n",
        "    whisper = pipeline(\n",
        "        \"automatic-speech-recognition\",\n",
        "        model=\"openai/whisper-tiny\",\n",
        "        device=0 if torch.cuda.is_available() else -1  # GPU if available, else CPU\n",
        "    )\n",
        "\n",
        "    # Stable Diffusion for image generation\n",
        "    # v1.5 provides good balance of quality and speed\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    sd = StableDiffusionPipeline.from_pretrained(\n",
        "        \"runwayml/stable-diffusion-v1-5\",\n",
        "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,  # Float16 on GPU for efficiency\n",
        "        safety_checker=None  # Disabled for faster inference and flexibility\n",
        "    ).to(device)\n",
        "\n",
        "    if device == \"cuda\":\n",
        "        sd.enable_attention_slicing()  # Memory optimization for GPU\n",
        "\n",
        "    return whisper, sd\n",
        "\n",
        "whisper_model, sd_model = load_models()\n",
        "\n",
        "# ==================== UI ====================\n",
        "st.title(\"üéôÔ∏è Audio-to-Image Generator\")\n",
        "st.markdown(\"Transform your voice into stunning AI-generated images!\")\n",
        "st.markdown(\"---\")\n",
        "\n",
        "# Input methods\n",
        "tab1, tab2 = st.tabs([\"üé§ Upload Audio\", \"‚úçÔ∏è Type Text\"])\n",
        "\n",
        "prompt_text = None\n",
        "\n",
        "with tab1:\n",
        "    st.write(\"Upload an audio file with your image description\")\n",
        "    audio_file = st.file_uploader(\n",
        "        \"Choose audio file\",\n",
        "        type=[\"wav\", \"mp3\", \"m4a\", \"flac\"],\n",
        "        help=\"Speak clearly: 'A beautiful sunset over mountains'\"\n",
        "    )\n",
        "\n",
        "    if audio_file:\n",
        "        st.audio(audio_file)\n",
        "\n",
        "        if st.button(\"üéß Transcribe Audio\", type=\"primary\"):\n",
        "            with st.spinner(\"Converting speech to text...\"):\n",
        "                # Save temp file\n",
        "                with open(\"temp_audio.wav\", \"wb\") as f:\n",
        "                    f.write(audio_file.read())\n",
        "\n",
        "                # Transcribe\n",
        "                result = whisper_model(\"temp_audio.wav\")\n",
        "                prompt_text = result[\"text\"]\n",
        "\n",
        "                st.success(f\"‚úÖ Transcription: **{prompt_text}**\")\n",
        "                st.session_state.prompt = prompt_text\n",
        "\n",
        "with tab2:\n",
        "    manual_prompt = st.text_area(\n",
        "        \"Describe the image you want to generate:\",\n",
        "        placeholder=\"Example: A serene lake surrounded by autumn trees at sunset\",\n",
        "        height=100\n",
        "    )\n",
        "    if manual_prompt:\n",
        "        st.session_state.prompt = manual_prompt\n",
        "\n",
        "# Settings\n",
        "with st.expander(\"‚öôÔ∏è Advanced Settings\"):\n",
        "    col1, col2 = st.columns(2)\n",
        "    steps = col1.slider(\"Quality (inference steps)\", 10, 50, 25,\n",
        "                       help=\"More steps = better quality but slower\")\n",
        "    guidance = col2.slider(\"Prompt strength\", 5.0, 15.0, 7.5,\n",
        "                          help=\"Higher = follows prompt more closely\")\n",
        "\n",
        "# Generate button\n",
        "st.markdown(\"---\")\n",
        "if st.button(\"üé® Generate Image\", type=\"primary\", use_container_width=True):\n",
        "\n",
        "    # Get prompt from session state\n",
        "    final_prompt = st.session_state.get('prompt', None)\n",
        "\n",
        "    if not final_prompt:\n",
        "        st.error(\"‚ùå Please provide audio or text first!\")\n",
        "        st.stop()\n",
        "\n",
        "    # Generate image\n",
        "    st.info(f\"üé® Generating image from: **{final_prompt}**\")\n",
        "    st.write(\"This may take 30 seconds to 3 minutes depending on your GPU...\")\n",
        "\n",
        "    progress_bar = st.progress(0)\n",
        "    start_time = time.time()\n",
        "\n",
        "    with st.spinner(\"Creating your masterpiece...\"):\n",
        "        try:\n",
        "            # Generate\n",
        "            image = sd_model(\n",
        "                prompt=final_prompt,\n",
        "                num_inference_steps=steps,\n",
        "                guidance_scale=guidance,\n",
        "                height=512,\n",
        "                width=512\n",
        "            ).images[0]\n",
        "\n",
        "            elapsed = time.time() - start_time\n",
        "            progress_bar.progress(100)\n",
        "\n",
        "            # Display\n",
        "            st.success(f\"‚úÖ Generated in {elapsed:.1f} seconds!\")\n",
        "            st.image(image, caption=final_prompt)\n",
        "\n",
        "            # Save and download\n",
        "            image.save(\"generated_image.png\")\n",
        "            with open(\"generated_image.png\", \"rb\") as f:\n",
        "                st.download_button(\n",
        "                    \"üíæ Download Image\",\n",
        "                    data=f,\n",
        "                    file_name=f\"ai_art_{int(time.time())}.png\",\n",
        "                    mime=\"image/png\",\n",
        "                    use_container_width=True\n",
        "                )\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"‚ùå Generation failed: {str(e)}\")\n",
        "            st.info(\"Try simplifying your prompt or reducing quality settings\")\n",
        "\n",
        "# Footer\n",
        "st.markdown(\"---\")\n",
        "st.caption(\"üîä Powered by OpenAI Whisper + Stable Diffusion v1.5\")\n",
        "\n",
        "# GPU info\n",
        "device_info = \"üöÄ GPU Accelerated\" if torch.cuda.is_available() else \"üê¢ CPU Mode (slower)\"\n",
        "st.caption(device_info)\n",
        "'''\n",
        "\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(app_code)\n",
        "\n",
        "print(\"‚úÖ App created!\")\n",
        "\n",
        "# ==================== STEP 3: Setup ngrok ====================\n",
        "import time\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# ‚ö†Ô∏è IMPORTANT: Set your ngrok token here\n",
        "# Get it from: https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "# Replace with your own token for public access\n",
        "NGROK_TOKEN = \"3443vHI71ODZeUY6WQUeBW45KG7_HL7SDdKFz6uty9yqd8Cg\"  # ‚ö†Ô∏è CHANGE THIS!\n",
        "\n",
        "if NGROK_TOKEN == \"YOUR_TOKEN_HERE\":\n",
        "    print(\"\\n‚ùå ERROR: Please set your ngrok token!\")\n",
        "    print(\"   1. Go to: https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
        "    print(\"   2. Copy your token\")\n",
        "    print(\"   3. Replace 'YOUR_TOKEN_HERE' in the code above\")\n",
        "    raise SystemExit\n",
        "\n",
        "try:\n",
        "    ngrok.set_auth_token(NGROK_TOKEN)\n",
        "    print(\"‚úÖ ngrok token configured\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Warning: Could not set ngrok token: {e}\")\n",
        "    print(\"   Continuing without ngrok (local access only)\")\n",
        "\n",
        "# Kill existing tunnels\n",
        "try:\n",
        "    for tunnel in ngrok.get_tunnels():\n",
        "        ngrok.disconnect(tunnel.public_url)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# ==================== STEP 4: Write app.py ====================\n",
        "try:\n",
        "    with open(\"app.py\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(app_code)\n",
        "    print(\"‚úÖ app.py generated successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to write app.py: {e}\")\n",
        "    raise\n",
        "\n",
        "# ==================== STEP 5: Start Streamlit and ngrok ====================\n",
        "import sys\n",
        "\n",
        "# Kill any existing streamlit on port 8501\n",
        "try:\n",
        "    if os.name == 'nt':  # Windows\n",
        "        os.system('netstat -ano | findstr :8501')\n",
        "    else:  # macOS/Linux\n",
        "        os.system('lsof -ti:8501 | xargs kill -9 2>/dev/null || true')\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Start Streamlit\n",
        "print(\"\\nüöÄ Starting Streamlit...\")\n",
        "try:\n",
        "    if sys.platform.startswith('win'):\n",
        "        subprocess.Popen(\n",
        "            [sys.executable, \"-m\", \"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\", \"--server.headless\", \"true\"],\n",
        "            creationflags=subprocess.CREATE_NEW_CONSOLE\n",
        "        )\n",
        "    else:\n",
        "        subprocess.Popen(\n",
        "            [\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\", \"--server.headless\", \"true\"],\n",
        "            stdout=subprocess.DEVNULL,\n",
        "            stderr=subprocess.DEVNULL,\n",
        "            start_new_session=True\n",
        "        )\n",
        "    \n",
        "    time.sleep(5)  # Give Streamlit time to start\n",
        "    print(\"‚úÖ Streamlit started!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error starting Streamlit: {e}\")\n",
        "    print(\"   You can start it manually with: streamlit run app.py\")\n",
        "\n",
        "# Create ngrok tunnel\n",
        "print(\"\\nüåê Creating public URL with ngrok...\")\n",
        "try:\n",
        "    public_url = ngrok.connect(8501)\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"‚úÖ SUCCESS! Your app is running!\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\nüåê Public URL (share this):\")\n",
        "    print(f\"   {public_url}\")\n",
        "    print(f\"\\nüè† Local URL:\")\n",
        "    print(f\"   http://localhost:8501\")\n",
        "    print(f\"\\nüìå Tips:\")\n",
        "    print(f\"   ‚Ä¢ Keep this notebook running\")\n",
        "    print(f\"   ‚Ä¢ First image generation takes longer (loading models)\")\n",
        "    print(f\"   ‚Ä¢ Use short, clear voice prompts\")\n",
        "    print(f\"   ‚Ä¢ CPU mode works but is slower than GPU\")\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ö†Ô∏è Could not create ngrok tunnel: {e}\")\n",
        "    print(\"\\nüìå App is running locally at: http://localhost:8501\")\n",
        "    print(\"   (ngrok tunnel failed, but local access works)\")\n",
        "    print(\"\\nüîß Troubleshooting:\")\n",
        "    print(\"   1. Check your ngrok token is correct\")\n",
        "    print(\"   2. Make sure you replaced 'YOUR_TOKEN_HERE'\")\n",
        "    print(\"   3. Try restarting the kernel and running again\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example Usage\n",
        "\n",
        "### Method 1: Audio Input\n",
        "1. Record or upload an audio file describing your desired image\n",
        "2. Supported formats: WAV, MP3, M4A, FLAC\n",
        "3. Click \"Transcribe Audio\" to convert speech to text\n",
        "4. Review the transcription\n",
        "5. Click \"Generate Image\" to create the image\n",
        "\n",
        "**Example Audio Prompts:**\n",
        "- \"A serene lake surrounded by autumn trees at sunset\"\n",
        "- \"A futuristic cityscape at night with neon lights\"\n",
        "- \"A cozy coffee shop with warm lighting\"\n",
        "\n",
        "### Method 2: Direct Text Input\n",
        "1. Type your image description directly\n",
        "2. Be descriptive for better results\n",
        "3. Click \"Generate Image\"\n",
        "\n",
        "**Example Text Prompts:**\n",
        "- \"A beautiful sunset over mountains with trees in the foreground\"\n",
        "- \"A modern minimalist living room with large windows\"\n",
        "- \"A vintage typewriter on a wooden desk with books\"\n",
        "\n",
        "### Tips for Best Results\n",
        "- **Be Descriptive**: Include details about colors, mood, style, composition\n",
        "- **Quality Settings**: \n",
        "  - More inference steps = higher quality but slower (25-50 recommended)\n",
        "  - Higher guidance scale = follows prompt more closely (7.5-10 recommended)\n",
        "- **Generation Time**: First generation takes longer (model loading), subsequent ones are faster\n",
        "- **GPU vs CPU**: GPU is 5-10x faster; CPU works but is slower"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRm2n-hrAqIf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
