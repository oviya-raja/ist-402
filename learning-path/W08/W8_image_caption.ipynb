{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/oviya-raja/ist-402/blob/main/learning-path/W08/W8_image_caption.ipynb)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Image Caption Generator (BLIP)\n",
        "\n",
        "## Overview\n",
        "This notebook implements an image captioning system using the BLIP (Bootstrapping Language-Image Pre-training) model to generate accurate, descriptive captions for uploaded images.\n",
        "\n",
        "## Architecture\n",
        "BLIP uses a vision-language transformer architecture:\n",
        "- **Vision Encoder**: Processes image into visual features\n",
        "- **Multimodal Fusion**: Combines visual and textual representations  \n",
        "- **Language Decoder**: Generates caption text autoregressively\n",
        "\n",
        "## Features\n",
        "- **Automatic Captioning**: Generate descriptive captions for any image\n",
        "- **Multiple Format Support**: JPG, JPEG, PNG\n",
        "- **Fast Processing**: ~2-5 seconds per image\n",
        "- **User-Friendly Interface**: Simple upload and view workflow\n",
        "- **Robust Handling**: Works with various image sizes and aspect ratios\n",
        "\n",
        "## Usage\n",
        "1. Run the cell below to install dependencies and launch the app\n",
        "2. Upload an image using the file uploader\n",
        "3. View the automatically generated caption\n",
        "4. Upload additional images to test with different types\n",
        "\n",
        "## Technical Stack\n",
        "- **Model**: Salesforce BLIP (blip-image-captioning-base)\n",
        "- **Image Processing**: PIL (Pillow)\n",
        "- **UI Framework**: Streamlit\n",
        "- **Deep Learning**: Transformers (Hugging Face)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GE5luHiHw0Ir",
        "outputId": "2fe0800d-eba6-4790-d11c-fffbc22820b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "‚úÖ app.py generated successfully\n",
            "üöÄ Starting Streamlit...\n",
            "   This will open in your default browser\n",
            "   If it doesn't open automatically, go to: http://localhost:8501\n",
            "\n",
            "‚úÖ Streamlit should be running!\n",
            "   üåê Open: http://localhost:8501\n",
            "\n",
            "   üí° To stop Streamlit, run: pkill -f streamlit\n",
            "\n",
            "============================================================\n",
            "‚úÖ Setup complete! Streamlit is running locally.\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# =====================================================\n",
        "#  BLIP Image Caption Generator ‚Äî Local Version (FIXED)\n",
        "# =====================================================\n",
        "# This cell installs dependencies and launches the Streamlit app\n",
        "# type: ignore\n",
        "\n",
        "# Install dependencies\n",
        "# - streamlit: Web interface framework\n",
        "# - transformers: Hugging Face library for BLIP model\n",
        "# - pillow: Image processing\n",
        "# - pyngrok: For public URL tunneling\n",
        "%pip install -q streamlit transformers pillow torch torchvision pyngrok\n",
        "\n",
        "# Save Streamlit app as a Python script\n",
        "# The app includes:\n",
        "# - Image upload and processing\n",
        "# - BLIP model loading (cached for efficiency)\n",
        "# - Caption generation and display\n",
        "app_code: str = \"\"\"\n",
        "import streamlit as st\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from PIL import Image\n",
        "\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    '''\n",
        "    Load BLIP model and processor.\n",
        "    Cached to avoid reloading on every interaction.\n",
        "    First run downloads the model (~1GB).\n",
        "    '''\n",
        "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "    return processor, model\n",
        "\n",
        "processor, model = load_model()\n",
        "\n",
        "st.title(\"üñºÔ∏è Image to Caption Generator (BLIP Model)\")\n",
        "st.markdown(\"Upload an image to generate an automatic caption using AI.\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Upload an Image\", type=[\"jpg\", \"jpeg\", \"png\"], \n",
        "                                 help=\"Supported formats: JPG, JPEG, PNG\")\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    # Load and display image\n",
        "    image = Image.open(uploaded_file).convert(\"RGB\")  # Convert to RGB for compatibility\n",
        "    st.image(image, caption=\"Uploaded Image\")\n",
        "    \n",
        "    # Generate caption\n",
        "    with st.spinner(\"Generating caption...\"):\n",
        "        inputs = processor(image, return_tensors=\"pt\")  # Preprocess image\n",
        "        out = model.generate(**inputs)  # Generate caption tokens\n",
        "        caption = processor.decode(out[0], skip_special_tokens=True)  # Decode to text\n",
        "\n",
        "    st.subheader(\"üìù Generated Caption:\")\n",
        "    st.success(caption)\n",
        "    \n",
        "    # Additional info\n",
        "    st.caption(\"üí° Tip: Try different types of images (nature, objects, people, scenes) to see the model's capabilities!\")\n",
        "\"\"\"\n",
        "\n",
        "# Write app.py with error handling\n",
        "try:\n",
        "    with open(\"app.py\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(app_code)\n",
        "    print(\"‚úÖ app.py generated successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to write app.py: {e}\")\n",
        "    raise\n",
        "\n",
        "# Setup ngrok for public URL\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# ‚ö†Ô∏è IMPORTANT: Set your ngrok token here\n",
        "# Get it from: https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "# Replace with your own token for public access\n",
        "NGROK_TOKEN = \"3443vHI71ODZeUY6WQUeBW45KG7_HL7SDdKFz6uty9yqd8Cg\"  # ‚ö†Ô∏è CHANGE THIS!\n",
        "\n",
        "if NGROK_TOKEN == \"YOUR_TOKEN_HERE\":\n",
        "    print(\"\\n‚ùå ERROR: Please set your ngrok token!\")\n",
        "    print(\"   1. Go to: https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
        "    print(\"   2. Copy your token\")\n",
        "    print(\"   3. Replace 'YOUR_TOKEN_HERE' in the code above\")\n",
        "    raise SystemExit\n",
        "\n",
        "try:\n",
        "    ngrok.set_auth_token(NGROK_TOKEN)\n",
        "    print(\"‚úÖ ngrok token configured\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Warning: Could not set ngrok token: {e}\")\n",
        "    print(\"   Continuing without ngrok (local access only)\")\n",
        "\n",
        "# Kill existing tunnels\n",
        "try:\n",
        "    for tunnel in ngrok.get_tunnels():\n",
        "        ngrok.disconnect(tunnel.public_url)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Start Streamlit locally\n",
        "import subprocess\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Kill any existing streamlit on port 8501\n",
        "try:\n",
        "    if os.name == 'nt':  # Windows\n",
        "        os.system('netstat -ano | findstr :8501')\n",
        "    else:  # macOS/Linux\n",
        "        os.system('lsof -ti:8501 | xargs kill -9 2>/dev/null || true')\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Start Streamlit\n",
        "print(\"\\nüöÄ Starting Streamlit...\")\n",
        "try:\n",
        "    if sys.platform.startswith('win'):\n",
        "        subprocess.Popen(\n",
        "            [sys.executable, \"-m\", \"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\", \"--server.headless\", \"true\"],\n",
        "            creationflags=subprocess.CREATE_NEW_CONSOLE\n",
        "        )\n",
        "    else:\n",
        "        subprocess.Popen(\n",
        "            [\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\", \"--server.headless\", \"true\"],\n",
        "            stdout=subprocess.DEVNULL,\n",
        "            stderr=subprocess.DEVNULL,\n",
        "            start_new_session=True\n",
        "        )\n",
        "    \n",
        "    time.sleep(5)  # Give Streamlit time to start\n",
        "    print(\"‚úÖ Streamlit started!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error starting Streamlit: {e}\")\n",
        "    print(\"   You can start it manually with: streamlit run app.py\")\n",
        "\n",
        "# Create ngrok tunnel\n",
        "print(\"\\nüåê Creating public URL with ngrok...\")\n",
        "try:\n",
        "    public_url = ngrok.connect(8501)\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"‚úÖ SUCCESS! Your app is running!\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\nüåê Public URL (share this):\")\n",
        "    print(f\"   {public_url}\")\n",
        "    print(f\"\\nüè† Local URL:\")\n",
        "    print(f\"   http://localhost:8501\")\n",
        "    print(f\"\\nüìå Tips:\")\n",
        "    print(f\"   ‚Ä¢ Keep this notebook running\")\n",
        "    print(f\"   ‚Ä¢ Upload images to test the caption generator\")\n",
        "    print(f\"   ‚Ä¢ Try different image types (nature, objects, people, scenes)\")\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ö†Ô∏è Could not create ngrok tunnel: {e}\")\n",
        "    print(\"\\nüìå App is running locally at: http://localhost:8501\")\n",
        "    print(\"   (ngrok tunnel failed, but local access works)\")\n",
        "    print(\"\\nüîß Troubleshooting:\")\n",
        "    print(\"   1. Check your ngrok token is correct\")\n",
        "    print(\"   2. Make sure you replaced 'YOUR_TOKEN_HERE'\")\n",
        "    print(\"   3. Try restarting the kernel and running again\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example Usage\n",
        "\n",
        "### Step 1: Upload Image\n",
        "Click \"Upload an Image\" and select a JPG, JPEG, or PNG file.\n",
        "\n",
        "### Step 2: View Caption\n",
        "The system automatically:\n",
        "1. Processes the image\n",
        "2. Generates a descriptive caption\n",
        "3. Displays the result\n",
        "\n",
        "### Testing with Different Image Types\n",
        "\n",
        "**Nature/Landscape Images:**\n",
        "- Mountain scenes, beaches, forests\n",
        "- Expected: Captions describing scenery, natural elements\n",
        "\n",
        "**People/Portrait Images:**\n",
        "- Portraits, group photos, candid shots\n",
        "- Expected: Captions identifying people and activities\n",
        "\n",
        "**Object Images:**\n",
        "- Products, everyday items, food\n",
        "- Expected: Precise object identification\n",
        "\n",
        "**Scene/Activity Images:**\n",
        "- Street scenes, indoor settings, activities\n",
        "- Expected: Captions describing setting and context\n",
        "\n",
        "### Understanding the Output\n",
        "- **Accuracy**: BLIP excels at identifying common objects and scenes\n",
        "- **Detail Level**: Captions are generally accurate but may be somewhat generic\n",
        "- **Limitations**: May miss fine details or abstract/artistic elements\n",
        "\n",
        "### Testing Limitations\n",
        "\n",
        "To demonstrate BLIP's limitations (missing fine details or abstract elements), try these images from `data/`:\n",
        "\n",
        "**Best images to show limitations:**\n",
        "1. **`scene_street.jpg`** - Complex scenes with many details\n",
        "   - Look for: Missing specific objects, people, or activities in the background\n",
        "   - BLIP may give a generic \"street scene\" description rather than detailed elements\n",
        "\n",
        "2. **`scene_activity.jpg`** - Activity scenes with multiple elements\n",
        "   - Look for: Generic activity description vs. specific actions or interactions\n",
        "   - May miss subtle interactions or specific activities happening\n",
        "\n",
        "3. **`people_group.jpg`** - Group photos with multiple people\n",
        "   - Look for: Generic \"group of people\" vs. specific number, poses, or interactions\n",
        "   - May miss individual details or relationships between people\n",
        "\n",
        "4. **`object_everyday.jpg`** - Everyday objects with fine details\n",
        "   - Look for: Generic object identification vs. specific brand, model, or detailed features\n",
        "   - May miss subtle design elements or specific characteristics\n",
        "\n",
        "**How to test:**\n",
        "1. Upload one of these images in the Streamlit app\n",
        "2. Compare the generated caption with what you actually see\n",
        "3. Note what specific details, objects, or elements are missing\n",
        "4. The caption will likely be accurate but generic, missing fine-grained details\n",
        "\n",
        "### Tips\n",
        "- Use clear, well-lit images for best results\n",
        "- The model works with various image sizes (auto-resized internally)\n",
        "- Processing time: ~2-5 seconds per image\n",
        "- For limitation testing: Choose complex scenes or images with many fine details"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
