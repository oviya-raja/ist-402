{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/oviya-raja/ist-402/blob/main/learning-path/W08/W8_pdf_Q_A.ipynb)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PDF Q&A RAG System \n",
        "\n",
        "## Overview\n",
        "This notebook implements a **Retrieval-Augmented Generation (RAG)** system for answering questions from PDF documents.\n",
        "\n",
        "## Architecture\n",
        "1. **Document Processing**: Extract text from uploaded PDFs\n",
        "2. **Text Chunking**: Split documents into manageable chunks (1000 chars, 200 overlap)\n",
        "3. **Embedding**: Convert chunks to vectors using MiniLM-L6-v2\n",
        "4. **Vector Store**: Build FAISS index for fast similarity search\n",
        "5. **Question Answering**: Retrieve relevant chunks and generate answers using FLAN-T5\n",
        "\n",
        "## Key Fixes in This Version\n",
        "- Improved prompt template for better FLAN-T5 comprehension\n",
        "- Better context preprocessing to remove noise\n",
        "- Fixed generation parameters (removed problematic min_length)\n",
        "- Increased chunk size for better context\n",
        "- Added context cleaning to remove figure/table noise\n",
        "\n",
        "## Usage\n",
        "1. Run the cell below to install dependencies and launch the app\n",
        "2. Upload PDF files in the Streamlit interface\n",
        "3. Click \"Build / Rebuild Index\" to process documents\n",
        "4. Ask questions and get answers grounded in your documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "‚úÖ app.py generated successfully\n",
            "üßπ Killing ALL existing ngrok processes...\n",
            "‚úÖ ngrok processes killed\n",
            "‚úÖ Loaded ngrok token from .env file\n",
            "‚úÖ ngrok token configured successfully\n",
            "üîå Disconnecting any remaining tunnels...\n",
            "‚úÖ All tunnels disconnected\n",
            "\n",
            "üöÄ Starting Streamlit...\n",
            "‚úÖ Streamlit started!\n",
            "\n",
            "üåê Creating public URL with ngrok...\n",
            "\n",
            "============================================================\n",
            "‚úÖ SUCCESS! Your app is running!\n",
            "============================================================\n",
            "\n",
            "üåê Public URL (share this):\n",
            "   NgrokTunnel: \"https://unrivalable-lenna-soothfastly.ngrok-free.dev\" -> \"http://localhost:8501\"\n",
            "\n",
            "üè† Local URL:\n",
            "   http://localhost:8501\n",
            "\n",
            "üìå Tips:\n",
            "   ‚Ä¢ Keep this notebook running\n",
            "   ‚Ä¢ Upload PDFs and build the index\n",
            "   ‚Ä¢ Ask questions to get answers from your documents\n",
            "\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "t=2025-12-14T22:26:40-0500 lvl=warn msg=\"Stopping forwarder\" name=http-8501-ee0e24ec-3c3a-4b1c-a537-acb163ecb549 acceptErr=\"failed to accept connection: Listener closed\"\n",
            "t=2025-12-14T22:26:40-0500 lvl=warn msg=\"Error restarting forwarder\" name=http-8501-ee0e24ec-3c3a-4b1c-a537-acb163ecb549 err=\"failed to start tunnel: session closed\"\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# üìö PDF Q&A RAG ‚Äî Launcher (FIXED VERSION)\n",
        "# =========================\n",
        "# This cell installs dependencies and launches the Streamlit app\n",
        "# type: ignore\n",
        "\n",
        "# 1) Install dependencies\n",
        "# Note: requests==2.32.4 required for Google Colab compatibility\n",
        "%pip install -q streamlit langchain-community faiss-cpu sentence-transformers \\\n",
        "                transformers accelerate safetensors pypdf pyngrok python-dotenv requests==2.32.4\n",
        "\n",
        "# 2) Create the Streamlit application\n",
        "app_code: str = '''\n",
        "import os\n",
        "import io\n",
        "import re\n",
        "import torch\n",
        "import streamlit as st\n",
        "\n",
        "# ---- LangChain & friends ----\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# ---- PDF parsing ----\n",
        "from pypdf import PdfReader\n",
        "\n",
        "# ---- Local LLM (FLAN-T5) ----\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "st.set_page_config(page_title=\"PDF Q&A (Local RAG)\", page_icon=\"üìö\")\n",
        "st.title(\"üìö PDF Q&A Chatbot ‚Äî Local RAG (Fixed)\")\n",
        "\n",
        "st.markdown(\n",
        "    \"Upload one or more PDFs. We\\'ll chunk + embed them (MiniLM), build a FAISS index, \"\n",
        "    \"then answer questions using retrieved chunks and a local FLAN-T5-Large model (better quality than base).\"\n",
        ")\n",
        "\n",
        "# ---------------------------\n",
        "# CACHED MODELS\n",
        "# ---------------------------\n",
        "\n",
        "@st.cache_resource\n",
        "def load_embeddings():\n",
        "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "    return HuggingFaceEmbeddings(model_name=model_name)\n",
        "\n",
        "@st.cache_resource\n",
        "def load_flan():\n",
        "    # Using flan-t5-large for better answer quality (780M params vs 250M in base)\n",
        "    # Falls back to base if large fails to load (memory constraints)\n",
        "    name = \"google/flan-t5-large\"\n",
        "    try:\n",
        "        tok = AutoTokenizer.from_pretrained(name)\n",
        "        dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(name, torch_dtype=dtype)\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        model.to(device)\n",
        "        print(f\"‚úÖ Loaded {name} successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Failed to load {name}, falling back to flan-t5-base: {e}\")\n",
        "        name = \"google/flan-t5-base\"\n",
        "        tok = AutoTokenizer.from_pretrained(name)\n",
        "        dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(name, torch_dtype=dtype)\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        model.to(device)\n",
        "    return tok, model, device\n",
        "\n",
        "embeddings = load_embeddings()\n",
        "tokenizer, flan, device = load_flan()\n",
        "\n",
        "# ---------------------------\n",
        "# HELPER FUNCTIONS\n",
        "# ---------------------------\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Clean extracted text to remove noise that confuses the model.\n",
        "    \"\"\"\n",
        "    # Remove sequences of numbers (like figure axis labels: \"0 200 400 600...\")\n",
        "    # Using [0-9] instead of \\\\d to avoid escape sequence warnings\n",
        "    text = re.sub(r\"([0-9]+\\\\s+){4,}\", \"\", text)\n",
        "    \n",
        "    # Remove figure/table references that are just numbers\n",
        "    text = re.sub(r\"Figure\\\\s*[0-9]+[.:]\", \"Figure: \", text)\n",
        "    text = re.sub(r\"Table\\\\s*[0-9]+[.:]\", \"Table: \", text)\n",
        "    \n",
        "    # Remove excessive whitespace\n",
        "    text = re.sub(r\"\\\\s+\", \" \", text)\n",
        "    \n",
        "    # Remove lines that are mostly numbers/symbols\n",
        "    # Use splitlines() to avoid escape sequence issues\n",
        "    lines = text.splitlines()\n",
        "    cleaned_lines = []\n",
        "    for line in lines:\n",
        "        # Keep line if it has enough alphabetic content\n",
        "        alpha_ratio = sum(c.isalpha() for c in line) / max(len(line), 1)\n",
        "        if alpha_ratio > 0.3 or len(line) < 10:\n",
        "            cleaned_lines.append(line)\n",
        "    \n",
        "    # Join with newline character\n",
        "    # Join with newline - properly escaped\n",
        "    return chr(10).join(cleaned_lines).strip()\n",
        "\n",
        "def read_pdfs(files):\n",
        "    texts = []\n",
        "    for f in files:\n",
        "        data = f.read()\n",
        "        reader = PdfReader(io.BytesIO(data))\n",
        "        content = []\n",
        "        for page in reader.pages:\n",
        "            try:\n",
        "                page_text = page.extract_text() or \"\"\n",
        "                # Clean each page\n",
        "                page_text = clean_text(page_text)\n",
        "                content.append(page_text)\n",
        "            except Exception:\n",
        "                content.append(\"\")\n",
        "        full_text = chr(10).join(content).strip()\n",
        "        if full_text:\n",
        "            texts.append(full_text)\n",
        "    return texts\n",
        "\n",
        "def build_vectorstore(raw_texts):\n",
        "    # Increased chunk size for better context\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,     # Larger chunks = more context\n",
        "        chunk_overlap=200,   # More overlap to preserve continuity\n",
        "        length_function=len,\n",
        "    )\n",
        "    docs = []\n",
        "    for t in raw_texts:\n",
        "        docs.extend(splitter.create_documents([t]))\n",
        "    vs = FAISS.from_documents(docs, embeddings)\n",
        "    return vs\n",
        "\n",
        "def make_prompt(question, contexts):\n",
        "    \"\"\"\n",
        "    Create a clear, structured prompt for FLAN-T5.\n",
        "    Key improvements:\n",
        "    - Cleaner instruction format\n",
        "    - Context limited to avoid truncation issues\n",
        "    - Explicit instruction to answer from context\n",
        "    \"\"\"\n",
        "    # Limit total context length to avoid truncation\n",
        "    max_context_chars = 1500\n",
        "    combined_context = \"\"\n",
        "    for ctx in contexts:\n",
        "        if len(combined_context) + len(ctx) < max_context_chars:\n",
        "            combined_context += ctx + chr(10) + chr(10)\n",
        "        else:\n",
        "            # Add partial context if space allows\n",
        "            remaining = max_context_chars - len(combined_context)\n",
        "            if remaining > 100:\n",
        "                combined_context += ctx[:remaining] + \"...\"\n",
        "            break\n",
        "    \n",
        "    combined_context = combined_context.strip()\n",
        "    \n",
        "    # FLAN-T5 works better with explicit instruction-following format\n",
        "    prompt = f\"\"\"Based on the following context, answer the question. If the answer is not in the context, say \"I don't know\".\n",
        "\n",
        "Context:\n",
        "{combined_context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "def generate_answer(prompt, max_new_tokens=256):\n",
        "    \"\"\"\n",
        "    Generate answer using FLAN-T5 with fixed parameters.\n",
        "    Key fixes:\n",
        "    - Removed min_length (was causing garbage output)\n",
        "    - Better temperature settings\n",
        "    - Proper handling of edge cases\n",
        "    \"\"\"\n",
        "    # Tokenize with proper truncation\n",
        "    inputs = tokenizer(\n",
        "        prompt, \n",
        "        return_tensors=\"pt\", \n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=False\n",
        "    ).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        output = flan.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            min_length=5,            # Ensure minimum answer length (5 tokens)\n",
        "            temperature=0.5,          # Lower temperature for more focused answers\n",
        "            do_sample=True,           # Enable sampling for better quality\n",
        "            top_p=0.95,               # Nucleus sampling (slightly higher)\n",
        "            top_k=50,                 # Limit vocabulary\n",
        "            repetition_penalty=1.1,   # Reduce repetition (slightly lower)\n",
        "            no_repeat_ngram_size=2,   # Prevent 2-gram repetition\n",
        "            early_stopping=False,     # Don't stop early - let it generate fully\n",
        "            num_beams=3,              # Use beam search for better quality\n",
        "        )\n",
        "    \n",
        "    answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    answer = answer.strip()\n",
        "    \n",
        "    # Remove common prefixes that FLAN-T5 might add\n",
        "    prefixes_to_remove = [\"answer:\", \"answer is:\", \"the answer is:\", \"answer:\", \"a:\"]\n",
        "    for prefix in prefixes_to_remove:\n",
        "        if answer.lower().startswith(prefix):\n",
        "            answer = answer[len(prefix):].strip()\n",
        "            break\n",
        "    \n",
        "    # Post-process: only reject clearly invalid answers\n",
        "    if not answer:\n",
        "        return \"I couldn\\'t generate an answer from the provided context.\"\n",
        "    \n",
        "    # Only reject if answer is suspiciously short AND mostly non-alphabetic\n",
        "    if len(answer) < 5:\n",
        "        # Very short answers might still be valid (like \"Yes\", \"No\", \"3\")\n",
        "        # Only reject if it's completely empty or just whitespace\n",
        "        if not answer.strip():\n",
        "            return \"I couldn\\'t generate an answer from the provided context.\"\n",
        "    \n",
        "    # Check for suspiciously numeric-only answers (but allow short numeric answers)\n",
        "    if len(answer) > 15:  # Only check longer answers\n",
        "        alpha_ratio = sum(c.isalpha() for c in answer) / max(len(answer), 1)\n",
        "        if alpha_ratio < 0.2:  # More lenient threshold\n",
        "            # Answer is mostly numbers/symbols - likely garbage\n",
        "            return \"I couldn\\'t find a clear answer in the provided context. Please try rephrasing your question or check if the document contains relevant information.\"\n",
        "    \n",
        "    return answer\n",
        "\n",
        "# ---------------------------\n",
        "# UI\n",
        "# ---------------------------\n",
        "st.subheader(\"üì§ Upload PDFs\")\n",
        "uploaded = st.file_uploader(\"Upload one or more PDFs\", type=[\"pdf\"], accept_multiple_files=True)\n",
        "\n",
        "if \"vectorstore\" not in st.session_state:\n",
        "    st.session_state.vectorstore = None\n",
        "\n",
        "col_a, col_b = st.columns([1,1])\n",
        "with col_a:\n",
        "    build_btn = st.button(\"üîß Build / Rebuild Index\")\n",
        "with col_b:\n",
        "    clear_btn = st.button(\"üóëÔ∏è Clear Index\")\n",
        "\n",
        "if clear_btn:\n",
        "    st.session_state.vectorstore = None\n",
        "    st.success(\"Cleared vector index.\")\n",
        "\n",
        "if build_btn:\n",
        "    if not uploaded:\n",
        "        st.warning(\"Please upload at least one PDF.\")\n",
        "    else:\n",
        "        with st.spinner(\"Reading PDFs and building FAISS index...\"):\n",
        "            texts = read_pdfs(uploaded)\n",
        "            if not any(texts):\n",
        "                st.error(\"No extractable text found in the PDFs.\")\n",
        "            else:\n",
        "                st.session_state.vectorstore = build_vectorstore(texts)\n",
        "                st.success(f\"Index ready! Processed {len(texts)} document(s). Ask questions below.\")\n",
        "\n",
        "st.divider()\n",
        "st.subheader(\"‚ùì Ask a Question\")\n",
        "\n",
        "q = st.text_input(\"Your question\", placeholder=\"Enter your question here...\")\n",
        "\n",
        "k = st.slider(\"Top-k chunks\", 2, 8, 4)\n",
        "max_tokens = st.slider(\"Max new tokens (answer length)\", 64, 512, 256, step=32)\n",
        "\n",
        "# Always show the button, but disable it if no index or no question\n",
        "button_disabled = st.session_state.vectorstore is None or not q.strip()\n",
        "\n",
        "if st.session_state.vectorstore is None:\n",
        "    st.warning(\"‚ö†Ô∏è Please upload PDFs and click **Build / Rebuild Index** first!\")\n",
        "\n",
        "# Show the button always\n",
        "if st.button(\"üîç Retrieve & Answer\", disabled=button_disabled, type=\"primary\"):\n",
        "    if st.session_state.vectorstore is None:\n",
        "        st.error(\"‚ùå No index found! Please upload PDFs and build the index first.\")\n",
        "    elif not q.strip():\n",
        "        st.error(\"‚ùå Please enter a question first.\")\n",
        "    else:\n",
        "        with st.spinner(\"Retrieving relevant chunks...\"):\n",
        "            docs = st.session_state.vectorstore.similarity_search(q, k=k)\n",
        "            contexts = [d.page_content for d in docs]\n",
        "        \n",
        "        st.write(\"**Retrieved Chunks:**\")\n",
        "        for i, c in enumerate(contexts, 1):\n",
        "            with st.expander(f\"Chunk {i}\"):\n",
        "                st.write(c)\n",
        "\n",
        "        with st.spinner(\"Generating answer with FLAN-T5...\"):\n",
        "            prompt = make_prompt(q, contexts)\n",
        "            ans = generate_answer(prompt, max_new_tokens=max_tokens)\n",
        "        \n",
        "        st.success(\"**Answer:**\")\n",
        "        st.write(ans)\n",
        "        \n",
        "        # Debug info (collapsible)\n",
        "        with st.expander(\"üîß Debug: View prompt sent to model\"):\n",
        "            st.code(prompt, language=\"text\")\n",
        "'''\n",
        "\n",
        "# Write app.py with error handling\n",
        "try:\n",
        "    with open(\"app.py\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(app_code)\n",
        "    print(\"‚úÖ app.py generated successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to write app.py: {e}\")\n",
        "    raise\n",
        "\n",
        "# 3) Setup ngrok for public URL\n",
        "from pyngrok import ngrok\n",
        "import os\n",
        "import time\n",
        "\n",
        "# ==============================================\n",
        "# AGGRESSIVE NGROK CLEANUP (MUST RUN FIRST)\n",
        "# ==============================================\n",
        "print(\"üßπ Killing ALL existing ngrok processes...\")\n",
        "try:\n",
        "    # Kill ngrok at OS level first (most aggressive)\n",
        "    os.system('pkill -9 ngrok 2>/dev/null || true')\n",
        "    os.system('killall ngrok 2>/dev/null || true')\n",
        "    time.sleep(1)  # Wait for processes to die\n",
        "    \n",
        "    # Then use pyngrok's kill\n",
        "    ngrok.kill()\n",
        "    time.sleep(1)  # Wait again\n",
        "    \n",
        "    print(\"‚úÖ ngrok processes killed\")\n",
        "except Exception as e:\n",
        "    print(f\"   Note: {e}\")\n",
        "\n",
        "# Load ngrok token from environment variables\n",
        "# Supports both Google Colab (userdata) and local (.env file)\n",
        "NGROK_TOKEN = None\n",
        "\n",
        "# Try Google Colab first\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    NGROK_TOKEN = userdata.get('NGROK_AUTHTOKEN')\n",
        "    if NGROK_TOKEN:\n",
        "        print(\"‚úÖ Loaded ngrok token from Google Colab userdata\")\n",
        "except ImportError:\n",
        "    # Not in Colab, try local .env file\n",
        "    try:\n",
        "        from dotenv import load_dotenv\n",
        "        load_dotenv()  # Load .env file if it exists\n",
        "        NGROK_TOKEN = os.getenv('NGROK_AUTHTOKEN')\n",
        "        if NGROK_TOKEN:\n",
        "            print(\"‚úÖ Loaded ngrok token from .env file\")\n",
        "    except ImportError:\n",
        "        # dotenv not installed, try environment variable directly\n",
        "        NGROK_TOKEN = os.getenv('NGROK_AUTHTOKEN')\n",
        "        if NGROK_TOKEN:\n",
        "            print(\"‚úÖ Loaded ngrok token from environment variable\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not load .env file: {e}\")\n",
        "\n",
        "# Fallback to environment variable if still not found\n",
        "if not NGROK_TOKEN:\n",
        "    NGROK_TOKEN = os.getenv('NGROK_AUTHTOKEN')\n",
        "\n",
        "if not NGROK_TOKEN:\n",
        "    print(\"\\n‚ùå ERROR: NGROK_AUTHTOKEN not found!\")\n",
        "    print(\"\\nüìù How to set it:\")\n",
        "    print(\"   For Google Colab:\")\n",
        "    print(\"   1. Go to: Runtime ‚Üí Manage secrets\")\n",
        "    print(\"   2. Add secret: NGROK_AUTHTOKEN = your_token_here\")\n",
        "    print(\"   3. Get token from: https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
        "    print(\"\\n   For Local (Jupyter/Local Python):\")\n",
        "    print(\"   1. Create a .env file in this directory\")\n",
        "    print(\"   2. Add: NGROK_AUTHTOKEN=your_token_here\")\n",
        "    print(\"   3. Get token from: https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
        "    print(\"\\n   Or set environment variable:\")\n",
        "    print(\"   export NGROK_AUTHTOKEN=your_token_here\")\n",
        "    raise SystemExit(\"NGROK_AUTHTOKEN not configured\")\n",
        "\n",
        "try:\n",
        "    ngrok.set_auth_token(NGROK_TOKEN)\n",
        "    print(\"‚úÖ ngrok token configured successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Warning: Could not set ngrok token: {e}\")\n",
        "    print(\"   Continuing without ngrok (local access only)\")\n",
        "\n",
        "# Disconnect any remaining tunnels via API\n",
        "print(\"üîå Disconnecting any remaining tunnels...\")\n",
        "try:\n",
        "    tunnels = ngrok.get_tunnels()\n",
        "    for tunnel in tunnels:\n",
        "        ngrok.disconnect(tunnel.public_url)\n",
        "        print(f\"   Disconnected: {tunnel.public_url}\")\n",
        "    if tunnels:\n",
        "        time.sleep(2)  # Wait for disconnections to complete\n",
        "    print(\"‚úÖ All tunnels disconnected\")\n",
        "except Exception as e:\n",
        "    print(f\"   Note: {e}\")\n",
        "\n",
        "# 4) Start Streamlit locally\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Kill any existing streamlit on port 8501\n",
        "try:\n",
        "    if os.name == 'nt':  # Windows\n",
        "        os.system('netstat -ano | findstr :8501')\n",
        "    else:  # macOS/Linux\n",
        "        os.system('lsof -ti:8501 | xargs kill -9 2>/dev/null || true')\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Start Streamlit\n",
        "print(\"\\nüöÄ Starting Streamlit...\")\n",
        "try:\n",
        "    if sys.platform.startswith('win'):\n",
        "        subprocess.Popen(\n",
        "            [sys.executable, \"-m\", \"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\", \"--server.headless\", \"true\"],\n",
        "            creationflags=subprocess.CREATE_NEW_CONSOLE\n",
        "        )\n",
        "    else:\n",
        "        subprocess.Popen(\n",
        "            [\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\", \"--server.headless\", \"true\"],\n",
        "            stdout=subprocess.DEVNULL,\n",
        "            stderr=subprocess.DEVNULL,\n",
        "            start_new_session=True\n",
        "        )\n",
        "    \n",
        "    time.sleep(5)  # Give Streamlit time to start\n",
        "    print(\"‚úÖ Streamlit started!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error starting Streamlit: {e}\")\n",
        "    print(\"   You can start it manually with: streamlit run app.py\")\n",
        "\n",
        "# Create ngrok tunnel\n",
        "print(\"\\nüåê Creating public URL with ngrok...\")\n",
        "try:\n",
        "    public_url = ngrok.connect(8501)\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"‚úÖ SUCCESS! Your app is running!\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\nüåê Public URL (share this):\")\n",
        "    print(f\"   {public_url}\")\n",
        "    print(f\"\\nüè† Local URL:\")\n",
        "    print(f\"   http://localhost:8501\")\n",
        "    print(f\"\\nüìå Tips:\")\n",
        "    print(f\"   ‚Ä¢ Keep this notebook running\")\n",
        "    print(f\"   ‚Ä¢ Upload PDFs and build the index\")\n",
        "    print(f\"   ‚Ä¢ Ask questions to get answers from your documents\")\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    \n",
        "except Exception as e:\n",
        "    error_msg = str(e)\n",
        "    print(f\"\\n‚ö†Ô∏è Could not create ngrok tunnel: {e}\")\n",
        "    \n",
        "    # Check for session limit error (ERR_NGROK_108)\n",
        "    if \"ERR_NGROK_108\" in error_msg or \"3 simultaneous\" in error_msg or \"agent sessions\" in error_msg:\n",
        "        print(\"\\nüí° Issue: You've reached ngrok's free account limit (3 simultaneous sessions)\")\n",
        "        print(\"   These sessions are running on OTHER machines (not this one)\")\n",
        "        print(\"\\nüîß How to fix:\")\n",
        "        print(\"   1. Go to: https://dashboard.ngrok.com/agents\")\n",
        "        print(\"   2. MANUALLY disconnect all active agent sessions\")\n",
        "        print(\"   3. Then re-run this cell\")\n",
        "        print(\"\\nüìå App is running locally at: http://localhost:8501\")\n",
        "        print(\"   (You can still use the app locally without ngrok)\")\n",
        "    elif \"ERR_NGROK_334\" in error_msg or \"already online\" in error_msg:\n",
        "        print(\"\\nüí° Issue: An ngrok endpoint is already registered to your account\")\n",
        "        print(\"   This happens when a previous session didn't close properly\")\n",
        "        print(\"\\nüîß How to fix:\")\n",
        "        print(\"   1. Go to: https://dashboard.ngrok.com/agents\")\n",
        "        print(\"   2. MANUALLY disconnect all active agent sessions\")\n",
        "        print(\"   3. Wait 30 seconds\")\n",
        "        print(\"   4. Go to: Runtime ‚Üí Restart runtime (in Colab menu)\")\n",
        "        print(\"   5. Re-run this cell\")\n",
        "        print(\"\\nüìå App is running locally at: http://localhost:8501\")\n",
        "        print(\"   (You can still use the app locally without ngrok)\")\n",
        "    else:\n",
        "        print(\"\\nüìå App is running locally at: http://localhost:8501\")\n",
        "        print(\"   (ngrok tunnel failed, but local access works)\")\n",
        "        print(\"\\nüîß Troubleshooting:\")\n",
        "        print(\"   1. Check your ngrok token is correct\")\n",
        "        print(\"   2. Try restarting the runtime and running again\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example Usage\n",
        "\n",
        "### Step 1: Upload PDFs\n",
        "Upload one or more PDF documents using the file uploader in the Streamlit interface.\n",
        "\n",
        "### Step 2: Build Index\n",
        "Click \"Build / Rebuild Index\" to:\n",
        "- Extract text from PDFs\n",
        "- Split into chunks\n",
        "- Generate embeddings\n",
        "- Build FAISS vector index\n",
        "\n",
        "### Step 3: Ask Questions\n",
        "Enter your question and adjust parameters:\n",
        "- **Top-k chunks**: Number of relevant chunks to retrieve (2-8, default: 4)\n",
        "- **Max tokens**: Maximum length of answer (64-512, default: 256)\n",
        "\n",
        "### Example Questions:\n",
        "- \"What is the main topic of this document?\"\n",
        "- \"Summarize the key findings\"\n",
        "- \"Explain the methodology used\"\n",
        "- \"What are the limitations mentioned?\"\n",
        "\n",
        "### Understanding the Output:\n",
        "- **Retrieved Chunks**: Shows the document sections used to answer\n",
        "- **Answer**: Generated response grounded in the retrieved context"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
