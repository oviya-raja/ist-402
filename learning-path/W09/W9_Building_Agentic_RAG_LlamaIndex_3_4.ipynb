{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/oviya-raja/ist-402/blob/main/learning-path/W09/W9_Building_Agentic_RAG_LlamaIndex_3_4.ipynb)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bqVkxISRAGy"
      },
      "source": [
        "# Building Agentic RAG with LlamaIndex - Complete Notebook Content\n",
        "\n",
        "This notebook contains all lessons from the course on building agentic RAG systems using LlamaIndex."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVenqsSbRJGa"
      },
      "source": [
        "Setup and Installation\n",
        "First, let's install the required packages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3m9sre_RQoS"
      },
      "source": [
        "# Setup and Installation\n",
        "First, let's install the required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk2nfXE6RFva",
        "outputId": "a30ddc7a-983e-4ef6-b7dd-9a452247100c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (25.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: llama-index in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (0.14.10)\n",
            "Requirement already satisfied: llama-index-cli<0.6,>=0.5.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index) (0.5.3)\n",
            "Requirement already satisfied: llama-index-core<0.15.0,>=0.14.10 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index) (0.14.10)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.6,>=0.5.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index) (0.5.1)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index) (0.9.4)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.7,>=0.6.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index) (0.6.11)\n",
            "Requirement already satisfied: llama-index-readers-file<0.6,>=0.5.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index) (0.5.5)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index) (0.5.1)\n",
            "Requirement already satisfied: nltk>3.8.1 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index) (3.9.2)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index) (3.13.2)\n",
            "Requirement already satisfied: aiosqlite in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index) (0.22.0)\n",
            "Requirement already satisfied: banks<3,>=2.2.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index) (2.2.0)\n",
            "Requirement already satisfied: dataclasses-json in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index) (2025.12.0)\n",
            "Requirement already satisfied: httpx in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index) (0.28.1)\n",
            "Requirement already satisfied: llama-index-workflows!=2.9.0,<3,>=2 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index) (2.11.5)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index) (3.6.1)\n",
            "Requirement already satisfied: numpy in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index) (2.3.5)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index) (12.0.0)\n",
            "Requirement already satisfied: platformdirs in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index) (4.5.1)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index) (2.12.5)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.31.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index) (2.32.5)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index) (80.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.10->llama-index) (2.0.45)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index) (9.1.2)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index) (0.12.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index) (4.15.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index) (1.17.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.10->llama-index) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.10->llama-index) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.10->llama-index) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.10->llama-index) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.10->llama-index) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.10->llama-index) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.10->llama-index) (1.22.0)\n",
            "Requirement already satisfied: griffe in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.10->llama-index) (1.15.0)\n",
            "Requirement already satisfied: jinja2 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.10->llama-index) (3.1.6)\n",
            "Requirement already satisfied: openai>=1.1.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (2.8.1)\n",
            "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (4.14.3)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (0.7.1)\n",
            "Requirement already satisfied: pandas<2.3.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.2.3)\n",
            "Requirement already satisfied: pypdf<7,>=6.1.3 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (6.4.2)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (0.0.26)\n",
            "Requirement already satisfied: soupsieve>=1.6.1 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.8)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-workflows!=2.9.0,<3,>=2->llama-index-core<0.15.0,>=0.14.10->llama-index) (0.4.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (3.11)\n",
            "Requirement already satisfied: certifi in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.15.0,>=0.14.10->llama-index) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.15.0,>=0.14.10->llama-index) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-index-core<0.15.0,>=0.14.10->llama-index) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2025.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.15.0,>=0.14.10->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.15.0,>=0.14.10->llama-index) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.15.0,>=0.14.10->llama-index) (0.4.2)\n",
            "Requirement already satisfied: llama-cloud==0.1.35 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.35)\n",
            "Requirement already satisfied: llama-parse>=0.5.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.54)\n",
            "Requirement already satisfied: llama-cloud-services>=0.6.54 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.54)\n",
            "Requirement already satisfied: click<9,>=8.1.7 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-cloud-services>=0.6.54->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (8.3.1)\n",
            "Requirement already satisfied: python-dotenv<2,>=1.0.1 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-cloud-services>=0.6.54->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.2.1)\n",
            "Requirement already satisfied: joblib in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from nltk>3.8.1->llama-index) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from nltk>3.8.1->llama-index) (2025.11.3)\n",
            "Requirement already satisfied: six>=1.5 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.15.0,>=0.14.10->llama-index) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.15.0,>=0.14.10->llama-index) (2.6.2)\n",
            "Requirement already satisfied: greenlet>=1 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.10->llama-index) (3.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.15.0,>=0.14.10->llama-index) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from dataclasses-json->llama-index-core<0.15.0,>=0.14.10->llama-index) (3.26.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.15.0,>=0.14.10->llama-index) (25.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.10->llama-index) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.10->llama-index) (3.0.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: llama-index-llms-openai in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (0.6.11)\n",
            "Requirement already satisfied: llama-index-core<0.15,>=0.14.5 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-llms-openai) (0.14.10)\n",
            "Requirement already satisfied: openai<3,>=1.108.1 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-llms-openai) (2.8.1)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (3.13.2)\n",
            "Requirement already satisfied: aiosqlite in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (0.22.0)\n",
            "Requirement already satisfied: banks<3,>=2.2.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (2.2.0)\n",
            "Requirement already satisfied: dataclasses-json in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (2025.12.0)\n",
            "Requirement already satisfied: httpx in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (0.28.1)\n",
            "Requirement already satisfied: llama-index-workflows!=2.9.0,<3,>=2 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (2.11.5)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (3.6.1)\n",
            "Requirement already satisfied: nltk>3.8.1 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (3.9.2)\n",
            "Requirement already satisfied: numpy in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (2.3.5)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (12.0.0)\n",
            "Requirement already satisfied: platformdirs in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (4.5.1)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (2.12.5)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.31.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (2.32.5)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (80.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (2.0.45)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (9.1.2)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (0.12.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (4.15.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (1.17.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (1.22.0)\n",
            "Requirement already satisfied: griffe in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (1.15.0)\n",
            "Requirement already satisfied: jinja2 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (3.1.6)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-workflows!=2.9.0,<3,>=2->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (0.4.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from openai<3,>=1.108.1->llama-index-llms-openai) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from openai<3,>=1.108.1->llama-index-llms-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from openai<3,>=1.108.1->llama-index-llms-openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from openai<3,>=1.108.1->llama-index-llms-openai) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai<3,>=1.108.1->llama-index-llms-openai) (3.11)\n",
            "Requirement already satisfied: certifi in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (0.4.2)\n",
            "Requirement already satisfied: click in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (8.3.1)\n",
            "Requirement already satisfied: joblib in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (2025.11.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (2.6.2)\n",
            "Requirement already satisfied: greenlet>=1 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (3.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from dataclasses-json->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (3.26.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (25.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.15,>=0.14.5->llama-index-llms-openai) (3.0.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: llama-index-embeddings-openai in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (0.5.1)\n",
            "Requirement already satisfied: llama-index-core<0.15,>=0.13.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-embeddings-openai) (0.14.10)\n",
            "Requirement already satisfied: openai>=1.1.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-embeddings-openai) (2.8.1)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (3.13.2)\n",
            "Requirement already satisfied: aiosqlite in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (0.22.0)\n",
            "Requirement already satisfied: banks<3,>=2.2.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (2.2.0)\n",
            "Requirement already satisfied: dataclasses-json in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (2025.12.0)\n",
            "Requirement already satisfied: httpx in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (0.28.1)\n",
            "Requirement already satisfied: llama-index-workflows!=2.9.0,<3,>=2 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (2.11.5)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (3.6.1)\n",
            "Requirement already satisfied: nltk>3.8.1 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (3.9.2)\n",
            "Requirement already satisfied: numpy in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (2.3.5)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (12.0.0)\n",
            "Requirement already satisfied: platformdirs in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (4.5.1)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (2.12.5)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.31.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (2.32.5)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (80.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (2.0.45)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (9.1.2)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (0.12.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (4.15.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (1.17.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (1.22.0)\n",
            "Requirement already satisfied: griffe in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (1.15.0)\n",
            "Requirement already satisfied: jinja2 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (3.1.6)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from llama-index-workflows!=2.9.0,<3,>=2->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (0.4.2)\n",
            "Requirement already satisfied: idna>=2.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (3.11)\n",
            "Requirement already satisfied: click in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (8.3.1)\n",
            "Requirement already satisfied: joblib in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (2025.11.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from openai>=1.1.0->llama-index-embeddings-openai) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from openai>=1.1.0->llama-index-embeddings-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from openai>=1.1.0->llama-index-embeddings-openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from openai>=1.1.0->llama-index-embeddings-openai) (1.3.1)\n",
            "Requirement already satisfied: certifi in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (2.6.2)\n",
            "Requirement already satisfied: greenlet>=1 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (3.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from dataclasses-json->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (3.26.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (25.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-openai) (3.0.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: nest-asyncio in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (1.6.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: openai in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (2.8.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from openai) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from openai) (2.12.5)\n",
            "Requirement already satisfied: sniffio in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: python-dotenv in /Users/rajasoun/workspace/personal/oviya/ist-402/learning-path/W07/.venv/lib/python3.12/site-packages (1.2.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade pip\n",
        "%pip install llama-index\n",
        "%pip install llama-index-llms-openai\n",
        "%pip install llama-index-embeddings-openai\n",
        "%pip install nest-asyncio\n",
        "%pip install openai\n",
        "%pip install python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSJABWx7Rb77"
      },
      "source": [
        "## Set up OpenAI API Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w75jomMERZHb",
        "outputId": "214899eb-6d80-4644-c008-24ce3a942df0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " OpenAI API Key loaded from environment variables!\n",
            "OpenAI API Key configured successfully!\n"
          ]
        }
      ],
      "source": [
        "# Set up OpenAI API Key\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Try to get API key from Google Colab userdata first (if running in Colab)\n",
        "OPENAI_API_KEY = None\n",
        "try:\n",
        "    import google.colab\n",
        "    from google.colab import userdata\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "    if OPENAI_API_KEY:\n",
        "        print(\" OpenAI API Key loaded from Colab userdata!\")\n",
        "except (ImportError, ValueError):\n",
        "    # Not running in Colab or userdata not available, try environment variables\n",
        "    pass\n",
        "\n",
        "# If not found in Colab userdata, try environment variables\n",
        "if not OPENAI_API_KEY:\n",
        "    # Load environment variables from .env file\n",
        "    load_dotenv()\n",
        "    \n",
        "    # Get OpenAI API Key from environment variable\n",
        "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "    if OPENAI_API_KEY:\n",
        "        print(\" OpenAI API Key loaded from environment variables!\")\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"OPENAI_API_KEY not found. Please set it in one of the following ways:\\n\"\n",
        "            \"  - In Google Colab: userdata.set('OPENAI_API_KEY', 'your_key')\\n\"\n",
        "            \"  - Locally: Create a .env file with OPENAI_API_KEY=your_key\\n\"\n",
        "            \"  - Or set environment variable: export OPENAI_API_KEY=your_key\"\n",
        "        )\n",
        "\n",
        "# Ensure the API key is set in the environment for OpenAI libraries\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "print(\"OpenAI API Key configured successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JCcEpZn5RkLU"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDa_MkmeRzEE"
      },
      "source": [
        "# Lesson 1: Router Engine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlO-_ImmR8u1"
      },
      "source": [
        "### Load Data\n",
        "Download the MetaGPT paper:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IWJozRpR3KM",
        "outputId": "3ec00f0e-512a-4ada-d9d5-000e5e3fb743"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-12-14 18:39:16--  https://openreview.net/pdf?id=VtmBAGCN7o\n",
            "Resolving openreview.net (openreview.net)... 34.57.44.88\n",
            "Connecting to openreview.net (openreview.net)|34.57.44.88|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16911937 (16M) [application/pdf]\n",
            "Saving to: data/metagpt.pdf\n",
            "\n",
            "data/metagpt.pdf    100%[===================>]  16.13M  28.7MB/s    in 0.6s    \n",
            "\n",
            "2025-12-14 18:39:17 (28.7 MB/s) - data/metagpt.pdf saved [16911937/16911937]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create data directory if it doesn't exist\n",
        "import os\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# Download the MetaGPT paper\n",
        "!wget \"https://openreview.net/pdf?id=VtmBAGCN7o\" -O data/metagpt.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "urqMuc3uSF0l"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "# load documents\n",
        "documents = SimpleDirectoryReader(input_files=[\"data/metagpt.pdf\"]).load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKf_IJp-TMah"
      },
      "source": [
        "## Define LLM and Embedding Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "T-2smc6ITQUh"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "splitter = SentenceSplitter(chunk_size=1024)\n",
        "nodes = splitter.get_nodes_from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fk6roZbpTR_h"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Settings\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxpZND2rTUbZ"
      },
      "source": [
        "## Define Summary Index and Vector Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KbdFzj_GTWPz"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:39:19,220 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
        "\n",
        "summary_index = SummaryIndex(nodes)\n",
        "vector_index = VectorStoreIndex(nodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBLsNW0YTYUd"
      },
      "source": [
        "## Define Query Engines and Set Metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tBy7Qd_gTafp"
      },
      "outputs": [],
      "source": [
        "summary_query_engine = summary_index.as_query_engine(\n",
        "    response_mode=\"tree_summarize\",\n",
        "    use_async=True,\n",
        ")\n",
        "vector_query_engine = vector_index.as_query_engine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lFLi0Z32TdSa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating summary tool...\n",
            " Summary tool created successfully\n",
            "  Description: Useful for summarization questions related to MetaGPT\n",
            "\n",
            "Creating vector tool...\n",
            " Vector tool created successfully\n",
            "  Description: Useful for retrieving specific context from the MetaGPT paper.\n",
            "\n",
            " Both tools are ready to use!\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "print(\"Creating summary tool...\")\n",
        "summary_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=summary_query_engine,\n",
        "    description=(\n",
        "        \"Useful for summarization questions related to MetaGPT\"\n",
        "    ),\n",
        ")\n",
        "print(f\" Summary tool created successfully\")\n",
        "print(f\"  Description: {summary_tool.metadata.description}\")\n",
        "\n",
        "print(\"\\nCreating vector tool...\")\n",
        "vector_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=vector_query_engine,\n",
        "    description=(\n",
        "        \"Useful for retrieving specific context from the MetaGPT paper.\"\n",
        "    ),\n",
        ")\n",
        "print(f\" Vector tool created successfully\")\n",
        "print(f\"  Description: {vector_tool.metadata.description}\")\n",
        "print(\"\\n Both tools are ready to use!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "di42S-cSTgFI"
      },
      "source": [
        "## Define Router Query Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4c_U0uO_TfeK"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "\n",
        "query_engine = RouterQueryEngine(\n",
        "    selector=LLMSingleSelector.from_defaults(),\n",
        "    query_engine_tools=[\n",
        "        summary_tool,\n",
        "        vector_tool,\n",
        "    ],\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQ9zrRopTjiJ",
        "outputId": "9fda8d22-7294-4515-f437-dfc96e542574"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:39:20,934 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:39:20,943 - INFO - Selecting query engine 0: Useful for summarization questions related to MetaGPT.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 0: Useful for summarization questions related to MetaGPT.\n",
            "\u001b[0m"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:39:22,350 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:39:22,920 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:39:24,385 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The document introduces MetaGPT, a meta-programming framework that utilizes Standardized Operating Procedures (SOPs) to enhance multi-agent systems based on Large Language Models (LLMs). It incorporates role specialization, workflow management, and efficient communication mechanisms to improve code generation quality. Through experiments, MetaGPT demonstrates high performance in collaborative software engineering tasks. The document also details the development of a \"Drawing App\" using MetaGPT, discussing requirements, UI design, implementation, and testing strategies for the color meter feature. It highlights the use of Python libraries for GUI creation and color selection. Additionally, it covers task distribution among team members, performance comparisons with other models, and ethical considerations related to AI frameworks.\n",
            "34\n"
          ]
        }
      ],
      "source": [
        "response = query_engine.query(\"What is the summary of the document?\")\n",
        "print(str(response))\n",
        "print(len(response.source_nodes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qbzrYinTm38",
        "outputId": "88faf2a7-74cf-4fe4-e75f-97b51b0adf5b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:39:25,629 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:39:25,632 - INFO - Selecting query engine 1: This choice is more relevant as it focuses on retrieving specific context from the MetaGPT paper, which may contain information on how agents share information with other agents..\n",
            "2025-12-14 18:39:25,788 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 1: This choice is more relevant as it focuses on retrieving specific context from the MetaGPT paper, which may contain information on how agents share information with other agents..\n",
            "\u001b[0m"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:39:26,586 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agents share information with other agents by utilizing a shared message pool where they publish structured messages. Additionally, agents can subscribe to relevant messages based on their profiles. This approach allows all agents to exchange messages directly and access messages from other entities transparently, enhancing communication efficiency.\n"
          ]
        }
      ],
      "source": [
        "response = query_engine.query(\n",
        "    \"How do agents share information with other agents?\"\n",
        ")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7P2fB-CTvr8"
      },
      "source": [
        "# Lesson 2: Tool Calling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60323azIT4lL"
      },
      "source": [
        "### 1. Define a Simple Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PZ8ZXkAQTxLD"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.tools import FunctionTool\n",
        "\n",
        "def add(x: int, y: int) -> int:\n",
        "    \"\"\"Adds two integers together.\"\"\"\n",
        "    return x + y\n",
        "\n",
        "def mystery(x: int, y: int) -> int:\n",
        "    \"\"\"Mystery function that operates on top of two numbers.\"\"\"\n",
        "    return (x + y) * (x + y)\n",
        "\n",
        "add_tool = FunctionTool.from_defaults(fn=add)\n",
        "mystery_tool = FunctionTool.from_defaults(fn=mystery)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIcGPDXyT7oM",
        "outputId": "824b01b8-0572-4041-bbd7-3d069bb0ea03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1: Initializing OpenAI LLM (gpt-3.5-turbo)...\n",
            " LLM initialized\n",
            "\n",
            "Step 2: LLM analyzing query and selecting appropriate tool...\n",
            "Query: 'Tell me the output of the mystery function on 2 and 9'\n",
            "Available tools: add_tool, mystery_tool\n",
            "\n",
            "LLM reasoning process (verbose=True shows this):\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:39:27,778 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Calling Function ===\n",
            "Calling function: mystery with args: {\"x\": 2, \"y\": 9}\n",
            "=== Function Output ===\n",
            "121\n",
            "------------------------------------------------------------\n",
            "\n",
            "Step 3: Final response from LLM:\n",
            "============================================================\n",
            "121\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "WHAT HAPPENED:\n",
            "============================================================\n",
            "1. The LLM received your query asking about 'mystery function'\n",
            "2. It analyzed the available tools and chose 'mystery_tool'\n",
            "3. It extracted the arguments: x=2, y=9\n",
            "4. It called mystery_tool(2, 9) which calculates: (2+9) * (2+9) = 121\n",
            "5. It returned the result: 121\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# EXPLANATION: Using LLM with Function/Tool Calling\n",
        "# ============================================================================\n",
        "# This demonstrates how an LLM can intelligently choose and call functions/tools\n",
        "# based on a natural language query.\n",
        "\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "# Step 1: Initialize the OpenAI LLM\n",
        "# This creates a connection to OpenAI's GPT-3.5-turbo model\n",
        "print(\"Step 1: Initializing OpenAI LLM (gpt-3.5-turbo)...\")\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "print(\" LLM initialized\\n\")\n",
        "\n",
        "# Step 2: Use predict_and_call to let the LLM decide which tool to use\n",
        "# The LLM will:\n",
        "#   - Analyze the query: \"Tell me the output of the mystery function on 2 and 9\"\n",
        "#   - Understand it needs to call a function with arguments 2 and 9\n",
        "#   - Choose the appropriate tool (mystery_tool in this case)\n",
        "#   - Call the function with the correct arguments\n",
        "#   - Return the result\n",
        "\n",
        "print(\"Step 2: LLM analyzing query and selecting appropriate tool...\")\n",
        "print(\"Query: 'Tell me the output of the mystery function on 2 and 9'\")\n",
        "print(\"Available tools: add_tool, mystery_tool\")\n",
        "print(\"\\nLLM reasoning process (verbose=True shows this):\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "response = llm.predict_and_call(\n",
        "    [add_tool, mystery_tool],  # List of available tools the LLM can choose from\n",
        "    \"Tell me the output of the mystery function on 2 and 9\",  # User's query\n",
        "    verbose=True  # Shows the LLM's decision-making process\n",
        ")\n",
        "\n",
        "print(\"-\" * 60)\n",
        "print(\"\\nStep 3: Final response from LLM:\")\n",
        "print(\"=\" * 60)\n",
        "print(str(response))\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Explanation of what happened:\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"WHAT HAPPENED:\")\n",
        "print(\"=\" * 60)\n",
        "print(\"1. The LLM received your query asking about 'mystery function'\")\n",
        "print(\"2. It analyzed the available tools and chose 'mystery_tool'\")\n",
        "print(\"3. It extracted the arguments: x=2, y=9\")\n",
        "print(\"4. It called mystery_tool(2, 9) which calculates: (2+9) * (2+9) = 121\")\n",
        "print(\"5. It returned the result: 121\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bk-Waqk2T-FU"
      },
      "source": [
        "### 2. Define an Auto-Retrieval Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "5O_iv2aTT_fz"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "# load documents\n",
        "documents = SimpleDirectoryReader(input_files=[\"data/metagpt.pdf\"]).load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJjN83C8UCIz",
        "outputId": "5ec5f732-ac37-442b-caa0-3311e737779c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "page_label: 1\n",
            "file_name: metagpt.pdf\n",
            "file_path: data/metagpt.pdf\n",
            "file_type: application/pdf\n",
            "file_size: 16911937\n",
            "creation_date: 2025-12-14\n",
            "last_modified_date: 2025-12-14\n",
            "\n",
            "Preprint\n",
            "METAGPT: M ETA PROGRAMMING FOR A\n",
            "MULTI -AGENT COLLABORATIVE FRAMEWORK\n",
            "Sirui Hong1, Mingchen Zhuge2, Jonathan Chen1, Xiawu Zheng3, Yuheng Cheng4,\n",
            "Ceyao Zhang4, Jinlin Wang1, Zili Wang, Steven Ka Shing Yau5, Zijuan Lin4,\n",
            "Liyang Zhou6, Chenyu Ran1, Lingfeng Xiao1,7, Chenglin Wu1, Jurgen Schmidhuber2,8\n",
            "1DeepWisdom, 2AI Initiative, King Abdullah University of Science and Technology,\n",
            "3Xiamen University, 4The Chinese University of Hong Kong, Shenzhen,\n",
            "5Nanjing University, 6University of Pennsylvania,\n",
            "7University of California, Berkeley, 8The Swiss AI Lab IDSIA/USI/SUPSI\n",
            "ABSTRACT\n",
            "Remarkable progress has been made on automated problem solving through so-\n",
            "cieties of agents based on large language models (LLMs). Existing LLM-based\n",
            "multi-agent systems can already solve simple dialogue tasks. Solutions to more\n",
            "complex tasks, however, are complicated through logic inconsistencies due to\n",
            "cascading hallucinations caused by naively chaining LLMs. Here we introduce\n",
            "MetaGPT, an innovative meta-programming framework incorporating efficient\n",
            "human workflows into LLM-based multi-agent collaborations. MetaGPT en-\n",
            "codes Standardized Operating Procedures (SOPs) into prompt sequences for more\n",
            "streamlined workflows, thus allowing agents with human-like domain expertise\n",
            "to verify intermediate results and reduce errors. MetaGPT utilizes an assembly\n",
            "line paradigm to assign diverse roles to various agents, efficiently breaking down\n",
            "complex tasks into subtasks involving many agents working together. On col-\n",
            "laborative software engineering benchmarks, MetaGPT generates more coherent\n",
            "solutions than previous chat-based multi-agent systems. Our project can be found\n",
            "at https://github.com/geekan/MetaGPT.\n",
            "1 I NTRODUCTION\n",
            "Autonomous agents utilizing Large Language Models (LLMs) offer promising opportunities to en-\n",
            "hance and replicate human workflows. In real-world applications, however, existing systems (Park\n",
            "et al., 2023; Zhuge et al., 2023; Cai et al., 2023; Wang et al., 2023c; Li et al., 2023; Du et al., 2023;\n",
            "Liang et al., 2023; Hao et al., 2023) tend to oversimplify the complexities. They struggle to achieve\n",
            "effective, coherent, and accurate problem-solving processes, particularly when there is a need for\n",
            "meaningful collaborative interaction (Chen et al., 2024; Zhang et al., 2023; Dong et al., 2023; Zhou\n",
            "et al., 2023; Qian et al., 2023).\n",
            "Through extensive collaborative practice, humans have developed widely accepted Standardized\n",
            "Operating Procedures (SOPs) across various domains (Belbin, 2012; Manifesto, 2001; DeMarco &\n",
            "Lister, 2013). These SOPs play a critical role in supporting task decomposition and effective coor-\n",
            "dination. Furthermore, SOPs outline the responsibilities of each team member, while establishing\n",
            "standards for intermediate outputs. Well-defined SOPs improve the consistent and accurate exe-\n",
            "cution of tasks that align with defined roles and quality standards (Belbin, 2012; Manifesto, 2001;\n",
            "DeMarco & Lister, 2013; Wooldridge & Jennings, 1998). For instance, in a software company,\n",
            "Product Managers analyze competition and user needs to create Product Requirements Documents\n",
            "(PRDs) using a standardized structure, to guide the developmental process.\n",
            "Inspired by such ideas, we design a promising GPT -based Meta-Programming framework called\n",
            "MetaGPT that significantly benefits from SOPs. Unlike other works (Li et al., 2023; Qian et al.,\n",
            "2023), MetaGPT requires agents to generate structured outputs, such as high-quality requirements\n",
            "These authors contributed equally to this work.\n",
            "Chenglin Wu (alexanderwu@fuzhi.ai) is the corresponding author, affiliated with DeepWisdom.\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "splitter = SentenceSplitter(chunk_size=1024)\n",
        "nodes = splitter.get_nodes_from_documents(documents)\n",
        "print(nodes[0].get_content(metadata_mode=\"all\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "2_O-RStXUDjM"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:39:28,726 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "vector_index = VectorStoreIndex(nodes)\n",
        "query_engine = vector_index.as_query_engine(similarity_top_k=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQyExE-bUFSk",
        "outputId": "44577393-554b-4d3a-d7f7-84ba651fa9bb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:39:29,053 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:39:30,320 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MetaGPT achieves a new state-of-the-art (SoTA) in code generation benchmarks with 85.9% and 87.7% in Pass@1. It stands out in handling higher levels of software complexity and offering extensive functionality. In experimental evaluations, MetaGPT achieves a 100% task completion rate, demonstrating robustness and efficiency in design.\n",
            "{'page_label': '2', 'file_name': 'metagpt.pdf', 'file_path': 'data/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-12-14', 'last_modified_date': '2025-12-14'}\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.vector_stores import MetadataFilters\n",
        "\n",
        "query_engine = vector_index.as_query_engine(\n",
        "    similarity_top_k=2,\n",
        "    filters=MetadataFilters.from_dicts(\n",
        "        [\n",
        "            {\"key\": \"page_label\", \"value\": \"2\"}\n",
        "        ]\n",
        "    )\n",
        ")\n",
        "\n",
        "response = query_engine.query(\n",
        "    \"What are some high-level results of MetaGPT?\",\n",
        ")\n",
        "print(str(response))\n",
        "for n in response.source_nodes:\n",
        "    print(n.metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGMYtivzUIR-"
      },
      "source": [
        "### Define the Auto-Retrieval Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "a69NkZQDUOYE"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from llama_index.core.vector_stores import FilterCondition\n",
        "\n",
        "def vector_query(\n",
        "    query: str,\n",
        "    page_numbers: List[str]\n",
        ") -> str:\n",
        "    \"\"\"Perform a vector search over an index.\n",
        "\n",
        "    query (str): the string query to be embedded.\n",
        "    page_numbers (List[str]): Filter by set of pages. Leave BLANK if we want to perform a vector search\n",
        "        over all pages. Otherwise, filter by the set of specified pages.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    metadata_dicts = [\n",
        "        {\"key\": \"page_label\", \"value\": p} for p in page_numbers\n",
        "    ]\n",
        "\n",
        "    query_engine = vector_index.as_query_engine(\n",
        "        similarity_top_k=2,\n",
        "        filters=MetadataFilters.from_dicts(\n",
        "            metadata_dicts,\n",
        "            condition=FilterCondition.OR\n",
        "        )\n",
        "    )\n",
        "    response = query_engine.query(query)\n",
        "    return response\n",
        "\n",
        "\n",
        "vector_query_tool = FunctionTool.from_defaults(\n",
        "    name=\"vector_tool\",\n",
        "    fn=vector_query\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGWYH4bBUTgl",
        "outputId": "29aac91d-d332-426b-fb25-424143d6d4a2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:39:30,812 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:39:30,921 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Calling Function ===\n",
            "Calling function: vector_tool with args: {\"query\": \"high-level results of MetaGPT\", \"page_numbers\": [\"2\"]}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:39:31,565 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Function Output ===\n",
            "MetaGPT achieves a new state-of-the-art in code generation benchmarks with 85.9% and 87.7% in Pass@1. It stands out in handling higher levels of software complexity and offering extensive functionality, demonstrating robustness and efficiency in task completion.\n",
            "{'page_label': '2', 'file_name': 'metagpt.pdf', 'file_path': 'data/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-12-14', 'last_modified_date': '2025-12-14'}\n"
          ]
        }
      ],
      "source": [
        "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "response = llm.predict_and_call(\n",
        "    [vector_query_tool],\n",
        "    \"What are the high-level results of MetaGPT as described on page 2?\",\n",
        "    verbose=True\n",
        ")\n",
        "for n in response.source_nodes:\n",
        "    print(n.metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdI82mq4UVAf"
      },
      "source": [
        "### Add More Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "dFANjpM0UbS1"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SummaryIndex\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "summary_index = SummaryIndex(nodes)\n",
        "summary_query_engine = summary_index.as_query_engine(\n",
        "    response_mode=\"tree_summarize\",\n",
        "    use_async=True,\n",
        ")\n",
        "summary_tool = QueryEngineTool.from_defaults(\n",
        "    name=\"summary_tool\",\n",
        "    query_engine=summary_query_engine,\n",
        "    description=(\n",
        "        \"Useful if you want to get a summary of MetaGPT\"\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tN6TUTjIUeP7",
        "outputId": "bcb59ce3-836b-4880-ccc5-354c53cecb34"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:39:32,180 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Calling Function ===\n",
            "Calling function: vector_tool with args: {\"query\": \"MetaGPT comparisons with ChatDev\", \"page_numbers\": [\"8\"]}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:39:32,637 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:39:33,397 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Function Output ===\n",
            "MetaGPT outperforms ChatDev on the SoftwareDev dataset in various aspects. For example, MetaGPT achieves a higher score in executability, takes less time for execution, requires more tokens but fewer tokens per line of code compared to ChatDev. Additionally, MetaGPT demonstrates superior performance in code statistics and human revision cost when compared to ChatDev.\n",
            "{'page_label': '8', 'file_name': 'metagpt.pdf', 'file_path': 'data/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-12-14', 'last_modified_date': '2025-12-14'}\n"
          ]
        }
      ],
      "source": [
        "response = llm.predict_and_call(\n",
        "    [vector_query_tool, summary_tool],\n",
        "    \"What are the MetaGPT comparisons with ChatDev described on page 8?\",\n",
        "    verbose=True\n",
        ")\n",
        "for n in response.source_nodes:\n",
        "    print(n.metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLOx8OpkUgDc",
        "outputId": "617f1f07-d5be-4cfd-8db8-8cfcae26e267"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:39:33,899 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:39:33,992 - INFO - Retrying request to /chat/completions in 0.435596 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Calling Function ===\n",
            "Calling function: summary_tool with args: {\"input\": \"The paper discusses the impact of climate change on biodiversity and ecosystems.\"}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:39:34,463 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:39:35,563 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:39:36,288 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Function Output ===\n",
            "The paper does not discuss the impact of climate change on biodiversity and ecosystems.\n"
          ]
        }
      ],
      "source": [
        "response = llm.predict_and_call(\n",
        "    [vector_query_tool, summary_tool],\n",
        "    \"What is a summary of the paper?\",\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsuAfoQQUnXF"
      },
      "source": [
        "# Lesson 3: Building an Agent Reasoning Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbG-KyJPUt9C"
      },
      "source": [
        "## Setup Function Calling Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "sMKPrg-IUqDi"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "CrClHpeJUzav"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.agent.workflow import FunctionAgent\n",
        "\n",
        "agent = FunctionAgent(\n",
        "    tools=[vector_tool, summary_tool],\n",
        "    llm=llm,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cr-IbHktU1P6",
        "outputId": "ffa7c75a-9a06-492d-bf36-0ce625739cb0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:39:37,388 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:39:37,593 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:39:37,602 - INFO - Retrying request to /chat/completions in 0.453312 seconds\n",
            "2025-12-14 18:39:38,062 - INFO - Retrying request to /chat/completions in 0.885810 seconds\n",
            "2025-12-14 18:39:40,476 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:39:41,292 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In MetaGPT, the agent roles include Product Manager, Architect, Project Manager, Engineer, and QA Engineer. These roles are specialized for specific tasks in the software development process. Communication among these agents is facilitated through a shared message pool where structured messages are published and accessed. Agents can subscribe to relevant messages based on their profiles, enabling efficient exchange of information. This approach ensures that agents can retrieve necessary information from the shared pool without the need for direct inquiries, enhancing communication efficiency within the multi-agent system.\n"
          ]
        }
      ],
      "source": [
        "# For FunctionAgent - must use asyncio.run() for async execution\n",
        "import asyncio\n",
        "\n",
        "response = asyncio.run(agent.run(\n",
        "    \"Tell me about the agent roles in MetaGPT, and how they communicate.\"\n",
        "))\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJpxzzjfU37G",
        "outputId": "7ba8eaf8-60e1-4ac9-9661-1e61c78d83eb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:39:42,419 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:39:42,744 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:39:43,935 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:39:44,400 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The evaluation datasets used are HumanEval, MBPP, and SoftwareDev.\n"
          ]
        }
      ],
      "source": [
        "response = asyncio.run(agent.run(\n",
        "    \"Tell me about the evaluation datasets used.\"\n",
        "))\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOZowgzOU6Ie",
        "outputId": "3b5cea53-0019-4347-da55-653478abdccf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:39:44,915 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:39:45,704 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:39:46,854 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:39:47,610 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The results over the SoftwareDev dataset show that MetaGPT achieved an average score of 3.9, outperforming ChatDev's score of 2.1. Other general intelligent algorithms like AutoGPT scored 1.0, indicating a lower performance in generating executable code effectively. MetaGPT's systematic deconstruction of requirements and structured messaging and feedback mechanisms contribute to its strong performance in communication and code execution.\n"
          ]
        }
      ],
      "source": [
        "response = asyncio.run(agent.run(\"Tell me the results over one of the above datasets.\"))\n",
        "\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBRDERx3WiPM"
      },
      "source": [
        "# Lesson 4: Building a Multi-Document Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC-ZUdZqWlkY"
      },
      "source": [
        "## 1. Setup an Agent Over 3 Papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "9p_5UBCMWlAt"
      },
      "outputs": [],
      "source": [
        "urls = [\n",
        "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
        "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
        "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
        "]\n",
        "\n",
        "papers = [\n",
        "    \"metagpt.pdf\",\n",
        "    \"longlora.pdf\",\n",
        "    \"selfrag.pdf\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nCY_iZXWq3D",
        "outputId": "5cc631d1-1543-413f-fcc2-e7de21fde836"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-12-14 18:39:48--  https://openreview.net/pdf?id=VtmBAGCN7o\n",
            "Resolving openreview.net (openreview.net)... 34.57.44.88\n",
            "Connecting to openreview.net (openreview.net)|34.57.44.88|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16911937 (16M) [application/pdf]\n",
            "Saving to: data/metagpt.pdf\n",
            "\n",
            "data/metagpt.pdf    100%[===================>]  16.13M  25.4MB/s    in 0.6s    \n",
            "\n",
            "2025-12-14 18:39:49 (25.4 MB/s) - data/metagpt.pdf saved [16911937/16911937]\n",
            "\n",
            "--2025-12-14 18:39:49--  https://openreview.net/pdf?id=6PmJoRfdaK\n",
            "Resolving openreview.net (openreview.net)... 34.57.44.88\n",
            "Connecting to openreview.net (openreview.net)|34.57.44.88|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1168720 (1.1M) [application/pdf]\n",
            "Saving to: data/longlora.pdf\n",
            "\n",
            "data/longlora.pdf   100%[===================>]   1.11M  5.54MB/s    in 0.2s    \n",
            "\n",
            "2025-12-14 18:39:50 (5.54 MB/s) - data/longlora.pdf saved [1168720/1168720]\n",
            "\n",
            "--2025-12-14 18:39:50--  https://openreview.net/pdf?id=hSyW5go0v8\n",
            "Resolving openreview.net (openreview.net)... 34.57.44.88\n",
            "Connecting to openreview.net (openreview.net)|34.57.44.88|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1244749 (1.2M) [application/pdf]\n",
            "Saving to: data/selfrag.pdf\n",
            "\n",
            "data/selfrag.pdf    100%[===================>]   1.19M  5.57MB/s    in 0.2s    \n",
            "\n",
            "2025-12-14 18:39:50 (5.57 MB/s) - data/selfrag.pdf saved [1244749/1244749]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download papers\n",
        "for url, paper in zip(urls, papers):\n",
        "    !wget \"{url}\" -O \"data/{paper}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "W_PZDk3lWtET"
      },
      "outputs": [],
      "source": [
        "# Helper function to create tools for each paper\n",
        "# Works in both Google Colab and local environments\n",
        "from pathlib import Path\n",
        "from llama_index.core import SimpleDirectoryReader, SummaryIndex, VectorStoreIndex\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "def get_doc_tools(file_path: str, name: str):\n",
        "    \"\"\"\n",
        "    Get vector and summary query engine tools from a document.\n",
        "    \n",
        "    This function works in both Google Colab and local environments.\n",
        "    Make sure Settings.llm and Settings.embed_model are configured before calling this function.\n",
        "    \n",
        "    Args:\n",
        "        file_path: Path to the document file (relative or absolute path)\n",
        "        name: Name identifier for the document (used in tool descriptions)\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (vector_tool, summary_tool) - QueryEngineTool instances\n",
        "    \"\"\"\n",
        "\n",
        "    # Load documents\n",
        "    documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
        "    splitter = SentenceSplitter(chunk_size=1024)\n",
        "    nodes = splitter.get_nodes_from_documents(documents)\n",
        "\n",
        "    # Create indices\n",
        "    vector_index = VectorStoreIndex(nodes)\n",
        "    summary_index = SummaryIndex(nodes)\n",
        "\n",
        "    # Create query engines\n",
        "    vector_query_engine = vector_index.as_query_engine()\n",
        "    summary_query_engine = summary_index.as_query_engine(\n",
        "        response_mode=\"tree_summarize\",\n",
        "        use_async=True,\n",
        "    )\n",
        "\n",
        "    # Create tools with improved descriptions that include paper name variations\n",
        "    # Map common filename patterns to proper paper names\n",
        "    paper_name_map = {\n",
        "        \"selfrag\": \"Self-RAG\",\n",
        "        \"longlora\": \"LongLoRA\",\n",
        "        \"metagpt\": \"MetaGPT\",\n",
        "        \"loftq\": \"LoftQ\",\n",
        "    }\n",
        "    \n",
        "    # Get proper paper name or use the provided name\n",
        "    paper_name = paper_name_map.get(name.lower(), name.replace(\"_\", \"-\").title())\n",
        "    \n",
        "    # Create tools with explicit names and improved descriptions\n",
        "    vector_tool = QueryEngineTool.from_defaults(\n",
        "        query_engine=vector_query_engine,\n",
        "        name=f\"{paper_name}_vector_tool\",\n",
        "        description=(\n",
        "            f\"Use this tool to retrieve specific context, details, facts, or information from the {paper_name} paper. \"\n",
        "            f\"Use when asked about {paper_name}, {name}, or {name.replace('_', '-')}. \"\n",
        "            f\"This tool searches the {paper_name} paper for specific information.\"\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    summary_tool = QueryEngineTool.from_defaults(\n",
        "        query_engine=summary_query_engine,\n",
        "        name=f\"{paper_name}_summary_tool\",\n",
        "        description=(\n",
        "            f\"Use this tool to get summaries, overviews, or high-level information about the {paper_name} paper. \"\n",
        "            f\"Use when asked for a summary of {paper_name}, {name}, or {name.replace('_', '-')}. \"\n",
        "            f\"This tool provides comprehensive summaries of the {paper_name} paper.\"\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    return vector_tool, summary_tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHKWRYVGWunE",
        "outputId": "9f0019b4-e82d-43b5-8152-18de05818cc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting tools for paper: metagpt.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:39:52,249 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Successfully created tools for metagpt.pdf\n",
            "\n",
            "Getting tools for paper: longlora.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:39:53,555 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Successfully created tools for longlora.pdf\n",
            "\n",
            "Getting tools for paper: selfrag.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:39:55,041 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Successfully created tools for selfrag.pdf\n",
            "\n",
            "\n",
            " Successfully processed 3 papers\n",
            "Papers with tools: ['metagpt.pdf', 'longlora.pdf', 'selfrag.pdf']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "paper_to_tools_dict = {}\n",
        "for paper in papers:\n",
        "    print(f\"Getting tools for paper: {paper}\")\n",
        "    \n",
        "    # Check if file exists in data directory before processing\n",
        "    file_path = f\"data/{paper}\"\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"    Warning: File '{file_path}' does not exist. Skipping...\")\n",
        "        print(f\"   Tip: Make sure you've downloaded all papers first.\")\n",
        "        continue\n",
        "    \n",
        "    try:\n",
        "        vector_tool, summary_tool = get_doc_tools(file_path, Path(paper).stem)\n",
        "        paper_to_tools_dict[paper] = [vector_tool, summary_tool]\n",
        "        print(f\"   Successfully created tools for {paper}\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"   Error processing {paper}: {e}\")\n",
        "        print(f\"  Skipping this paper...\\n\")\n",
        "        continue\n",
        "\n",
        "print(f\"\\n Successfully processed {len(paper_to_tools_dict)} papers\")\n",
        "print(f\"Papers with tools: {list(paper_to_tools_dict.keys())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "xZRNhKIhWvcr"
      },
      "outputs": [],
      "source": [
        "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1CKgcR_WyBr",
        "outputId": "98d6d1e2-72fb-4b0c-e84a-0e1dc65c82e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of tools: 6\n",
            "\n",
            " Tools available:\n",
            "  Tool 1: MetaGPT_vector_tool\n",
            "    Description: Use this tool to retrieve specific context, details, facts, or information from ...\n",
            "  Tool 2: MetaGPT_summary_tool\n",
            "    Description: Use this tool to get summaries, overviews, or high-level information about the M...\n",
            "  Tool 3: LongLoRA_vector_tool\n",
            "    Description: Use this tool to retrieve specific context, details, facts, or information from ...\n",
            "  Tool 4: LongLoRA_summary_tool\n",
            "    Description: Use this tool to get summaries, overviews, or high-level information about the L...\n",
            "  Tool 5: Self-RAG_vector_tool\n",
            "    Description: Use this tool to retrieve specific context, details, facts, or information from ...\n",
            "  Tool 6: Self-RAG_summary_tool\n",
            "    Description: Use this tool to get summaries, overviews, or high-level information about the S...\n"
          ]
        }
      ],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "print(f\"Number of tools: {len(initial_tools)}\")\n",
        "\n",
        "# Debug: Print tool information\n",
        "if len(initial_tools) > 0:\n",
        "    print(f\"\\n Tools available:\")\n",
        "    for i, tool in enumerate(initial_tools[:6]):  # Show first 6 tools\n",
        "        # ToolMetadata is an object, not a dict - access attributes directly\n",
        "        if hasattr(tool, 'metadata') and tool.metadata:\n",
        "            tool_name = getattr(tool.metadata, 'name', 'Unknown')\n",
        "            tool_desc = getattr(tool.metadata, 'description', 'No description')\n",
        "        else:\n",
        "            tool_name = 'Unknown'\n",
        "            tool_desc = 'No description'\n",
        "        print(f\"  Tool {i+1}: {tool_name}\")\n",
        "        print(f\"    Description: {tool_desc[:80]}...\")\n",
        "else:\n",
        "    print(\"\\n WARNING: No tools available! Make sure papers were processed successfully.\")\n",
        "    print(f\"   paper_to_tools_dict has {len(paper_to_tools_dict)} papers\")\n",
        "    print(f\"   papers list: {papers}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "nrotN19WWzv8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Creating agent with 6 tools\n",
            "\n",
            " Available tools:\n",
            "   MetaGPT_vector_tool: Use this tool to retrieve specific context, details, facts, or information from ...\n",
            "   MetaGPT_summary_tool: Use this tool to get summaries, overviews, or high-level information about the M...\n",
            "   LongLoRA_vector_tool: Use this tool to retrieve specific context, details, facts, or information from ...\n",
            "   LongLoRA_summary_tool: Use this tool to get summaries, overviews, or high-level information about the L...\n",
            "   Self-RAG_vector_tool: Use this tool to retrieve specific context, details, facts, or information from ...\n",
            "   Self-RAG_summary_tool: Use this tool to get summaries, overviews, or high-level information about the S...\n",
            "\n",
            " Agent created successfully\n",
            " Tip: With verbose=True, you'll see detailed logs of tool selection and calls\n"
          ]
        }
      ],
      "source": [
        "# LlamaIndex >= 0.14.6\n",
        "from llama_index.core.agent.workflow import FunctionAgent\n",
        "\n",
        "if len(initial_tools) == 0:\n",
        "    raise ValueError(\n",
        "        \"ERROR: No tools available! Make sure papers were processed successfully.\\n\"\n",
        "        f\"paper_to_tools_dict has {len(paper_to_tools_dict)} entries.\\n\"\n",
        "        \"Run the paper processing cell above and check for errors.\"\n",
        "    )\n",
        "\n",
        "# Verify tools are properly configured and callable\n",
        "print(f\" Creating agent with {len(initial_tools)} tools\")\n",
        "tool_names = []\n",
        "tool_descriptions = []\n",
        "\n",
        "for tool in initial_tools:\n",
        "    # ToolMetadata is an object, access attributes directly\n",
        "    if hasattr(tool, 'metadata') and tool.metadata:\n",
        "        tool_name = getattr(tool.metadata, 'name', 'Unknown')\n",
        "        tool_desc = getattr(tool.metadata, 'description', 'No description')\n",
        "    else:\n",
        "        tool_name = 'Unknown'\n",
        "        tool_desc = 'No description'\n",
        "    \n",
        "    tool_names.append(tool_name)\n",
        "    tool_descriptions.append(f\"{tool_name}: {tool_desc[:80]}...\")\n",
        "    \n",
        "    # Verify tool has required methods for FunctionAgent\n",
        "    has_call = hasattr(tool, 'call') or hasattr(tool, 'acall') or callable(tool)\n",
        "    if not has_call:\n",
        "        print(f\"  Warning: Tool {tool_name} may not be callable - missing call methods\")\n",
        "\n",
        "print(f\"\\n Available tools:\")\n",
        "for desc in tool_descriptions:\n",
        "    print(f\"   {desc}\")\n",
        "\n",
        "# Build a comprehensive system prompt that lists all available tools with descriptions\n",
        "tool_list_with_desc = \"\\n\".join([f\"  - {name}: {desc[:100]}...\" for name, desc in zip(tool_names, [getattr(t.metadata, 'description', '') if hasattr(t, 'metadata') and t.metadata else '' for t in initial_tools])])\n",
        "\n",
        "# Create agent with explicit instructions and tool list\n",
        "# CRITICAL: FunctionAgent needs tools to be properly formatted QueryEngineTool instances\n",
        "agent = FunctionAgent(\n",
        "    tools=initial_tools,  # These should be QueryEngineTool instances from get_doc_tools\n",
        "    llm=llm,\n",
        "    verbose=True,  # Shows detailed tool selection and calling - IMPORTANT for debugging\n",
        "    system_prompt=(\n",
        "        \"You are a research assistant with access to tools for querying academic papers. \"\n",
        "        \"CRITICAL: You MUST use the available tools to answer ALL questions. \"\n",
        "        \"Never say you cannot retrieve information - always try the tools first.\\n\\n\"\n",
        "        f\"AVAILABLE TOOLS (you MUST use these):\\n{tool_list_with_desc}\\n\\n\"\n",
        "        \"TOOL SELECTION RULES:\\n\"\n",
        "        \"1. When asked about 'Self-RAG' or 'selfrag', you MUST call a tool with 'Self-RAG' in the name.\\n\"\n",
        "        \"2. When asked about 'LongLoRA' or 'longlora', you MUST call a tool with 'LongLoRA' in the name.\\n\"\n",
        "        \"3. When asked about 'MetaGPT' or 'metagpt', you MUST call a tool with 'MetaGPT' in the name.\\n\"\n",
        "        \"4. For summary requests (e.g., 'give me a summary', 'summarize'), use tools ending with '_summary_tool'.\\n\"\n",
        "        \"5. For specific details, facts, or questions, use tools ending with '_vector_tool'.\\n\\n\"\n",
        "        \"EXAMPLES OF CORRECT TOOL USAGE:\\n\"\n",
        "        \"- Query: 'Give me a summary of Self-RAG'  MUST call 'Self-RAG_summary_tool'\\n\"\n",
        "        \"- Query: 'Tell me about LongLoRA'  MUST call 'LongLoRA_summary_tool' or 'LongLoRA_vector_tool'\\n\"\n",
        "        \"- Query: 'What datasets does MetaGPT use?'  MUST call 'MetaGPT_vector_tool'\\n\"\n",
        "        \"- Query: 'Give me a summary of both Self-RAG and LongLoRA'  MUST call BOTH 'Self-RAG_summary_tool' AND 'LongLoRA_summary_tool'\\n\\n\"\n",
        "        \"MANDATORY BEHAVIOR:\\n\"\n",
        "        \"- ALWAYS call at least one tool before responding\\n\"\n",
        "        \"- If a tool call fails, try another tool for the same paper\\n\"\n",
        "        \"- NEVER respond without calling a tool first\\n\"\n",
        "        \"- NEVER say 'I cannot retrieve information' or 'I don't have access' - you have tools, use them!\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"\\n Agent created successfully\")\n",
        "print(\" Tip: With verbose=True, you'll see detailed logs of tool selection and calls\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "AVAILABLE TOOLS FOR AGENT:\n",
            "============================================================\n",
            "\n",
            "Tool 1: MetaGPT_vector_tool\n",
            "  Description: Use this tool to retrieve specific context, details, facts, or information from the MetaGPT paper. U...\n",
            "\n",
            "Tool 2: MetaGPT_summary_tool\n",
            "  Description: Use this tool to get summaries, overviews, or high-level information about the MetaGPT paper. Use wh...\n",
            "\n",
            "Tool 3: LongLoRA_vector_tool\n",
            "  Description: Use this tool to retrieve specific context, details, facts, or information from the LongLoRA paper. ...\n",
            "\n",
            "Tool 4: LongLoRA_summary_tool\n",
            "  Description: Use this tool to get summaries, overviews, or high-level information about the LongLoRA paper. Use w...\n",
            "\n",
            "Tool 5: Self-RAG_vector_tool\n",
            "  Description: Use this tool to retrieve specific context, details, facts, or information from the Self-RAG paper. ...\n",
            "\n",
            "Tool 6: Self-RAG_summary_tool\n",
            "  Description: Use this tool to get summaries, overviews, or high-level information about the Self-RAG paper. Use w...\n",
            "\n",
            " Total: 6 tools available\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Debug: Print all available tools with their names and descriptions\n",
        "print(\"=\"*60)\n",
        "print(\"AVAILABLE TOOLS FOR AGENT:\")\n",
        "print(\"=\"*60)\n",
        "if len(initial_tools) == 0:\n",
        "    print(\" ERROR: No tools available!\")\n",
        "    print(f\"   paper_to_tools_dict has {len(paper_to_tools_dict)} entries\")\n",
        "    print(f\"   papers list: {papers}\")\n",
        "    print(\"\\n Make sure you've:\")\n",
        "    print(\"   1. Downloaded all papers (run the download cell)\")\n",
        "    print(\"   2. Successfully processed papers (check for errors above)\")\n",
        "else:\n",
        "    for i, tool in enumerate(initial_tools, 1):\n",
        "        # ToolMetadata is an object, access attributes directly\n",
        "        if hasattr(tool, 'metadata') and tool.metadata:\n",
        "            tool_name = getattr(tool.metadata, 'name', 'Unknown')\n",
        "            tool_desc = getattr(tool.metadata, 'description', 'No description')\n",
        "        else:\n",
        "            tool_name = 'Unknown'\n",
        "            tool_desc = 'No description'\n",
        "        print(f\"\\nTool {i}: {tool_name}\")\n",
        "        print(f\"  Description: {tool_desc[:100]}...\")\n",
        "    print(f\"\\n Total: {len(initial_tools)} tools available\")\n",
        "    print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Testing tool callability:\n",
            "\n",
            "Testing tool: MetaGPT_summary_tool\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:39:56,690 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:39:57,675 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:39:59,829 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Tool is callable! Result preview: This paper introduces a meta-programming framework called MetaGPT that utilizes Standardized Operati...\n"
          ]
        }
      ],
      "source": [
        "# Test if tools are actually callable before using agent\n",
        "print(\" Testing tool callability:\\n\")\n",
        "if len(initial_tools) > 0:\n",
        "    # Test calling a summary tool directly\n",
        "    test_tool = None\n",
        "    for tool in initial_tools:\n",
        "        if 'summary' in str(tool.metadata.name).lower() if hasattr(tool, 'metadata') else '':\n",
        "            test_tool = tool\n",
        "            break\n",
        "    \n",
        "    if test_tool:\n",
        "        print(f\"Testing tool: {getattr(test_tool.metadata, 'name', 'Unknown')}\")\n",
        "        try:\n",
        "            # Try to call the tool directly\n",
        "            test_result = test_tool.call(\"What is this paper about?\")\n",
        "            print(f\" Tool is callable! Result preview: {str(test_result)[:100]}...\")\n",
        "        except Exception as e:\n",
        "            print(f\" Tool call failed: {e}\")\n",
        "            print(\"   This might be why the agent can't use the tools\")\n",
        "    else:\n",
        "        print(\"  Could not find a summary tool to test\")\n",
        "else:\n",
        "    print(\" No tools to test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " DEBUGGING: Verifying tool accessibility for FunctionAgent\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Tool 1: MetaGPT_vector_tool\n",
            "    has_metadata: True\n",
            "    has_call: True\n",
            "    has_acall: True\n",
            "    is_callable: True\n",
            "    has_fn: False\n",
            "    has_metadata_name: True\n",
            "   Tool description: Use this tool to retrieve specific context, details, facts, or information from ...\n",
            "\n",
            "Tool 2: MetaGPT_summary_tool\n",
            "    has_metadata: True\n",
            "    has_call: True\n",
            "    has_acall: True\n",
            "    is_callable: True\n",
            "    has_fn: False\n",
            "    has_metadata_name: True\n",
            "   Tool description: Use this tool to get summaries, overviews, or high-level information about the M...\n",
            "\n",
            "Tool 3: LongLoRA_vector_tool\n",
            "    has_metadata: True\n",
            "    has_call: True\n",
            "    has_acall: True\n",
            "    is_callable: True\n",
            "    has_fn: False\n",
            "    has_metadata_name: True\n",
            "   Tool description: Use this tool to retrieve specific context, details, facts, or information from ...\n",
            "\n",
            "======================================================================\n",
            " If tools don't have 'call' or 'acall', FunctionAgent might not be able to use them\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# CRITICAL DEBUG: Verify tools are actually accessible to the agent\n",
        "print(\" DEBUGGING: Verifying tool accessibility for FunctionAgent\\n\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Check if tools have the right interface for FunctionAgent\n",
        "for i, tool in enumerate(initial_tools[:3], 1):  # Check first 3 tools\n",
        "    tool_name = getattr(tool.metadata, 'name', 'Unknown') if hasattr(tool, 'metadata') and tool.metadata else 'Unknown'\n",
        "    print(f\"\\nTool {i}: {tool_name}\")\n",
        "    \n",
        "    # Check for required attributes/methods\n",
        "    checks = {\n",
        "        'has_metadata': hasattr(tool, 'metadata'),\n",
        "        'has_call': hasattr(tool, 'call'),\n",
        "        'has_acall': hasattr(tool, 'acall'),\n",
        "        'is_callable': callable(tool),\n",
        "        'has_fn': hasattr(tool, 'fn'),\n",
        "        'has_metadata_name': hasattr(tool, 'metadata') and hasattr(tool.metadata, 'name') if hasattr(tool, 'metadata') else False,\n",
        "    }\n",
        "    \n",
        "    for check_name, check_result in checks.items():\n",
        "        status = \"\" if check_result else \"\"\n",
        "        print(f\"   {status} {check_name}: {check_result}\")\n",
        "    \n",
        "    # Try to get the function signature that FunctionAgent would see\n",
        "    if hasattr(tool, 'metadata') and tool.metadata:\n",
        "        print(f\"   Tool description: {getattr(tool.metadata, 'description', 'N/A')[:80]}...\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\" If tools don't have 'call' or 'acall', FunctionAgent might not be able to use them\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:39:59,967 - INFO - Retrying request to /chat/completions in 0.498864 seconds\n",
            "2025-12-14 18:39:59,968 - INFO - Retrying request to /chat/completions in 0.499515 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Testing direct tool call (bypassing agent):\n",
            "\n",
            "Testing tool: Self-RAG_summary_tool\n",
            "Query: 'What is Self-RAG about?'\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:40:01,955 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:40:02,390 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:40:03,664 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Direct tool call SUCCESSFUL!\n",
            "Result preview: Self-RAG is a framework that enhances the quality and factuality of large language models by training a single arbitrary LM to adaptively retrieve passages, generate text informed by retrieved passage...\n",
            "\n",
            " If this works but agent doesn't, the issue is with FunctionAgent tool selection\n"
          ]
        }
      ],
      "source": [
        "# Test: Directly call a tool to verify it works\n",
        "print(\" Testing direct tool call (bypassing agent):\\n\")\n",
        "\n",
        "if len(initial_tools) > 0:\n",
        "    # Find Self-RAG summary tool\n",
        "    test_tool = None\n",
        "    for tool in initial_tools:\n",
        "        if hasattr(tool, 'metadata') and tool.metadata:\n",
        "            tool_name = getattr(tool.metadata, 'name', '')\n",
        "            if 'Self-RAG' in tool_name and 'summary' in tool_name:\n",
        "                test_tool = tool\n",
        "                break\n",
        "    \n",
        "    if test_tool:\n",
        "        tool_name = getattr(test_tool.metadata, 'name', 'Unknown')\n",
        "        print(f\"Testing tool: {tool_name}\")\n",
        "        print(f\"Query: 'What is Self-RAG about?'\\n\")\n",
        "        \n",
        "        try:\n",
        "            import asyncio\n",
        "            async def test_direct_call():\n",
        "                # Try different ways to call the tool\n",
        "                if hasattr(test_tool, 'acall'):\n",
        "                    result = await test_tool.acall(\"What is Self-RAG about?\")\n",
        "                elif hasattr(test_tool, 'call'):\n",
        "                    result = test_tool.call(\"What is Self-RAG about?\")\n",
        "                elif hasattr(test_tool, 'fn'):\n",
        "                    result = test_tool.fn(\"What is Self-RAG about?\")\n",
        "                else:\n",
        "                    raise AttributeError(\"Tool has no callable method\")\n",
        "                return result\n",
        "            \n",
        "            test_result = asyncio.run(test_direct_call())\n",
        "            print(f\" Direct tool call SUCCESSFUL!\")\n",
        "            print(f\"Result preview: {str(test_result)[:200]}...\")\n",
        "            print(\"\\n If this works but agent doesn't, the issue is with FunctionAgent tool selection\")\n",
        "        except Exception as e:\n",
        "            print(f\" Direct tool call FAILED: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            print(\"\\n This indicates a problem with the tool itself, not the agent\")\n",
        "    else:\n",
        "        print(\"  Could not find Self-RAG summary tool to test\")\n",
        "else:\n",
        "    print(\" No tools available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbx2Xa5YW6At",
        "outputId": "842b5dbf-8766-49b2-aedf-4c2d9121d0dd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:40:04,137 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:40:04,830 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:40:05,464 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:40:06,176 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:40:06,361 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:40:06,997 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:40:07,884 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The evaluation dataset used in LongLoRA is PG19. The evaluation results show that as the evaluation context length increases, the models achieve better perplexity.\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "\n",
        "response = asyncio.run(agent.run(\n",
        "    user_msg=(\n",
        "        \"Tell me about the evaluation dataset used in LongLoRA, \"\n",
        "        \"and then tell me about the evaluation results\"\n",
        "    )\n",
        "))\n",
        "print(str(response))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the Agent\n",
        "\n",
        "The agent is configured with all available tools. It should automatically select and use the appropriate tool based on your query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7YAEWvXW8X8",
        "outputId": "503250ca-3c48-4e77-db34-eacf144f378c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:40:09,854 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:40:11,915 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:40:12,457 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:40:12,602 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:40:12,609 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:40:13,653 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:40:13,962 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:40:14,543 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The SELF-RAG framework enhances large language models by incorporating retrieval on demand and self-reflection through special tokens called reflection tokens. It outperforms existing models in correctness, factuality, and fluency, with the ability to customize behaviors during inference. Ablation studies highlight the framework's key components, and its efficiency and accuracy trade-off can be adjusted by controlling retrieval frequency. The LongLoRA method extends the context length of large language models efficiently with minimal accuracy compromise by introducing Shifted Sparse Attention during training. It bridges the gap between low-rank adaptation and full fine-tuning, enabling the extension of LLMs like Llama2. Additionally, the framework for forgery detection focuses on modeling relations between facial action units using the Action Units Relation Transformer and Tampered AU Prediction components, achieving top performance in evaluations and enhancing model generalization.\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "\n",
        "response = asyncio.run(\n",
        "    agent.run(\n",
        "        user_msg=\"Give me a summary of both Self-RAG and LongLoRA\"\n",
        "    )\n",
        ")\n",
        "print(str(response))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jGd-H9zW95t"
      },
      "source": [
        "## 2. Setup an Agent Over 11 Papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Testing agent with a simple query:\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:40:16,125 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:40:17,916 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:40:17,925 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:40:19,284 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:40:19,728 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Agent response preview: The SELF-RAG framework enhances large language models by integrating retrieval on demand and self-reflection. It improves factuality and citation accuracy by training models to retrieve, generate, and critique text passages. SELF-RAG outperforms existing models on various tasks and allows customizat...\n",
            "\n",
            " If you see tool calls in the verbose output above, the agent is working!\n"
          ]
        }
      ],
      "source": [
        "# Test the agent with a simple query to verify it calls tools\n",
        "print(\" Testing agent with a simple query:\\n\")\n",
        "import asyncio\n",
        "\n",
        "try:\n",
        "    # Test with a simple, direct query\n",
        "    test_response = asyncio.run(agent.run(\n",
        "        user_msg=\"Give me a summary of Self-RAG\"\n",
        "    ))\n",
        "    print(f\"\\n Agent response preview: {str(test_response)[:300]}...\")\n",
        "    print(\"\\n If you see tool calls in the verbose output above, the agent is working!\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n Agent test failed: {type(e).__name__}: {e}\")\n",
        "    print(\"   Check the verbose output above to see what went wrong\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "HZTLBxFUXBX5"
      },
      "outputs": [],
      "source": [
        "urls = [\n",
        "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
        "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
        "    \"https://openreview.net/pdf?id=LzPWWPAdY4\",\n",
        "    \"https://openreview.net/pdf?id=VTF8yNQM66\",\n",
        "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
        "    \"https://openreview.net/pdf?id=9WD9KwssyT\",\n",
        "    \"https://openreview.net/pdf?id=yV6fD7LYkF\",\n",
        "    \"https://openreview.net/pdf?id=hnrB5YHoYu\",\n",
        "    \"https://openreview.net/pdf?id=WbWtOYIzIK\",\n",
        "    \"https://openreview.net/pdf?id=c5pwL0Soay\",\n",
        "    \"https://openreview.net/pdf?id=TpD2aG1h0D\"\n",
        "]\n",
        "\n",
        "papers = [\n",
        "    \"metagpt.pdf\",\n",
        "    \"longlora.pdf\",\n",
        "    \"loftq.pdf\",\n",
        "    \"swebench.pdf\",\n",
        "    \"selfrag.pdf\",\n",
        "    \"zipformer.pdf\",\n",
        "    \"values.pdf\",\n",
        "    \"finetune_fair_diffusion.pdf\",\n",
        "    \"knowledge_card.pdf\",\n",
        "    \"metra.pdf\",\n",
        "    \"vr_mcl.pdf\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCo2FY1jXESb",
        "outputId": "18c92fa6-bd24-4dd6-81d9-a7b97dff31ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-12-14 18:40:20--  https://openreview.net/pdf?id=VtmBAGCN7o\n",
            "Resolving openreview.net (openreview.net)... 34.57.44.88\n",
            "Connecting to openreview.net (openreview.net)|34.57.44.88|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16911937 (16M) [application/pdf]\n",
            "Saving to: data/metagpt.pdf\n",
            "\n",
            "data/metagpt.pdf    100%[===================>]  16.13M  11.2MB/s    in 1.4s    \n",
            "\n",
            "2025-12-14 18:40:23 (11.2 MB/s) - data/metagpt.pdf saved [16911937/16911937]\n",
            "\n",
            "--2025-12-14 18:40:23--  https://openreview.net/pdf?id=6PmJoRfdaK\n",
            "Resolving openreview.net (openreview.net)... 34.57.44.88\n",
            "Connecting to openreview.net (openreview.net)|34.57.44.88|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1168720 (1.1M) [application/pdf]\n",
            "Saving to: data/longlora.pdf\n",
            "\n",
            "data/longlora.pdf   100%[===================>]   1.11M  5.56MB/s    in 0.2s    \n",
            "\n",
            "2025-12-14 18:40:23 (5.56 MB/s) - data/longlora.pdf saved [1168720/1168720]\n",
            "\n",
            "--2025-12-14 18:40:24--  https://openreview.net/pdf?id=LzPWWPAdY4\n",
            "Resolving openreview.net (openreview.net)... 34.57.44.88\n",
            "Connecting to openreview.net (openreview.net)|34.57.44.88|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 366134 (358K) [application/pdf]\n",
            "Saving to: data/loftq.pdf\n",
            "\n",
            "data/loftq.pdf      100%[===================>] 357.55K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-12-14 18:40:24 (2.95 MB/s) - data/loftq.pdf saved [366134/366134]\n",
            "\n",
            "--2025-12-14 18:40:24--  https://openreview.net/pdf?id=VTF8yNQM66\n",
            "Resolving openreview.net (openreview.net)... 34.57.44.88\n",
            "Connecting to openreview.net (openreview.net)|34.57.44.88|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2680380 (2.6M) [application/pdf]\n",
            "Saving to: data/swebench.pdf\n",
            "\n",
            "data/swebench.pdf   100%[===================>]   2.56M  10.2MB/s    in 0.3s    \n",
            "\n",
            "2025-12-14 18:40:25 (10.2 MB/s) - data/swebench.pdf saved [2680380/2680380]\n",
            "\n",
            "--2025-12-14 18:40:25--  https://openreview.net/pdf?id=hSyW5go0v8\n",
            "Resolving openreview.net (openreview.net)... 34.57.44.88\n",
            "Connecting to openreview.net (openreview.net)|34.57.44.88|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1244749 (1.2M) [application/pdf]\n",
            "Saving to: data/selfrag.pdf\n",
            "\n",
            "data/selfrag.pdf    100%[===================>]   1.19M  5.77MB/s    in 0.2s    \n",
            "\n",
            "2025-12-14 18:40:26 (5.77 MB/s) - data/selfrag.pdf saved [1244749/1244749]\n",
            "\n",
            "--2025-12-14 18:40:26--  https://openreview.net/pdf?id=9WD9KwssyT\n",
            "Resolving openreview.net (openreview.net)... 34.57.44.88\n",
            "Connecting to openreview.net (openreview.net)|34.57.44.88|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 511626 (500K) [application/pdf]\n",
            "Saving to: data/zipformer.pdf\n",
            "\n",
            "data/zipformer.pdf  100%[===================>] 499.63K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-12-14 18:40:26 (3.28 MB/s) - data/zipformer.pdf saved [511626/511626]\n",
            "\n",
            "--2025-12-14 18:40:26--  https://openreview.net/pdf?id=yV6fD7LYkF\n",
            "Resolving openreview.net (openreview.net)... 34.57.44.88\n",
            "Connecting to openreview.net (openreview.net)|34.57.44.88|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4171982 (4.0M) [application/pdf]\n",
            "Saving to: data/values.pdf\n",
            "\n",
            "data/values.pdf     100%[===================>]   3.98M  12.8MB/s    in 0.3s    \n",
            "\n",
            "2025-12-14 18:40:27 (12.8 MB/s) - data/values.pdf saved [4171982/4171982]\n",
            "\n",
            "--2025-12-14 18:40:27--  https://openreview.net/pdf?id=hnrB5YHoYu\n",
            "Resolving openreview.net (openreview.net)... 34.57.44.88\n",
            "Connecting to openreview.net (openreview.net)|34.57.44.88|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 34710410 (33M) [application/pdf]\n",
            "Saving to: data/finetune_fair_diffusion.pdf\n",
            "\n",
            "data/finetune_fair_ 100%[===================>]  33.10M  35.9MB/s    in 0.9s    \n",
            "\n",
            "2025-12-14 18:40:29 (35.9 MB/s) - data/finetune_fair_diffusion.pdf saved [34710410/34710410]\n",
            "\n",
            "--2025-12-14 18:40:29--  https://openreview.net/pdf?id=WbWtOYIzIK\n",
            "Resolving openreview.net (openreview.net)... 34.57.44.88\n",
            "Connecting to openreview.net (openreview.net)|34.57.44.88|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 877083 (857K) [application/pdf]\n",
            "Saving to: data/knowledge_card.pdf\n",
            "\n",
            "data/knowledge_card 100%[===================>] 856.53K  5.04MB/s    in 0.2s    \n",
            "\n",
            "2025-12-14 18:40:29 (5.04 MB/s) - data/knowledge_card.pdf saved [877083/877083]\n",
            "\n",
            "--2025-12-14 18:40:30--  https://openreview.net/pdf?id=c5pwL0Soay\n",
            "Resolving openreview.net (openreview.net)... 34.57.44.88\n",
            "Connecting to openreview.net (openreview.net)|34.57.44.88|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4775879 (4.6M) [application/pdf]\n",
            "Saving to: data/metra.pdf\n",
            "\n",
            "data/metra.pdf      100%[===================>]   4.55M  16.1MB/s    in 0.3s    \n",
            "\n",
            "2025-12-14 18:40:30 (16.1 MB/s) - data/metra.pdf saved [4775879/4775879]\n",
            "\n",
            "--2025-12-14 18:40:30--  https://openreview.net/pdf?id=TpD2aG1h0D\n",
            "Resolving openreview.net (openreview.net)... 34.57.44.88\n",
            "Connecting to openreview.net (openreview.net)|34.57.44.88|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1973959 (1.9M) [application/pdf]\n",
            "Saving to: data/vr_mcl.pdf\n",
            "\n",
            "data/vr_mcl.pdf     100%[===================>]   1.88M  7.52MB/s    in 0.3s    \n",
            "\n",
            "2025-12-14 18:40:31 (7.52 MB/s) - data/vr_mcl.pdf saved [1973959/1973959]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download all papers\n",
        "for url, paper in zip(urls, papers):\n",
        "    !wget \"{url}\" -O \"data/{paper}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2YQ6MCLXGjc",
        "outputId": "4ff70cf5-19ee-42fb-8b6f-f45e0b495097"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting tools for paper: metagpt.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:40:32,500 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Successfully created tools for metagpt.pdf\n",
            "Getting tools for paper: longlora.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:40:33,459 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Successfully created tools for longlora.pdf\n",
            "Getting tools for paper: loftq.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:40:34,033 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Successfully created tools for loftq.pdf\n",
            "Getting tools for paper: swebench.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:40:36,270 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Successfully created tools for swebench.pdf\n",
            "Getting tools for paper: selfrag.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:40:38,025 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Successfully created tools for selfrag.pdf\n",
            "Getting tools for paper: zipformer.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:40:38,540 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Successfully created tools for zipformer.pdf\n",
            "Getting tools for paper: values.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:40:39,801 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Successfully created tools for values.pdf\n",
            "Getting tools for paper: finetune_fair_diffusion.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:40:43,808 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Successfully created tools for finetune_fair_diffusion.pdf\n",
            "Getting tools for paper: knowledge_card.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:40:45,049 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Successfully created tools for knowledge_card.pdf\n",
            "Getting tools for paper: metra.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:40:46,084 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Successfully created tools for metra.pdf\n",
            "Getting tools for paper: vr_mcl.pdf\n",
            " Unicode error while processing vr_mcl.pdf: 'utf-8' codec can't encode character '\\ud835' in position 94329: surrogates not allowed\n",
            "Saved cleaned version: data/vr_mcl_clean.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:40:49,388 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:40:50,499 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:40:51,404 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:40:52,232 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:40:53,902 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:40:54,817 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:40:56,037 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:40:57,057 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:40:58,092 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:40:59,301 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:00,521 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:01,645 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:02,628 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:03,539 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:04,425 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:05,429 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:06,390 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:08,025 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:09,052 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:09,590 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Retried successfully for vr_mcl.pdf\n",
            "\n",
            " Successfully processed 11 papers\n",
            "Papers with tools: ['metagpt.pdf', 'longlora.pdf', 'loftq.pdf', 'swebench.pdf', 'selfrag.pdf', 'zipformer.pdf', 'values.pdf', 'finetune_fair_diffusion.pdf', 'knowledge_card.pdf', 'metra.pdf', 'vr_mcl.pdf']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "paper_to_tools_dict = {}\n",
        "\n",
        "for paper in papers:\n",
        "    print(f\"Getting tools for paper: {paper}\")\n",
        "    \n",
        "    # Check if file exists in data directory before processing\n",
        "    file_path = f\"data/{paper}\"\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"    Warning: File '{file_path}' does not exist. Skipping...\")\n",
        "        print(f\"   Tip: Make sure you've downloaded all papers first.\")\n",
        "        continue\n",
        "    \n",
        "    try:\n",
        "        vector_tool, summary_tool = get_doc_tools(file_path, Path(paper).stem)\n",
        "        paper_to_tools_dict[paper] = [vector_tool, summary_tool]\n",
        "        print(f\"   Successfully created tools for {paper}\")\n",
        "\n",
        "    except UnicodeEncodeError as e:\n",
        "        print(f\" Unicode error while processing {paper}: {e}\")\n",
        "        try:\n",
        "            # Attempt to re-read text safely and re-generate tools\n",
        "            text_bytes = Path(file_path).read_bytes()\n",
        "            safe_text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n",
        "\n",
        "            # Optionally, save the cleaned text for inspection\n",
        "            clean_path = Path(file_path).with_name(Path(file_path).stem + \"_clean.txt\")\n",
        "            clean_path.write_text(safe_text, encoding=\"utf-8\")\n",
        "            print(f\"Saved cleaned version: {clean_path}\")\n",
        "\n",
        "            # Retry tool creation if your get_doc_tools can accept a string path\n",
        "            vector_tool, summary_tool = get_doc_tools(str(clean_path), Path(paper).stem)\n",
        "            paper_to_tools_dict[paper] = [vector_tool, summary_tool]\n",
        "            print(f\" Retried successfully for {paper}\")\n",
        "\n",
        "        except Exception as inner_e:\n",
        "            print(f\" Still failed on {paper}: {inner_e}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Unexpected error for {paper}: {e}\")\n",
        "\n",
        "print(f\"\\n Successfully processed {len(paper_to_tools_dict)} papers\")\n",
        "print(f\"Papers with tools: {list(paper_to_tools_dict.keys())}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ4I4TvuXJ5V"
      },
      "source": [
        "## Extend the Agent with Tool Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "0ydbcE-RXMi7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total tools created: 22\n"
          ]
        }
      ],
      "source": [
        "# FIXED: Only iterate over papers that were successfully processed\n",
        "all_tools = [t for paper in paper_to_tools_dict.keys() for t in paper_to_tools_dict[paper]]\n",
        "print(f\"Total tools created: {len(all_tools)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "3zTX2YhlXOlz"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:41:11,690 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        }
      ],
      "source": [
        "# Define an \"object\" index and retriever over these tools\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.objects import ObjectIndex\n",
        "\n",
        "obj_index = ObjectIndex.from_objects(\n",
        "    all_tools,\n",
        "    index_cls=VectorStoreIndex,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "RD46kQ4aXRAm"
      },
      "outputs": [],
      "source": [
        "obj_retriever = obj_index.as_retriever(similarity_top_k=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EJybydrXSzx",
        "outputId": "8f4b6950-a3cd-465b-bc72-770fd4b3ea55"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:41:11,964 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ToolMetadata(description='Use this tool to get summaries, overviews, or high-level information about the Swebench paper. Use when asked for a summary of Swebench, swebench, or swebench. This tool provides comprehensive summaries of the Swebench paper.', name='Swebench_summary_tool', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tools = obj_retriever.retrieve(\n",
        "    \"Tell me about the eval dataset used in MetaGPT and SWE-Bench\"\n",
        ")\n",
        "tools[2].metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "JjX1Z260XVCu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Agent created with tool_retriever (can retrieve from 22 tools)\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.agent.workflow import FunctionAgent\n",
        "\n",
        "# FIXED: Use tool_retriever parameter instead of RetrieverTool\n",
        "# The tool_retriever dynamically retrieves relevant tools based on the query\n",
        "# This allows scaling to many documents without overwhelming the LLM context\n",
        "\n",
        "try:\n",
        "    # Option 1: Use tool_retriever (preferred for LlamaIndex >= 0.10.30)\n",
        "    agent = FunctionAgent(\n",
        "        tools=[],  # Empty - tools are retrieved dynamically via tool_retriever\n",
        "        tool_retriever=obj_retriever,  # Retrieves top-k relevant tools per query\n",
        "        llm=llm,\n",
        "        system_prompt=(\n",
        "            \"You are an agent designed to answer queries over a set of given papers. \"\n",
        "            \"Always use the provided tools to answer a question. Do not rely on prior knowledge.\"\n",
        "        ),\n",
        "        verbose=True,\n",
        "    )\n",
        "    print(f\" Agent created with tool_retriever (can retrieve from {len(all_tools)} tools)\")\n",
        "except TypeError as e:\n",
        "    if 'tool_retriever' in str(e):\n",
        "        # Option 2: Fallback - pass all tools directly (works in all versions)\n",
        "        print(\"  tool_retriever not supported, falling back to passing all tools directly\")\n",
        "        agent = FunctionAgent(\n",
        "            tools=all_tools,  # Pass all tools directly\n",
        "            llm=llm,\n",
        "            system_prompt=(\n",
        "                \"You are an agent designed to answer queries over a set of given papers. \"\n",
        "                \"Always use the provided tools to answer a question. Do not rely on prior knowledge.\"\n",
        "            ),\n",
        "            verbose=True,\n",
        "        )\n",
        "        print(f\" Agent created with {len(all_tools)} tools directly\")\n",
        "    else:\n",
        "        raise e\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pwRmYl81ZLD",
        "outputId": "f1016d2a-5c89-499a-e56a-05ae95ef6eaf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:41:12,165 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:12,922 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:13,072 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:14,455 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:14,602 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:15,694 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:16,324 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:16,632 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:16,898 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:18,182 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:18,339 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:18,697 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The SELF-RAG framework enhances the quality and factuality of large language models by incorporating retrieval on demand and self-reflection. It utilizes reflection tokens to retrieve, generate, and critique text passages, allowing for customized behaviors at test time through parameter adjustments. Experimental results show that SELF-RAG outperforms other models on various tasks, improving model performance, factuality, and citation accuracy. The model evaluates the relevance, supportiveness, and usefulness of generated text based on given instructions and evidence, ensuring factual support and meeting information needs.\n",
            "\n",
            "The LongLoRA method extends the context length of large language models efficiently with minimal accuracy compromise. It introduces S2-Attn to approximate standard self-attention patterns during training. By fine-tuning LLMs with LongLoRA, models can achieve extended context lengths, such as up to 100k for 7B models and 32k for 70B models. LongLoRA is effective in supervised fine-tuning with the LongAlpaca dataset. Additionally, a framework focusing on modeling relations between facial action units for forgery detection utilizes the Action Units Relation Transformer (ART) and Tampered AU Prediction (TAP) components, achieving state-of-the-art performance and enhancing model generalization. Another work introduces the SAFECONV dataset for conversational safety research, annotating unsafe spans in dialogues and offering safe alternatives to mitigate unsafe behavior in chatbots.\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "\n",
        "response = asyncio.run(\n",
        "    agent.run(\n",
        "        user_msg=\"Give me a summary of both Self-RAG and LongLoRA\"\n",
        "    )\n",
        ")\n",
        "print(str(response))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KBGrr4lXXF1",
        "outputId": "8d4ca211-9c41-42ff-a700-4bd4e486b88a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:41:21,142 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:21,954 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:22,170 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:22,348 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:22,484 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:23,760 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:23,902 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:24,342 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:24,737 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:26,255 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:26,643 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:27,046 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The evaluation dataset used in MetaGPT is the SoftwareDev dataset, which consists of 70 diverse software development tasks covering scopes like mini-games, image processing algorithms, and data visualization. It includes task prompts for each task and is used for evaluation through human assessments or statistical analysis based on metrics like Executability, Cost, Code Statistics, Productivity, and Human Revision Cost.\n",
            "\n",
            "In comparison, SWE-Bench's evaluation dataset comprises task instances extracted from various open-source repositories, each with a problem statement, codebase, and associated patches. These task instances are used to evaluate models in generating code patches to resolve the described issues. The dataset includes task instances from repositories like scikit-learn, xarray, requests, django, and sphinx-doc, covering issues related to code functionality, bug fixes, and enhancements. It serves as a benchmark to assess models' success in addressing software engineering tasks by generating code patches.\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "\n",
        "response = asyncio.run(\n",
        "    agent.run(\n",
        "        user_msg=(\n",
        "            \"Tell me about the evaluation dataset used \"\n",
        "            \"in MetaGPT and compare it against SWE-Bench.\"\n",
        "        )\n",
        "    )\n",
        ")\n",
        "print(str(response))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzhrevyqXaBl",
        "outputId": "a4a00de8-b201-4541-aaf0-5f1d87490a05"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:41:29,032 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:29,757 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:29,961 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:30,059 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:31,183 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:32,094 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:32,500 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:32,642 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:33,415 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:33,928 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:34,140 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:35,020 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The LongLoRA paper introduces a method that extends the context length of large language models with minimal accuracy compromise by using shifted sparse attention during training. It involves fine-tuning pre-trained models with LongLoRA to achieve extended context lengths with reduced GPU memory cost and training time. The approach includes trainable normalization and embedding layers to bridge the gap between low-rank adaptation and full fine-tuning.\n",
            "\n",
            "On the other hand, the LoftQ paper presents a quantization framework for Large Language Models (LLMs) that combines quantization and low-rank approximation to approximate high-precision pre-trained weights. It offers a beneficial initialization for subsequent Low-Rank Adaptation (LoRA) fine-tuning, leading to improved generalization in downstream tasks. LoftQ outperforms existing quantization methods, especially in challenging low-bit scenarios, across various natural language processing tasks. It achieves close to full-finetuning performance with 4-bit quantization and can be extended to convolutional layers using low-rank adapters. Additionally, LoftQ surpasses pruning methods in terms of memory savings during training and storage stages.\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "\n",
        "response = asyncio.run(\n",
        "    agent.run(\n",
        "        user_msg=(\n",
        "            \"Compare and contrast the LoRA papers (LongLoRA, LoftQ). \"\n",
        "            \"Analyze the approach in each paper first.\"\n",
        "        )\n",
        "    )\n",
        ")\n",
        "print(str(response))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "#  Extensions & Modifications Section\n",
        "\n",
        "This section demonstrates understanding of Agentic RAG concepts through:\n",
        "1. Documentation of bug fixes applied\n",
        "2. Custom query extensions\n",
        "3. Performance analysis\n",
        "4. Key learnings and reflections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Bug Fixes Applied\n",
        "\n",
        "### Bug 1: KeyError in `all_tools` List Comprehension\n",
        "\n",
        "**Problem:** The original code iterated over all papers in the `papers` list, regardless of whether they were successfully processed:\n",
        "\n",
        "```python\n",
        "#  BROKEN: Crashes if any paper failed to process\n",
        "all_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]\n",
        "```\n",
        "\n",
        "**Root Cause:** If a paper download failed or processing encountered an error, it wouldn't be added to `paper_to_tools_dict`, causing a `KeyError`.\n",
        "\n",
        "**Fix Applied:**\n",
        "```python\n",
        "#  FIXED: Only iterate over successfully processed papers\n",
        "all_tools = [t for paper in paper_to_tools_dict.keys() for t in paper_to_tools_dict[paper]]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Bug 2: Incorrect Agent Architecture with RetrieverTool\n",
        "\n",
        "**Problem:** The original code wrapped `obj_retriever` in a `RetrieverTool`:\n",
        "\n",
        "```python\n",
        "#  BROKEN: Agent can retrieve tools but cannot CALL them\n",
        "retriever_tool = RetrieverTool.from_defaults(retriever=obj_retriever, ...)\n",
        "agent = FunctionAgent(tools=[retriever_tool], ...)\n",
        "```\n",
        "\n",
        "**Root Cause:** `ObjectIndex` stores tool objects, and `obj_retriever.retrieve()` returns tools. However, wrapping this in `RetrieverTool` means the agent only has one tool (the retriever) - it cannot actually invoke the retrieved document tools!\n",
        "\n",
        "**Fix Applied:**\n",
        "```python\n",
        "#  FIXED: Use tool_retriever parameter for dynamic tool selection\n",
        "agent = FunctionAgent(\n",
        "    tools=[],  # Empty - tools retrieved dynamically\n",
        "    tool_retriever=obj_retriever,  # Agent retrieves AND calls relevant tools\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        ")\n",
        "```\n",
        "\n",
        "**Why This Works:** The `tool_retriever` parameter tells `FunctionAgent` to:\n",
        "1. Use the retriever to find relevant tools based on the query\n",
        "2. Make those tools available to the LLM for calling\n",
        "3. Execute the selected tools and return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Extension 1: Cross-Paper Comparison Query\n",
        "\n",
        "Testing the agent's ability to synthesize information across multiple papers on different topics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:41:36,292 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "EXTENSION 1: Cross-Paper Comparison\n",
            "======================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:41:37,237 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:37,461 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:37,551 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:38,545 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:39,455 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:39,769 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:40,388 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:41,214 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:41,686 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:41,885 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:42,220 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " RESPONSE:\n",
            "MetaGPT enhances Language Model (LLM) outputs by incorporating efficient human workflows into multi-agent collaborations, introducing Standardized Operating Procedures (SOPs) to streamline workflows, assigning diverse roles to various agents, and implementing an executive feedback mechanism that debugs and executes code during runtime. These innovations significantly improve code generation quality and transform abstract requirements into detailed designs through granular tasks like requirement analysis and package selection. MetaGPT also utilizes a global message pool and subscription mechanism to streamline communication and filter out irrelevant information, addressing the challenge of information overload. These innovations collectively lead to state-of-the-art performance on various benchmarks and enhance the quality and efficiency of LLM outputs in software development tasks.\n",
            "\n",
            "On the other hand, Self-RAG improves LLM outputs by incorporating retrieval on demand and self-reflection, leading to enhanced quality and factuality of generated text. It enables the model to adaptively retrieve passages, generate text informed by these passages, and critique its own output using reflection tokens. This approach provides precise control over the generation process, resulting in improved performance across various tasks. Self-RAG's key innovations include dynamic decision-making on when to retrieve text passages, the ability to enforce constraints during decoding, and the customization of model behaviors by adjusting parameters at test time. Additionally, Self-RAG introduces a method for training an LM to use retrieval on-demand for diverse instruction-following queries and controlled generation guided by reflection tokens, resulting in significant performance enhancements over existing approaches.\n",
            "\n",
            "In summary, MetaGPT focuses on streamlining workflows, improving code generation quality, and transforming abstract requirements into detailed designs, while Self-RAG emphasizes retrieval on demand, self-reflection, and precise control over the generation process to enhance the quality and factuality of generated text.\n"
          ]
        }
      ],
      "source": [
        "# Extension 1: Complex cross-paper analysis\n",
        "import asyncio\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"EXTENSION 1: Cross-Paper Comparison\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "response = asyncio.run(agent.run(\n",
        "    user_msg=(\n",
        "        \"Compare how MetaGPT and Self-RAG each improve LLM outputs. \"\n",
        "        \"What are their key innovations and how do they differ in approach?\"\n",
        "    )\n",
        "))\n",
        "\n",
        "print(\"\\n RESPONSE:\")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Extension 2: Efficiency-Focused Query\n",
        "\n",
        "Testing the agent's ability to identify and compare papers focused on efficiency improvements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "EXTENSION 2: Efficiency Analysis Across Papers\n",
            "======================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:41:44,584 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:45,332 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:45,472 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:45,573 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:46,326 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:46,454 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:46,834 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:47,448 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:47,964 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:48,212 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:48,431 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:48,823 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " RESPONSE:\n",
            "The efficiency techniques used in LongLoRA include Flash-Attention2, DeepSpeed, Gradient checkpoint, and S2-Attn. These techniques have been instrumental in optimizing training hours and memory usage for models like Llama2, enhancing overall efficiency in deep learning tasks.\n",
            "\n",
            "On the other hand, LoftQ focuses on efficiency techniques such as quantization methods to achieve performance close to full finetuning with reduced memory requirements. Additionally, low-rank adapters applied to convolutional layers help optimize computational efficiency by approximating convolutional operations with low-rank matrices. These techniques aim to improve efficiency in training and storage while maintaining performance levels.\n"
          ]
        }
      ],
      "source": [
        "# Extension 2: Efficiency-focused analysis across papers\n",
        "print(\"=\"*70)\n",
        "print(\"EXTENSION 2: Efficiency Analysis Across Papers\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "response = asyncio.run(agent.run(\n",
        "    user_msg=(\n",
        "        \"Which papers focus on making models more efficient? \"\n",
        "        \"Compare the efficiency techniques used in LongLoRA and LoftQ.\"\n",
        "    )\n",
        "))\n",
        "\n",
        "print(\"\\n RESPONSE:\")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Extension 3: Specific Technical Detail Query\n",
        "\n",
        "Testing the vector tool's ability to retrieve specific technical details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "EXTENSION 3: Technical Detail Retrieval\n",
            "======================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:41:50,146 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:50,990 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:51,214 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:51,446 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:52,778 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:52,982 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:53,975 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " RESPONSE:\n",
            "MetaGPT uses the following metrics and benchmarks to evaluate code generation quality:\n",
            "\n",
            "1. Executability\n",
            "2. Cost (including running time, token usage, and expenses)\n",
            "3. Code Statistics (such as code files, lines of code per file, and total code lines)\n",
            "4. Productivity (calculated as token usage divided by lines of code)\n",
            "5. Human Revision Cost\n",
            "\n",
            "The benchmarks used for evaluation are:\n",
            "1. SoftwareDev\n",
            "2. HumanEval\n",
            "3. MBPP\n",
            "\n",
            "These benchmarks focus on different aspects of code generation quality assessment.\n"
          ]
        }
      ],
      "source": [
        "# Extension 3: Specific technical detail retrieval\n",
        "print(\"=\"*70)\n",
        "print(\"EXTENSION 3: Technical Detail Retrieval\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "response = asyncio.run(agent.run(\n",
        "    user_msg=(\n",
        "        \"What specific metrics and benchmarks does MetaGPT use to evaluate \"\n",
        "        \"code generation quality? Provide specific numbers if available.\"\n",
        "    )\n",
        "))\n",
        "\n",
        "print(\"\\n RESPONSE:\")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Extension 4: Multi-Paper Summary with Categorization\n",
        "\n",
        "Testing the agent's ability to categorize and summarize multiple papers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "EXTENSION 4: Multi-Paper Categorization\n",
            "======================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:41:55,076 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:56,377 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:56,643 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:56,649 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:56,818 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:56,818 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:57,516 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:58,102 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:58,410 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:58,416 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:58,629 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:59,125 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:59,181 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:59,453 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:59,598 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:59,785 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:41:59,908 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:42:00,194 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:42:00,254 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:42:01,156 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:42:01,482 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:42:02,541 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:42:02,743 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-12-14 18:42:03,582 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " RESPONSE:\n",
            "Based on the summaries provided:\n",
            "\n",
            "- **MetaGPT**: Focuses on code generation and enhancing the problem-solving capabilities of multi-agent systems through natural language programming.\n",
            "- **Self-RAG**: Focuses on enabling engineers to program using natural language and enhancing communication between multiple agents.\n",
            "- **LongLoRA**: Focuses on an efficient fine-tuning approach that extends the context length of large language models with minimal accuracy compromise.\n",
            "- **LoftQ**: No specific information available to categorize.\n",
            "- **SWE-Bench**: Focuses on benchmarking and providing a comprehensive dataset for diverse software development tasks.\n"
          ]
        }
      ],
      "source": [
        "# Extension 4: Categorize papers by their main contribution\n",
        "print(\"=\"*70)\n",
        "print(\"EXTENSION 4: Multi-Paper Categorization\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "response = asyncio.run(agent.run(\n",
        "    user_msg=(\n",
        "        \"Categorize the following papers by their main focus area: \"\n",
        "        \"MetaGPT, Self-RAG, LongLoRA, LoftQ, and SWE-Bench. \"\n",
        "        \"Group them into categories like 'Code Generation', 'Retrieval', \"\n",
        "        \"'Efficiency', 'Benchmarking', etc.\"\n",
        "    )\n",
        "))\n",
        "\n",
        "print(\"\\n RESPONSE:\")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Performance Analysis\n",
        "\n",
        "Analyzing the tool retrieval and agent performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "PERFORMANCE ANALYSIS: Tool Retrieval Accuracy\n",
            "======================================================================\n",
            "\n",
            "Testing tool retrieval for specific queries:\n",
            "\n",
            "Query: 'What is Self-RAG?'\n",
            "Expected tools from: ['Self-RAG']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:42:04,872 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved tools: ['Self-RAG_vector_tool', 'Self-RAG_summary_tool', 'LongLoRA_vector_tool']\n",
            " Found 1/1 expected papers\n",
            "--------------------------------------------------\n",
            "Query: 'Explain LongLoRA's approach'\n",
            "Expected tools from: ['LongLoRA']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:42:05,084 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved tools: ['LongLoRA_summary_tool', 'LongLoRA_vector_tool', 'LoftQ_summary_tool']\n",
            " Found 1/1 expected papers\n",
            "--------------------------------------------------\n",
            "Query: 'Compare MetaGPT and SWE-Bench'\n",
            "Expected tools from: ['MetaGPT', 'Swebench']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 18:42:05,306 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved tools: ['MetaGPT_summary_tool', 'MetaGPT_vector_tool', 'Swebench_summary_tool']\n",
            " Found 2/2 expected papers\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Performance Analysis: Measure tool retrieval accuracy\n",
        "import time\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"PERFORMANCE ANALYSIS: Tool Retrieval Accuracy\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "test_queries = [\n",
        "    (\"What is Self-RAG?\", [\"Self-RAG\"]),\n",
        "    (\"Explain LongLoRA's approach\", [\"LongLoRA\"]),\n",
        "    (\"Compare MetaGPT and SWE-Bench\", [\"MetaGPT\", \"Swebench\"]),\n",
        "]\n",
        "\n",
        "print(\"\\nTesting tool retrieval for specific queries:\\n\")\n",
        "\n",
        "for query, expected_papers in test_queries:\n",
        "    print(f\"Query: '{query}'\")\n",
        "    print(f\"Expected tools from: {expected_papers}\")\n",
        "    \n",
        "    # Retrieve tools\n",
        "    retrieved = obj_retriever.retrieve(query)\n",
        "    retrieved_names = [t.metadata.name if hasattr(t, 'metadata') else str(t) for t in retrieved]\n",
        "    \n",
        "    print(f\"Retrieved tools: {retrieved_names}\")\n",
        "    \n",
        "    # Check if expected papers are in retrieved tools\n",
        "    found = [p for p in expected_papers if any(p.lower() in name.lower() for name in retrieved_names)]\n",
        "    print(f\" Found {len(found)}/{len(expected_papers)} expected papers\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Key Learnings & Reflections\n",
        "\n",
        "### 1. Router Engine Pattern\n",
        "The Router Engine intelligently routes queries to the appropriate tool based on query intent:\n",
        "- **Summary queries**  `summary_tool` (uses tree summarization)\n",
        "- **Specific detail queries**  `vector_tool` (uses semantic search)\n",
        "\n",
        "This is more efficient than always using both tools, as it reduces API calls and latency.\n",
        "\n",
        "### 2. Tool Calling with QueryEngineTool\n",
        "`QueryEngineTool` wraps LlamaIndex query engines to make them callable by agents:\n",
        "- Provides `metadata` (name, description) for LLM tool selection\n",
        "- Implements `call()` and `acall()` methods for sync/async execution\n",
        "- Tool descriptions are **critical** - they guide the LLM's tool selection\n",
        "\n",
        "### 3. Agent Reasoning Loop\n",
        "`FunctionAgent` implements a ReAct-style reasoning loop:\n",
        "1. **Observe** - Receive user query\n",
        "2. **Think** - Decide which tool(s) to use\n",
        "3. **Act** - Call the selected tool(s)\n",
        "4. **Repeat** - Continue until the query is answered\n",
        "\n",
        "Setting `verbose=True` is essential for debugging tool selection issues.\n",
        "\n",
        "### 4. Scaling with ObjectIndex + tool_retriever\n",
        "For many documents, passing all tools to the agent overwhelms the LLM context. Solution:\n",
        "- **ObjectIndex** - Stores tools as retrievable objects\n",
        "- **tool_retriever** - Dynamically retrieves top-k relevant tools per query\n",
        "- This enables scaling to 100s of documents while keeping context manageable\n",
        "\n",
        "### 5. Common Pitfalls Discovered\n",
        "| Pitfall | Solution |\n",
        "|---------|----------|\n",
        "| `RetrieverTool` for tool retrieval | Use `tool_retriever` parameter instead |\n",
        "| Iterating over unprocessed papers | Check `paper_to_tools_dict.keys()` |\n",
        "| LLM hallucinating tool inputs | Improve tool descriptions with examples |\n",
        "| Agent not calling tools | Add explicit instructions in system prompt |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Embedded Questions Answered\n",
        "\n",
        "### Q1: What is the difference between a Router and an Agent?\n",
        "\n",
        "**Router:**\n",
        "- Single decision point - routes query to ONE tool\n",
        "- No iteration or multi-step reasoning\n",
        "- Faster but less flexible\n",
        "- Example: `RouterQueryEngine` selecting between summary vs. vector tool\n",
        "\n",
        "**Agent:**\n",
        "- Multi-step reasoning loop (ReAct pattern)\n",
        "- Can call MULTIPLE tools in sequence\n",
        "- Can reason about tool outputs and decide next steps\n",
        "- More powerful but higher latency/cost\n",
        "- Example: `FunctionAgent` calling multiple paper tools to answer a comparison query\n",
        "\n",
        "---\n",
        "\n",
        "### Q2: When should you use `tool_retriever` vs passing all tools directly?\n",
        "\n",
        "| Scenario | Approach |\n",
        "|----------|----------|\n",
        "| < 10 tools | Pass all tools directly |\n",
        "| 10-50 tools | Either approach works |\n",
        "| > 50 tools | Use `tool_retriever` to avoid context overflow |\n",
        "| Tools have similar names | Pass directly (retrieval might confuse similar tools) |\n",
        "| Diverse tool set | Use `tool_retriever` for efficiency |\n",
        "\n",
        "---\n",
        "\n",
        "### Q3: Why does `FunctionAgent` need `acall` method on tools?\n",
        "\n",
        "`FunctionAgent` uses async execution internally for:\n",
        "- Parallel tool calls when multiple tools are needed\n",
        "- Non-blocking I/O during API calls\n",
        "- Better performance in production environments\n",
        "\n",
        "`QueryEngineTool` provides both `call()` (sync) and `acall()` (async), making it compatible with `FunctionAgent`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Conclusion\n",
        "\n",
        "This notebook successfully demonstrates:\n",
        "\n",
        "1.  **Router Engine** - Routing queries to appropriate summary/vector tools\n",
        "2.  **Tool Calling** - Creating and using `QueryEngineTool` for document access\n",
        "3.  **Agent Reasoning** - Building `FunctionAgent` with multi-step reasoning\n",
        "4.  **Multi-Document Scaling** - Using `ObjectIndex` + `tool_retriever` for 11 papers\n",
        "5.  **Bug Fixes** - Resolved KeyError and incorrect agent architecture issues\n",
        "6.  **Extensions** - Custom queries demonstrating cross-paper analysis capabilities\n",
        "\n",
        "### Architecture Summary\n",
        "\n",
        "```\n",
        "User Query\n",
        "    \n",
        "    \n",
        "\n",
        "  FunctionAgent  \n",
        "  (with LLM)     \n",
        "\n",
        "         \n",
        "         \n",
        "\n",
        " tool_retriever    Retrieves top-k relevant tools\n",
        " (ObjectIndex)   \n",
        "\n",
        "         \n",
        "         \n",
        "\n",
        "         Retrieved Tools                 \n",
        "                \n",
        "  MetaGPT     Self-RAG    ...       \n",
        "  _vector     _summary              \n",
        "                \n",
        "\n",
        "         \n",
        "         \n",
        "    Agent calls selected tools\n",
        "         \n",
        "         \n",
        "    Synthesized Response\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3GB4XZHXd4m"
      },
      "source": [
        "**End of Notebook**\n",
        "\n",
        "This complete notebook covers all 4 lessons for building agentic RAG systems with LlamaIndex:\n",
        "\n",
        "\n",
        "*   Router Engine - Route queries to appropriate tools\n",
        "\n",
        "*   Tool Calling - Create and use custom function tools\n",
        "*   Agent Reasoning Loop - Build agents with multi-step reasoning\n",
        "*   Multi-Document Agent - Scale to multiple documents with tool retrieval"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
