{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/oviya-raja/ist-402/blob/main/learning-path/W09/W9_Building_Agentic_RAG_LlamaIndex_3_4.ipynb)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bqVkxISRAGy"
      },
      "source": [
        "# Building Agentic RAG with LlamaIndex - Complete Notebook Content\n",
        "\n",
        "This notebook contains all lessons from the course on building agentic RAG systems using LlamaIndex."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVenqsSbRJGa"
      },
      "source": [
        "Setup and Installation\n",
        "First, let's install the required packages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3m9sre_RQoS"
      },
      "source": [
        "# Setup and Installation\n",
        "First, let's install the required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk2nfXE6RFva",
        "outputId": "a30ddc7a-983e-4ef6-b7dd-9a452247100c"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade pip\n",
        "%pip install llama-index\n",
        "%pip install llama-index-llms-openai\n",
        "%pip install llama-index-embeddings-openai\n",
        "%pip install nest-asyncio\n",
        "%pip install openai\n",
        "%pip install python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSJABWx7Rb77"
      },
      "source": [
        "## Set up OpenAI API Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w75jomMERZHb",
        "outputId": "214899eb-6d80-4644-c008-24ce3a942df0"
      },
      "outputs": [],
      "source": [
        "# Set up OpenAI API Key\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Try to get API key from Google Colab userdata first (if running in Colab)\n",
        "OPENAI_API_KEY = None\n",
        "try:\n",
        "    import google.colab\n",
        "    from google.colab import userdata\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "    if OPENAI_API_KEY:\n",
        "        print(\"‚úÖ OpenAI API Key loaded from Colab userdata!\")\n",
        "except (ImportError, ValueError):\n",
        "    # Not running in Colab or userdata not available, try environment variables\n",
        "    pass\n",
        "\n",
        "# If not found in Colab userdata, try environment variables\n",
        "if not OPENAI_API_KEY:\n",
        "    # Load environment variables from .env file\n",
        "    load_dotenv()\n",
        "    \n",
        "    # Get OpenAI API Key from environment variable\n",
        "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "    if OPENAI_API_KEY:\n",
        "        print(\"‚úÖ OpenAI API Key loaded from environment variables!\")\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"OPENAI_API_KEY not found. Please set it in one of the following ways:\\n\"\n",
        "            \"  - In Google Colab: userdata.set('OPENAI_API_KEY', 'your_key')\\n\"\n",
        "            \"  - Locally: Create a .env file with OPENAI_API_KEY=your_key\\n\"\n",
        "            \"  - Or set environment variable: export OPENAI_API_KEY=your_key\"\n",
        "        )\n",
        "\n",
        "# Ensure the API key is set in the environment for OpenAI libraries\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "print(\"OpenAI API Key configured successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCcEpZn5RkLU"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDa_MkmeRzEE"
      },
      "source": [
        "# Lesson 1: Router Engine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlO-_ImmR8u1"
      },
      "source": [
        "### Load Data\n",
        "Download the MetaGPT paper:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IWJozRpR3KM",
        "outputId": "3ec00f0e-512a-4ada-d9d5-000e5e3fb743"
      },
      "outputs": [],
      "source": [
        "# Create data directory if it doesn't exist\n",
        "import os\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# Download the MetaGPT paper\n",
        "!wget \"https://openreview.net/pdf?id=VtmBAGCN7o\" -O data/metagpt.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urqMuc3uSF0l"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "# load documents\n",
        "documents = SimpleDirectoryReader(input_files=[\"data/metagpt.pdf\"]).load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKf_IJp-TMah"
      },
      "source": [
        "## Define LLM and Embedding Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-2smc6ITQUh"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "splitter = SentenceSplitter(chunk_size=1024)\n",
        "nodes = splitter.get_nodes_from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fk6roZbpTR_h"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Settings\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxpZND2rTUbZ"
      },
      "source": [
        "## Define Summary Index and Vector Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbdFzj_GTWPz"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
        "\n",
        "summary_index = SummaryIndex(nodes)\n",
        "vector_index = VectorStoreIndex(nodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBLsNW0YTYUd"
      },
      "source": [
        "## Define Query Engines and Set Metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBy7Qd_gTafp"
      },
      "outputs": [],
      "source": [
        "summary_query_engine = summary_index.as_query_engine(\n",
        "    response_mode=\"tree_summarize\",\n",
        "    use_async=True,\n",
        ")\n",
        "vector_query_engine = vector_index.as_query_engine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFLi0Z32TdSa"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "print(\"Creating summary tool...\")\n",
        "summary_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=summary_query_engine,\n",
        "    description=(\n",
        "        \"Useful for summarization questions related to MetaGPT\"\n",
        "    ),\n",
        ")\n",
        "print(f\"‚úì Summary tool created successfully\")\n",
        "print(f\"  Description: {summary_tool.metadata.description}\")\n",
        "\n",
        "print(\"\\nCreating vector tool...\")\n",
        "vector_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=vector_query_engine,\n",
        "    description=(\n",
        "        \"Useful for retrieving specific context from the MetaGPT paper.\"\n",
        "    ),\n",
        ")\n",
        "print(f\"‚úì Vector tool created successfully\")\n",
        "print(f\"  Description: {vector_tool.metadata.description}\")\n",
        "print(\"\\n‚úì Both tools are ready to use!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "di42S-cSTgFI"
      },
      "source": [
        "## Define Router Query Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4c_U0uO_TfeK"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "\n",
        "query_engine = RouterQueryEngine(\n",
        "    selector=LLMSingleSelector.from_defaults(),\n",
        "    query_engine_tools=[\n",
        "        summary_tool,\n",
        "        vector_tool,\n",
        "    ],\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQ9zrRopTjiJ",
        "outputId": "9fda8d22-7294-4515-f437-dfc96e542574"
      },
      "outputs": [],
      "source": [
        "response = query_engine.query(\"What is the summary of the document?\")\n",
        "print(str(response))\n",
        "print(len(response.source_nodes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qbzrYinTm38",
        "outputId": "88faf2a7-74cf-4fe4-e75f-97b51b0adf5b"
      },
      "outputs": [],
      "source": [
        "response = query_engine.query(\n",
        "    \"How do agents share information with other agents?\"\n",
        ")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7P2fB-CTvr8"
      },
      "source": [
        "# Lesson 2: Tool Calling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60323azIT4lL"
      },
      "source": [
        "### 1. Define a Simple Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZ8ZXkAQTxLD"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.tools import FunctionTool\n",
        "\n",
        "def add(x: int, y: int) -> int:\n",
        "    \"\"\"Adds two integers together.\"\"\"\n",
        "    return x + y\n",
        "\n",
        "def mystery(x: int, y: int) -> int:\n",
        "    \"\"\"Mystery function that operates on top of two numbers.\"\"\"\n",
        "    return (x + y) * (x + y)\n",
        "\n",
        "add_tool = FunctionTool.from_defaults(fn=add)\n",
        "mystery_tool = FunctionTool.from_defaults(fn=mystery)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIcGPDXyT7oM",
        "outputId": "824b01b8-0572-4041-bbd7-3d069bb0ea03"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# EXPLANATION: Using LLM with Function/Tool Calling\n",
        "# ============================================================================\n",
        "# This demonstrates how an LLM can intelligently choose and call functions/tools\n",
        "# based on a natural language query.\n",
        "\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "# Step 1: Initialize the OpenAI LLM\n",
        "# This creates a connection to OpenAI's GPT-3.5-turbo model\n",
        "print(\"Step 1: Initializing OpenAI LLM (gpt-3.5-turbo)...\")\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "print(\"‚úì LLM initialized\\n\")\n",
        "\n",
        "# Step 2: Use predict_and_call to let the LLM decide which tool to use\n",
        "# The LLM will:\n",
        "#   - Analyze the query: \"Tell me the output of the mystery function on 2 and 9\"\n",
        "#   - Understand it needs to call a function with arguments 2 and 9\n",
        "#   - Choose the appropriate tool (mystery_tool in this case)\n",
        "#   - Call the function with the correct arguments\n",
        "#   - Return the result\n",
        "\n",
        "print(\"Step 2: LLM analyzing query and selecting appropriate tool...\")\n",
        "print(\"Query: 'Tell me the output of the mystery function on 2 and 9'\")\n",
        "print(\"Available tools: add_tool, mystery_tool\")\n",
        "print(\"\\nLLM reasoning process (verbose=True shows this):\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "response = llm.predict_and_call(\n",
        "    [add_tool, mystery_tool],  # List of available tools the LLM can choose from\n",
        "    \"Tell me the output of the mystery function on 2 and 9\",  # User's query\n",
        "    verbose=True  # Shows the LLM's decision-making process\n",
        ")\n",
        "\n",
        "print(\"-\" * 60)\n",
        "print(\"\\nStep 3: Final response from LLM:\")\n",
        "print(\"=\" * 60)\n",
        "print(str(response))\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Explanation of what happened:\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"WHAT HAPPENED:\")\n",
        "print(\"=\" * 60)\n",
        "print(\"1. The LLM received your query asking about 'mystery function'\")\n",
        "print(\"2. It analyzed the available tools and chose 'mystery_tool'\")\n",
        "print(\"3. It extracted the arguments: x=2, y=9\")\n",
        "print(\"4. It called mystery_tool(2, 9) which calculates: (2+9) * (2+9) = 121\")\n",
        "print(\"5. It returned the result: 121\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bk-Waqk2T-FU"
      },
      "source": [
        "### 2. Define an Auto-Retrieval Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5O_iv2aTT_fz"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "# load documents\n",
        "documents = SimpleDirectoryReader(input_files=[\"data/metagpt.pdf\"]).load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJjN83C8UCIz",
        "outputId": "5ec5f732-ac37-442b-caa0-3311e737779c"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "splitter = SentenceSplitter(chunk_size=1024)\n",
        "nodes = splitter.get_nodes_from_documents(documents)\n",
        "print(nodes[0].get_content(metadata_mode=\"all\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_O-RStXUDjM"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "vector_index = VectorStoreIndex(nodes)\n",
        "query_engine = vector_index.as_query_engine(similarity_top_k=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQyExE-bUFSk",
        "outputId": "44577393-554b-4d3a-d7f7-84ba651fa9bb"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.vector_stores import MetadataFilters\n",
        "\n",
        "query_engine = vector_index.as_query_engine(\n",
        "    similarity_top_k=2,\n",
        "    filters=MetadataFilters.from_dicts(\n",
        "        [\n",
        "            {\"key\": \"page_label\", \"value\": \"2\"}\n",
        "        ]\n",
        "    )\n",
        ")\n",
        "\n",
        "response = query_engine.query(\n",
        "    \"What are some high-level results of MetaGPT?\",\n",
        ")\n",
        "print(str(response))\n",
        "for n in response.source_nodes:\n",
        "    print(n.metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGMYtivzUIR-"
      },
      "source": [
        "### Define the Auto-Retrieval Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a69NkZQDUOYE"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from llama_index.core.vector_stores import FilterCondition\n",
        "\n",
        "def vector_query(\n",
        "    query: str,\n",
        "    page_numbers: List[str]\n",
        ") -> str:\n",
        "    \"\"\"Perform a vector search over an index.\n",
        "\n",
        "    query (str): the string query to be embedded.\n",
        "    page_numbers (List[str]): Filter by set of pages. Leave BLANK if we want to perform a vector search\n",
        "        over all pages. Otherwise, filter by the set of specified pages.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    metadata_dicts = [\n",
        "        {\"key\": \"page_label\", \"value\": p} for p in page_numbers\n",
        "    ]\n",
        "\n",
        "    query_engine = vector_index.as_query_engine(\n",
        "        similarity_top_k=2,\n",
        "        filters=MetadataFilters.from_dicts(\n",
        "            metadata_dicts,\n",
        "            condition=FilterCondition.OR\n",
        "        )\n",
        "    )\n",
        "    response = query_engine.query(query)\n",
        "    return response\n",
        "\n",
        "\n",
        "vector_query_tool = FunctionTool.from_defaults(\n",
        "    name=\"vector_tool\",\n",
        "    fn=vector_query\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGWYH4bBUTgl",
        "outputId": "29aac91d-d332-426b-fb25-424143d6d4a2"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "response = llm.predict_and_call(\n",
        "    [vector_query_tool],\n",
        "    \"What are the high-level results of MetaGPT as described on page 2?\",\n",
        "    verbose=True\n",
        ")\n",
        "for n in response.source_nodes:\n",
        "    print(n.metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdI82mq4UVAf"
      },
      "source": [
        "### Add More Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFANjpM0UbS1"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SummaryIndex\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "summary_index = SummaryIndex(nodes)\n",
        "summary_query_engine = summary_index.as_query_engine(\n",
        "    response_mode=\"tree_summarize\",\n",
        "    use_async=True,\n",
        ")\n",
        "summary_tool = QueryEngineTool.from_defaults(\n",
        "    name=\"summary_tool\",\n",
        "    query_engine=summary_query_engine,\n",
        "    description=(\n",
        "        \"Useful if you want to get a summary of MetaGPT\"\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tN6TUTjIUeP7",
        "outputId": "bcb59ce3-836b-4880-ccc5-354c53cecb34"
      },
      "outputs": [],
      "source": [
        "response = llm.predict_and_call(\n",
        "    [vector_query_tool, summary_tool],\n",
        "    \"What are the MetaGPT comparisons with ChatDev described on page 8?\",\n",
        "    verbose=True\n",
        ")\n",
        "for n in response.source_nodes:\n",
        "    print(n.metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLOx8OpkUgDc",
        "outputId": "617f1f07-d5be-4cfd-8db8-8cfcae26e267"
      },
      "outputs": [],
      "source": [
        "response = llm.predict_and_call(\n",
        "    [vector_query_tool, summary_tool],\n",
        "    \"What is a summary of the paper?\",\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsuAfoQQUnXF"
      },
      "source": [
        "# Lesson 3: Building an Agent Reasoning Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbG-KyJPUt9C"
      },
      "source": [
        "## Setup Function Calling Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMKPrg-IUqDi"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrClHpeJUzav"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.agent.workflow import FunctionAgent\n",
        "\n",
        "agent = FunctionAgent(\n",
        "    tools=[vector_tool, summary_tool],\n",
        "    llm=llm,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cr-IbHktU1P6",
        "outputId": "ffa7c75a-9a06-492d-bf36-0ce625739cb0"
      },
      "outputs": [],
      "source": [
        "# For FunctionAgent - must use asyncio.run() for async execution\n",
        "import asyncio\n",
        "\n",
        "response = asyncio.run(agent.run(\n",
        "    \"Tell me about the agent roles in MetaGPT, and how they communicate.\"\n",
        "))\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJpxzzjfU37G",
        "outputId": "7ba8eaf8-60e1-4ac9-9661-1e61c78d83eb"
      },
      "outputs": [],
      "source": [
        "response = asyncio.run(agent.run(\n",
        "    \"Tell me about the evaluation datasets used.\"\n",
        "))\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOZowgzOU6Ie",
        "outputId": "3b5cea53-0019-4347-da55-653478abdccf"
      },
      "outputs": [],
      "source": [
        "response = asyncio.run(agent.run(\"Tell me the results over one of the above datasets.\"))\n",
        "\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBRDERx3WiPM"
      },
      "source": [
        "# Lesson 4: Building a Multi-Document Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC-ZUdZqWlkY"
      },
      "source": [
        "## 1. Setup an Agent Over 3 Papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9p_5UBCMWlAt"
      },
      "outputs": [],
      "source": [
        "urls = [\n",
        "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
        "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
        "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
        "]\n",
        "\n",
        "papers = [\n",
        "    \"metagpt.pdf\",\n",
        "    \"longlora.pdf\",\n",
        "    \"selfrag.pdf\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nCY_iZXWq3D",
        "outputId": "5cc631d1-1543-413f-fcc2-e7de21fde836"
      },
      "outputs": [],
      "source": [
        "# Download papers\n",
        "for url, paper in zip(urls, papers):\n",
        "    !wget \"{url}\" -O \"data/{paper}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_PZDk3lWtET"
      },
      "outputs": [],
      "source": [
        "# Helper function to create tools for each paper\n",
        "# Works in both Google Colab and local environments\n",
        "from pathlib import Path\n",
        "from llama_index.core import SimpleDirectoryReader, SummaryIndex, VectorStoreIndex\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "def get_doc_tools(file_path: str, name: str):\n",
        "    \"\"\"\n",
        "    Get vector and summary query engine tools from a document.\n",
        "    \n",
        "    This function works in both Google Colab and local environments.\n",
        "    Make sure Settings.llm and Settings.embed_model are configured before calling this function.\n",
        "    \n",
        "    Args:\n",
        "        file_path: Path to the document file (relative or absolute path)\n",
        "        name: Name identifier for the document (used in tool descriptions)\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (vector_tool, summary_tool) - QueryEngineTool instances\n",
        "    \"\"\"\n",
        "\n",
        "    # Load documents\n",
        "    documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
        "    splitter = SentenceSplitter(chunk_size=1024)\n",
        "    nodes = splitter.get_nodes_from_documents(documents)\n",
        "\n",
        "    # Create indices\n",
        "    vector_index = VectorStoreIndex(nodes)\n",
        "    summary_index = SummaryIndex(nodes)\n",
        "\n",
        "    # Create query engines\n",
        "    vector_query_engine = vector_index.as_query_engine()\n",
        "    summary_query_engine = summary_index.as_query_engine(\n",
        "        response_mode=\"tree_summarize\",\n",
        "        use_async=True,\n",
        "    )\n",
        "\n",
        "    # Create tools\n",
        "    vector_tool = QueryEngineTool.from_defaults(\n",
        "        query_engine=vector_query_engine,\n",
        "        description=f\"Useful for retrieving specific context from {name}.\",\n",
        "    )\n",
        "\n",
        "    summary_tool = QueryEngineTool.from_defaults(\n",
        "        query_engine=summary_query_engine,\n",
        "        description=f\"Useful for summarization questions related to {name}.\",\n",
        "    )\n",
        "\n",
        "    return vector_tool, summary_tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHKWRYVGWunE",
        "outputId": "9f0019b4-e82d-43b5-8152-18de05818cc5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "paper_to_tools_dict = {}\n",
        "for paper in papers:\n",
        "    print(f\"Getting tools for paper: {paper}\")\n",
        "    \n",
        "    # Check if file exists in data directory before processing\n",
        "    file_path = f\"data/{paper}\"\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"  ‚ö†Ô∏è  Warning: File '{file_path}' does not exist. Skipping...\")\n",
        "        print(f\"  üí° Tip: Make sure you've downloaded all papers first.\")\n",
        "        continue\n",
        "    \n",
        "    try:\n",
        "        vector_tool, summary_tool = get_doc_tools(file_path, Path(paper).stem)\n",
        "        paper_to_tools_dict[paper] = [vector_tool, summary_tool]\n",
        "        print(f\"  ‚úì Successfully created tools for {paper}\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Error processing {paper}: {e}\")\n",
        "        print(f\"  Skipping this paper...\\n\")\n",
        "        continue\n",
        "\n",
        "print(f\"\\n‚úì Successfully processed {len(paper_to_tools_dict)} papers\")\n",
        "print(f\"Papers with tools: {list(paper_to_tools_dict.keys())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZRNhKIhWvcr"
      },
      "outputs": [],
      "source": [
        "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1CKgcR_WyBr",
        "outputId": "98d6d1e2-72fb-4b0c-e84a-0e1dc65c82e6"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "print(f\"Number of tools: {len(initial_tools)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrotN19WWzv8"
      },
      "outputs": [],
      "source": [
        "# LlamaIndex >= 0.14.6\n",
        "from llama_index.core.agent.workflow import FunctionAgent\n",
        "# If any items in `initial_tools` are plain Python functions, wrap them first:\n",
        "# from llama_index.core.tools import FunctionTool\n",
        "# initial_tools = [FunctionTool.from_defaults(fn) for fn in initial_tools]\n",
        "\n",
        "agent = FunctionAgent(\n",
        "    tools=initial_tools,\n",
        "    llm=llm,\n",
        "    verbose=True,  # optional: shows workflow logs\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbx2Xa5YW6At",
        "outputId": "842b5dbf-8766-49b2-aedf-4c2d9121d0dd"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "\n",
        "response = asyncio.run(agent.run(\n",
        "    user_msg=(\n",
        "        \"Tell me about the evaluation dataset used in LongLoRA, \"\n",
        "        \"and then tell me about the evaluation results\"\n",
        "    )\n",
        "))\n",
        "print(str(response))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7YAEWvXW8X8",
        "outputId": "503250ca-3c48-4e77-db34-eacf144f378c"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "\n",
        "response = asyncio.run(\n",
        "    agent.run(\n",
        "        user_msg=\"Give me a summary of both Self-RAG and LongLoRA\"\n",
        "    )\n",
        ")\n",
        "print(str(response))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jGd-H9zW95t"
      },
      "source": [
        "## 2. Setup an Agent Over 11 Papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZTLBxFUXBX5"
      },
      "outputs": [],
      "source": [
        "urls = [\n",
        "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
        "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
        "    \"https://openreview.net/pdf?id=LzPWWPAdY4\",\n",
        "    \"https://openreview.net/pdf?id=VTF8yNQM66\",\n",
        "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
        "    \"https://openreview.net/pdf?id=9WD9KwssyT\",\n",
        "    \"https://openreview.net/pdf?id=yV6fD7LYkF\",\n",
        "    \"https://openreview.net/pdf?id=hnrB5YHoYu\",\n",
        "    \"https://openreview.net/pdf?id=WbWtOYIzIK\",\n",
        "    \"https://openreview.net/pdf?id=c5pwL0Soay\",\n",
        "    \"https://openreview.net/pdf?id=TpD2aG1h0D\"\n",
        "]\n",
        "\n",
        "papers = [\n",
        "    \"metagpt.pdf\",\n",
        "    \"longlora.pdf\",\n",
        "    \"loftq.pdf\",\n",
        "    \"swebench.pdf\",\n",
        "    \"selfrag.pdf\",\n",
        "    \"zipformer.pdf\",\n",
        "    \"values.pdf\",\n",
        "    \"finetune_fair_diffusion.pdf\",\n",
        "    \"knowledge_card.pdf\",\n",
        "    \"metra.pdf\",\n",
        "    \"vr_mcl.pdf\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCo2FY1jXESb",
        "outputId": "18c92fa6-bd24-4dd6-81d9-a7b97dff31ae"
      },
      "outputs": [],
      "source": [
        "# Download all papers\n",
        "for url, paper in zip(urls, papers):\n",
        "    !wget \"{url}\" -O \"data/{paper}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2YQ6MCLXGjc",
        "outputId": "4ff70cf5-19ee-42fb-8b6f-f45e0b495097"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "paper_to_tools_dict = {}\n",
        "\n",
        "for paper in papers:\n",
        "    print(f\"Getting tools for paper: {paper}\")\n",
        "    \n",
        "    # Check if file exists in data directory before processing\n",
        "    file_path = f\"data/{paper}\"\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"  ‚ö†Ô∏è  Warning: File '{file_path}' does not exist. Skipping...\")\n",
        "        print(f\"  üí° Tip: Make sure you've downloaded all papers first.\")\n",
        "        continue\n",
        "    \n",
        "    try:\n",
        "        vector_tool, summary_tool = get_doc_tools(file_path, Path(paper).stem)\n",
        "        paper_to_tools_dict[paper] = [vector_tool, summary_tool]\n",
        "        print(f\"  ‚úì Successfully created tools for {paper}\")\n",
        "\n",
        "    except UnicodeEncodeError as e:\n",
        "        print(f\" Unicode error while processing {paper}: {e}\")\n",
        "        try:\n",
        "            # Attempt to re-read text safely and re-generate tools\n",
        "            text_bytes = Path(file_path).read_bytes()\n",
        "            safe_text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n",
        "\n",
        "            # Optionally, save the cleaned text for inspection\n",
        "            clean_path = Path(file_path).with_name(Path(file_path).stem + \"_clean.txt\")\n",
        "            clean_path.write_text(safe_text, encoding=\"utf-8\")\n",
        "            print(f\"Saved cleaned version: {clean_path}\")\n",
        "\n",
        "            # Retry tool creation if your get_doc_tools can accept a string path\n",
        "            vector_tool, summary_tool = get_doc_tools(str(clean_path), Path(paper).stem)\n",
        "            paper_to_tools_dict[paper] = [vector_tool, summary_tool]\n",
        "            print(f\" Retried successfully for {paper}\")\n",
        "\n",
        "        except Exception as inner_e:\n",
        "            print(f\" Still failed on {paper}: {inner_e}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Unexpected error for {paper}: {e}\")\n",
        "\n",
        "print(f\"\\n‚úì Successfully processed {len(paper_to_tools_dict)} papers\")\n",
        "print(f\"Papers with tools: {list(paper_to_tools_dict.keys())}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ4I4TvuXJ5V"
      },
      "source": [
        "## Extend the Agent with Tool Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ydbcE-RXMi7"
      },
      "outputs": [],
      "source": [
        "all_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zTX2YhlXOlz"
      },
      "outputs": [],
      "source": [
        "# Define an \"object\" index and retriever over these tools\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.objects import ObjectIndex\n",
        "\n",
        "obj_index = ObjectIndex.from_objects(\n",
        "    all_tools,\n",
        "    index_cls=VectorStoreIndex,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RD46kQ4aXRAm"
      },
      "outputs": [],
      "source": [
        "obj_retriever = obj_index.as_retriever(similarity_top_k=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EJybydrXSzx",
        "outputId": "8f4b6950-a3cd-465b-bc72-770fd4b3ea55"
      },
      "outputs": [],
      "source": [
        "tools = obj_retriever.retrieve(\n",
        "    \"Tell me about the eval dataset used in MetaGPT and SWE-Bench\"\n",
        ")\n",
        "tools[2].metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjX1Z260XVCu"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.agent.workflow import FunctionAgent\n",
        "from llama_index.core.tools import RetrieverTool  # ‚úÖ wrap retrievers as tools\n",
        "\n",
        "retriever_tool = RetrieverTool.from_defaults(\n",
        "    retriever=obj_retriever,\n",
        "    name=\"paper_retriever\",\n",
        "    description=\"Retrieve relevant chunks from the loaded papers.\"\n",
        ")\n",
        "\n",
        "agent = FunctionAgent(\n",
        "    tools=[retriever_tool],\n",
        "    llm=llm,\n",
        "    system_prompt=(\n",
        "        \"You are an agent designed to answer queries over a set of given papers. \"\n",
        "        \"Always use the provided tools to answer a question. Do not rely on prior knowledge.\"\n",
        "    ),\n",
        "    verbose=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pwRmYl81ZLD",
        "outputId": "f1016d2a-5c89-499a-e56a-05ae95ef6eaf"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "\n",
        "response = asyncio.run(\n",
        "    agent.run(\n",
        "        user_msg=\"Give me a summary of both Self-RAG and LongLoRA\"\n",
        "    )\n",
        ")\n",
        "print(str(response))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KBGrr4lXXF1",
        "outputId": "8d4ca211-9c41-42ff-a700-4bd4e486b88a"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "\n",
        "response = asyncio.run(\n",
        "    agent.run(\n",
        "        user_msg=(\n",
        "            \"Tell me about the evaluation dataset used \"\n",
        "            \"in MetaGPT and compare it against SWE-Bench.\"\n",
        "        )\n",
        "    )\n",
        ")\n",
        "print(str(response))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzhrevyqXaBl",
        "outputId": "a4a00de8-b201-4541-aaf0-5f1d87490a05"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "\n",
        "response = asyncio.run(\n",
        "    agent.run(\n",
        "        user_msg=(\n",
        "            \"Compare and contrast the LoRA papers (LongLoRA, LoftQ). \"\n",
        "            \"Analyze the approach in each paper first.\"\n",
        "        )\n",
        "    )\n",
        ")\n",
        "print(str(response))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3GB4XZHXd4m"
      },
      "source": [
        "**End of Notebook**\n",
        "\n",
        "This complete notebook covers all 4 lessons for building agentic RAG systems with LlamaIndex:\n",
        "\n",
        "\n",
        "*   Router Engine - Route queries to appropriate tools\n",
        "\n",
        "*   Tool Calling - Create and use custom function tools\n",
        "*   Agent Reasoning Loop - Build agents with multi-step reasoning\n",
        "*   Multi-Document Agent - Scale to multiple documents with tool retrieval"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
