# LLM Fundamentals - Personal Notes

Core AI concepts and how LLMs work.

## ğŸ§  Core Concepts

### 1. Tokens
Text split into small units (tokens).

### 2. Embeddings
Tokens â†’ vectors (numerical representation).

### 3. Vector Relationships (Attention)
The model computes how each token/vector relates to every other.

### 4. Layers
Stacked attention + feedforward blocks that apply transformations repeatedly.

### 5. Tensors
Data and weights stored in multi-dimensional arrays.

### 6. Parameters (Weights)
Learned numerical values inside tensors, updated during training.

## ğŸ”— Flow

**LLM = Tokens â†’ Embeddings â†’ Vector Relationships (Attention) â†’ Layers â†’ Tensors â†’ Parameters (Weights)**

## ğŸ“ Code Examples

Add your code examples and experiments here:
- Model loading examples
- Tokenization examples
- Embedding experiments
- Training code

## ğŸ¯ Key Learnings

Document your understanding of:
- How transformers work
- Attention mechanisms
- Training vs inference
- Model architectures

