{"concept": "Backpropagation", "question": "Explain backpropagation in neural networks.", "remember": "Backpropagation is an algorithm used to compute gradients in neural networks by propagating error backward from the output layer to earlier layers.", "understand": "Backpropagation adjusts the weights of a neural network by comparing the predicted output to the true label, computing an error, and then propagating that error backward through each layer to update the weights using gradient descent.", "apply": "Given a simple 2-layer network predicting house prices, we can use backpropagation to compute how much each weight contributed to the prediction error and then update those weights to reduce future error.", "analyze": "Backpropagation relies on the chain rule of calculus to break the overall loss gradient into products of local gradients at each layer. It assumes differentiable activation functions and can be inefficient or unstable in very deep networks due to vanishing or exploding gradients.", "evaluate": "Backpropagation is effective and widely used, but can be sensitive to learning rate, initial weights, and architecture. It also requires labeled data and can be computationally expensive on large models.", "create": "Design a small neural network to classify handwritten digits and describe how you would use backpropagation and gradient descent to train it on the MNIST dataset."}
{"concept": "Overfitting", "question": "What is overfitting and how can we reduce it?", "remember": "Overfitting occurs when a model learns the training data too well, including noise and irrelevant patterns, leading to poor performance on new, unseen data.", "understand": "Overfitting happens when a model becomes too complex relative to the amount of training data, essentially memorizing the training set rather than learning generalizable patterns.", "apply": "If you train a decision tree with unlimited depth on a small dataset, it will create branches for every single training example, achieving 100% training accuracy but failing on test data.", "analyze": "Overfitting results from the bias-variance tradeoff: high model complexity (low bias) increases variance. Factors include insufficient data, too many parameters, lack of regularization, and training for too long.", "evaluate": "Overfitting is a critical problem that must be addressed through techniques like cross-validation, regularization, early stopping, and data augmentation. However, some overfitting can be acceptable if the model still generalizes reasonably well.", "create": "Design a training strategy for a deep neural network that includes techniques to prevent overfitting: dropout layers, L2 regularization, data augmentation, and early stopping based on validation loss."}



