layer,category,week_number,week_name,item_name,description,keywords,file_path,executable_type,command_to_run,dependencies,prerequisites,priority,difficulty_level,estimated_time_minutes,learning_objectives,output_artifacts,portfolio_value
CORE,concept,W00,Core AI Concepts,Tokens and Tokenization,Text split into small units (tokens). Foundation of all LLM processing. Understand BPE subword and character tokenization.,tokens tokenization bpe subword text processing,core/tokens.py,python,"python learning-path/W00/core/tokens.py",transformers tiktoken,None,1,beginner,45,"Understand token units|Compare tokenization methods|Analyze token efficiency",tokenizer comparison,foundation
CORE,concept,W00,Core AI Concepts,Embeddings,Tokens converted to vectors (numerical representations). Semantic meaning encoded in high-dimensional space.,embeddings vectors numerical representation semantic space,core/embeddings.py,python,"python learning-path/W00/core/embeddings.py",sentence-transformers numpy,None,1,beginner,60,"Convert tokens to vectors|Visualize embedding space|Measure semantic similarity",embedding visualizations,foundation
CORE,concept,W00,Core AI Concepts,Vector Relationships and Attention,How each token/vector relates to every other. Self-attention mechanism computes contextual relationships.,attention self-attention vector relationships context,core/attention.py,python,"python learning-path/W00/core/attention.py",torch transformers,None,1,intermediate,75,"Understand attention mechanism|Visualize attention patterns|Compute attention scores",attention heatmaps,foundation
CORE,concept,W00,Core AI Concepts,Transformer Layers,Stacked attention + feedforward blocks. Multiple layers apply attention and transformations repeatedly.,transformer layers feedforward blocks stacked architecture,core/transformer_layers.py,python,"python learning-path/W00/core/transformer_layers.py",torch transformers,None,1,intermediate,60,"Understand layer composition|Trace data through layers|Analyze layer outputs",layer analysis,foundation
CORE,concept,W00,Core AI Concepts,Tensors,Data and weights stored and computed in tensors (multi-dimensional arrays). Core data structure for deep learning.,tensors multidimensional arrays data structure computation,core/tensors.py,python,"python learning-path/W00/core/tensors.py",torch numpy,None,1,beginner,45,"Manipulate tensor shapes|Understand broadcasting|Perform tensor operations",tensor exercises,foundation
CORE,concept,W00,Core AI Concepts,Parameters and Weights,Learned numerical values inside tensors updated during training. The knowledge of the model.,parameters weights learned values training optimization,core/parameters.py,python,"python learning-path/W00/core/parameters.py",torch transformers,None,1,intermediate,60,"Count model parameters|Inspect weight matrices|Understand parameter updates",parameter analysis,foundation
CORE,concept,W00,Core AI Concepts,LLM Architecture Overview,Complete picture: Tokens → Embeddings → Attention → Layers → Tensors → Parameters. End-to-end understanding.,llm architecture overview pipeline complete understanding,core/llm_architecture.py,python,"python learning-path/W00/core/llm_architecture.py",transformers torch,None,1,intermediate,90,"Trace full LLM pipeline|Inspect model components|Extract architecture details",architecture diagram,foundation
CORE,exercise,W00,Core AI Concepts,Model Inspection Exercise,Retrieve hidden size number of layers attention heads tensor shapes embedding size vocabulary size from any model.,model inspection architecture config hidden size layers heads,exercises/model_inspection.py,python,"python learning-path/W00/exercises/model_inspection.py",transformers torch,None,2,beginner,60,"Extract model config|Analyze architecture|Compare model sizes",model comparison report,high
CORE,exercise,W00,Core AI Concepts,Build Mini GPT from Scratch,Build a real small GPT-style model (10-20M params) from scratch. Implements attention layers and training loop.,mini gpt from scratch training attention implementation,exercises/bloom_mini_gpt.py,python,"python learning-path/W00/exercises/bloom_mini_gpt.py",torch tokenizers,None,3,advanced,180,"Implement transformer blocks|Train from scratch|Generate text",working mini LLM,flagship
CORE,exercise,W00,Core AI Concepts,Custom Tokenizer Training,Train a BPE tokenizer on custom corpus. Understand vocabulary construction and subword segmentation.,tokenizer training bpe vocabulary custom corpus,exercises/train_tokenizer.py,python,"python learning-path/W00/exercises/train_tokenizer.py",tokenizers,None,2,intermediate,45,"Train BPE tokenizer|Configure vocab size|Test tokenization",custom tokenizer.json,high
CORE,project,W00,Core AI Concepts,Bloom Taxonomy Educational LLM,Build a tutor that answers using Bloom's Taxonomy levels: remember understand apply analyze evaluate create.,bloom taxonomy educational tutor levels learning,projects/bloom_tutor/main.py,python,"python learning-path/W00/projects/bloom_tutor/main.py",transformers sentence-transformers faiss-cpu,None,4,advanced,240,"Design Bloom prompts|Generate training data|Build educational RAG",bloom tutor system,flagship
CORE,project,W00,Core AI Concepts,Bloom RAG System,RAG system that retrieves from learning notes and answers in Bloom's Taxonomy format with 6 cognitive levels.,bloom rag retrieval taxonomy educational notes,projects/bloom_tutor/bloom_rag.py,python,"python learning-path/W00/projects/bloom_tutor/bloom_rag.py",sentence-transformers faiss-cpu transformers,None,4,advanced,120,"Build note retrieval|Implement Bloom prompt|Ground answers in context",bloom rag system,flagship
CORE,project,W00,Core AI Concepts,Bloom Dataset Generator,Auto-generate Bloom's Taxonomy training data from notes. Creates instruction dataset for fine-tuning.,bloom dataset generator training data instruction,projects/bloom_tutor/bloom_dataset_generator.py,python,"python learning-path/W00/projects/bloom_tutor/bloom_dataset_generator.py",transformers datasets,None,3,intermediate,90,"Parse learning notes|Generate Q&A pairs|Format for training",bloom_training_data.jsonl,high
CORE,project,W00,Core AI Concepts,Bloom Evaluation Script,Evaluate Bloom quality: checks for all 6 sections provides human rubric scores measures completeness.,bloom evaluation quality assessment rubric sections,projects/bloom_tutor/eval_bloom.py,python,"python learning-path/W00/projects/bloom_tutor/eval_bloom.py",transformers,None,3,intermediate,60,"Check section coverage|Score with rubric|Measure quality",evaluation report,high
CORE,deployment,W00,Core AI Concepts,Ollama Bloom Tutor Deployment,Deploy Bloom tutor via Ollama Modelfile. Bakes Bloom's Taxonomy prompt into local model.,ollama deployment modelfile bloom tutor local,projects/bloom_tutor/Modelfile,config,"ollama create bloom-tutor -f Modelfile",ollama,None,4,intermediate,30,"Create Modelfile|Build Ollama model|Run locally",running bloom tutor,high
APPLICATION,concept,W01,Prompt Engineering,LLM Fundamentals,Understanding how LLMs work: tokenization attention mechanisms and generation strategies. Foundation for building AI applications.,llm fundamentals tokenization attention transformer architecture,concepts/llm_fundamentals.py,python,"python learning-path/W01/concepts/llm_fundamentals.py",transformers torch,None,1,beginner,60,"Understand tokenization|Learn attention mechanisms|Explore generation strategies",notes,foundation
APPLICATION,concept,W01,Prompt Engineering,Prompt Design Patterns,Common prompt patterns: zero-shot few-shot chain-of-thought and instruction following. Core skill for application builders.,prompt patterns zero-shot few-shot cot instruction following,concepts/prompt_patterns.py,python,"python learning-path/W01/concepts/prompt_patterns.py",transformers,None,1,beginner,45,"Master zero-shot prompting|Implement few-shot examples|Apply chain-of-thought",code snippets,foundation
APPLICATION,exercise,W01,Prompt Engineering,Prompt Engineering Exercise,Introduction to prompt engineering with Mistral-7B model. Learn system prompts and prompt engineering techniques.,prompt engineering mistral system prompt llm instruction tuning,exercises/prompt_engineering.py,python,"python learning-path/W01/exercises/prompt_engineering.py",transformers torch accelerate,None,1,beginner,90,"Design effective system prompts|Test prompt variations|Measure output quality",working prompts,high
APPLICATION,exercise,W01,Prompt Engineering,Temperature and Sampling,Explore temperature top-p top-k sampling and their effects on generation diversity and quality.,temperature sampling top-p top-k generation parameters,exercises/sampling_exploration.py,python,"python learning-path/W01/exercises/sampling_exploration.py",transformers torch,None,2,beginner,60,"Understand temperature effects|Compare sampling strategies|Tune generation parameters",comparison report,medium
APPLICATION,concept,W02,RAG Foundations,Data Engineering for RAG,Data pipelines for RAG: ETL processes data quality validation lineage tracking and ingestion patterns.,data engineering etl pipelines quality validation lineage ingestion,concepts/data_engineering_rag.py,python,"python learning-path/W02/concepts/data_engineering_rag.py",pandas great-expectations,W01,2,intermediate,90,"Build data pipelines|Validate data quality|Track data lineage",data pipeline,high
APPLICATION,concept,W02,RAG Foundations,Embeddings Deep Dive,Understanding embeddings: how they work semantic similarity and choosing embedding models for applications.,embeddings vectors semantic similarity cosine distance,concepts/embeddings_explained.py,python,"python learning-path/W02/concepts/embeddings_explained.py",sentence-transformers numpy,W01,1,beginner,75,"Understand vector representations|Calculate similarity scores|Visualize embedding spaces",visualizations,foundation
APPLICATION,concept,W02,RAG Foundations,Embedding Models Comparison,Compare embedding models: all-MiniLM-L6 vs BGE vs E5. Practical guide for application developers.,embeddings comparison bge e5 minilm vector similarity benchmark,concepts/embedding_comparison.py,python,"python learning-path/W02/concepts/embedding_comparison.py",sentence-transformers mteb,W01,2,intermediate,90,"Benchmark embedding models|Analyze dimensionality tradeoffs|Select optimal embeddings",benchmark results,high
APPLICATION,concept,W02,RAG Foundations,Chunking Strategies,Document chunking methods: fixed-size recursive semantic and token-aware. Critical for RAG application quality.,chunking splitting semantic recursive token-aware documents,concepts/chunking_strategies.py,python,"python learning-path/W02/concepts/chunking_strategies.py",langchain tiktoken,W01,1,beginner,60,"Implement chunking methods|Compare chunk quality|Optimize chunk sizes",chunked documents,high
APPLICATION,exercise,W02,RAG Foundations,Simple RAG System,Build a simple RAG chatbot using LangChain and FAISS for FAQ-based question answering. First complete AI application.,rag retrieval augmented generation langchain faiss embeddings vector search chatbot,exercises/simple_rag.py,python,"python learning-path/W02/exercises/simple_rag.py",langchain langchain-community sentence-transformers faiss-cpu,W01,1,beginner,120,"Build basic RAG pipeline|Implement vector retrieval|Generate contextual responses",working chatbot,flagship
APPLICATION,concept,W03,RAG Production,Retrieval Strategies,Advanced retrieval: hybrid search combining BM25 sparse retrieval with dense embeddings and reranking.,hybrid search bm25 reranking cross-encoder retrieval dense sparse,concepts/retrieval_strategies.py,python,"python learning-path/W03/concepts/retrieval_strategies.py",rank-bm25 sentence-transformers,W02,1,intermediate,90,"Implement hybrid search|Apply cross-encoder reranking|Optimize retrieval quality",retrieval pipeline,high
APPLICATION,concept,W03,RAG Production,Context Window Management,Handling long contexts: summarization chains map-reduce and context compression techniques.,context window long-context summarization compression map-reduce,concepts/context_management.py,python,"python learning-path/W03/concepts/context_management.py",langchain transformers tiktoken,W02,2,intermediate,75,"Manage context limits|Implement summarization|Apply compression techniques",optimized contexts,medium
APPLICATION,concept,W03,RAG Production,RAG Evaluation Metrics,Understanding RAG metrics: retrieval (MRR NDCG Recall@K) and generation (faithfulness relevance).,evaluation metrics mrr ndcg recall faithfulness relevance ragas,concepts/evaluation_metrics.py,python,"python learning-path/W03/concepts/evaluation_metrics.py",ragas datasets,W02,1,intermediate,60,"Calculate retrieval metrics|Measure generation quality|Interpret evaluation scores",metrics dashboard,high
APPLICATION,exercise,W03,RAG Production,RAG System with Evaluation,Complete RAG system with model evaluation framework. Production-ready RAG application.,rag evaluation model comparison mistral qa pipeline retrieval,exercises/rag_system_exercise.py,python,"python learning-path/W03/exercises/rag_system_exercise.py",transformers sentence-transformers faiss-cpu ragas datasets evaluate,W01 W02,1,intermediate,150,"Build complete RAG system|Evaluate multiple models|Rank by composite scores",evaluated system,flagship
APPLICATION,exercise,W03,RAG Production,Hallucination Detection,Implement hallucination detection: faithfulness scoring factual consistency and citation verification.,hallucination detection faithfulness factual consistency citation,exercises/hallucination_detection.py,python,"python learning-path/W03/exercises/hallucination_detection.py",ragas transformers,W02,2,intermediate,90,"Detect hallucinations|Score faithfulness|Verify factual consistency",detection pipeline,high
APPLICATION,assignment,W03,RAG Production,GreenTech RAG Assignment,Complete RAG assignment for GreenTech Marketplace customer support. Real-world business application.,rag assignment greentech marketplace customer support business,assignments/W3_RAG_Assignment.ipynb,notebook,"jupyter notebook learning-path/W03/assignments/W3_RAG_Assignment.ipynb",transformers sentence-transformers faiss-cpu ragas datasets evaluate,W01 W02,2,intermediate,240,"Complete all 6 objectives|Build production RAG|Document findings",assignment submission,flagship
APPLICATION,project,W03,RAG Production,Job Fitment Agent,AI-powered job fitment analysis agent. Analyzes job postings and provides fitment scores with skill gap identification.,job fitment agent job search matching skill gap analysis career,projects/job_fitment/main.py,python,"cd learning-path/W03/projects/job_fitment && python main.py",playwright sentence-transformers faiss-cpu openai,W01 W02,4,advanced,180,"Scrape job postings|Analyze skill fitment|Identify skill gaps",fitment reports,flagship
APPLICATION,concept,W06,Safety and Guardrails,LLM Security Fundamentals,Understanding LLM vulnerabilities: prompt injection jailbreaks and data leakage.,security vulnerabilities prompt injection jailbreak data leakage,concepts/security_fundamentals.py,python,"python learning-path/W06/concepts/security_fundamentals.py",transformers,W01,1,intermediate,60,"Understand attack vectors|Identify vulnerabilities|Plan mitigations",security guide,foundation
APPLICATION,concept,W06,Safety and Guardrails,Content Moderation Strategies,Approaches to content moderation: classification filtering and human-in-the-loop.,content moderation classification filtering human-in-the-loop,concepts/content_moderation.py,python,"python learning-path/W06/concepts/content_moderation.py",transformers,W01,2,intermediate,60,"Implement classification|Design filters|Plan human review",moderation pipeline,medium
APPLICATION,exercise,W06,Safety and Guardrails,Input Validation Guards,Implement input validation: prompt injection detection PII filtering and input sanitization.,input validation prompt injection pii filtering sanitization,exercises/input_guards.py,python,"python learning-path/W06/exercises/input_guards.py",guardrails-ai presidio-analyzer,W01,1,intermediate,90,"Detect prompt injection|Filter PII|Sanitize inputs",validation pipeline,high
APPLICATION,exercise,W06,Safety and Guardrails,Output Validation Guards,Implement output validation: toxicity filtering factuality checking and format enforcement.,output validation toxicity factuality format enforcement,exercises/output_guards.py,python,"python learning-path/W06/exercises/output_guards.py",guardrails-ai detoxify,W01,2,intermediate,90,"Filter toxic content|Check factuality|Enforce output format",validation pipeline,high
APPLICATION,concept,W07,AI Agents,Async Patterns for Agents,AsyncIO patterns for concurrent agent execution. Parallel tool calls and non-blocking workflows.,async asyncio concurrent parallel agents non-blocking,concepts/async_patterns.py,python,"python learning-path/W07/concepts/async_patterns.py",asyncio aiohttp,W01,2,intermediate,75,"Implement async/await|Run parallel tool calls|Design non-blocking agents",async agent code,high
APPLICATION,concept,W07,AI Agents,Agent Architecture Patterns,Understanding agent architectures: ReAct function calling and tool use patterns. Core of agentic workflows.,agent architecture react function calling tool use patterns,concepts/agent_patterns.py,python,"python learning-path/W07/concepts/agent_patterns.py",langchain openai,W01,1,intermediate,75,"Understand ReAct pattern|Implement function calling|Design tool interfaces",architecture guide,foundation
APPLICATION,concept,W07,AI Agents,Agent Memory Systems,Memory for agents: buffer memory summary memory vector-backed memory and episodic memory.,agent memory buffer summary vector episodic conversation,concepts/memory_systems.py,python,"python learning-path/W07/concepts/memory_systems.py",langchain faiss-cpu,W02,1,intermediate,90,"Implement memory types|Compare memory strategies|Design memory architecture",memory implementations,high
APPLICATION,concept,W07,AI Agents,Planning and Reasoning,Agent planning: chain-of-thought tree-of-thought and plan-and-execute patterns.,planning reasoning cot tree-of-thought plan-execute,concepts/planning_reasoning.py,python,"python learning-path/W07/concepts/planning_reasoning.py",langchain openai,W01,2,advanced,90,"Implement CoT prompting|Apply tree-of-thought|Design planning systems",reasoning examples,high
APPLICATION,concept,W07,AI Agents,Tool Design Principles,Designing effective tools: API design error handling and tool descriptions for agents.,tool design api error handling descriptions agents,concepts/tool_design.py,python,"python learning-path/W07/concepts/tool_design.py",langchain pydantic,W01,2,intermediate,60,"Design tool interfaces|Handle errors gracefully|Write clear descriptions",tool templates,medium
APPLICATION,exercise,W07,AI Agents,Basic Agent Implementation,Build a basic ReAct agent with tools using LangChain or OpenAI function calling.,basic agent react langchain openai function calling tools,exercises/basic_agent.py,python,"python learning-path/W07/exercises/basic_agent.py",langchain openai,W01,1,intermediate,120,"Build ReAct agent|Implement tool calling|Test agent behavior",working agent,high
APPLICATION,exercise,W07,AI Agents,Multi-Tool Agent,Build an agent with multiple tools: search calculator code execution and file operations.,multi-tool agent search calculator code execution files,exercises/multi_tool_agent.py,python,"python learning-path/W07/exercises/multi_tool_agent.py",langchain openai duckduckgo-search,W01,2,intermediate,150,"Integrate multiple tools|Handle tool selection|Manage tool errors",multi-tool agent,high
APPLICATION,exercise,W07,AI Agents,MCP Integration,Integrate Model Context Protocol (MCP) servers for extended agent capabilities.,mcp model context protocol integration servers anthropic,exercises/mcp_integration.py,python,"python learning-path/W07/exercises/mcp_integration.py",mcp anthropic,W01,3,advanced,120,"Connect MCP servers|Use MCP tools|Build MCP-enabled agent",mcp agent,flagship
APPLICATION,assignment,W07,AI Agents,OpenAI Agent Builder Project,OpenAI Agent Builder project for automating workflows. Demonstrates agentic workflow mastery.,agent builder openai workflow automation agentic,assignments/agent_builder_project.py,python,"cd learning-path/W07/assignments && python main.py",openai langchain playwright,W03,2,advanced,300,"Build production agent|Automate complex workflow|Generate actionable outputs",agent system,flagship
APPLICATION,concept,W08,Multimodal AI,Vision-Language Models,Understanding VLMs: architecture training and capabilities of models like BLIP LLaVA and GPT-4V.,vision language models vlm blip llava gpt4v architecture,concepts/vlm_overview.py,python,"python learning-path/W08/concepts/vlm_overview.py",transformers pillow,W01,1,intermediate,75,"Understand VLM architecture|Compare model capabilities|Select appropriate models",comparison guide,foundation
APPLICATION,concept,W08,Multimodal AI,Audio Processing with LLMs,Speech-to-text text-to-speech and audio understanding with Whisper and similar models.,audio processing speech-to-text tts whisper audio understanding,concepts/audio_processing.py,python,"python learning-path/W08/concepts/audio_processing.py",transformers soundfile openai-whisper,W01,2,intermediate,60,"Process audio input|Generate speech output|Understand audio models",audio pipeline,medium
APPLICATION,concept,W08,Multimodal AI,Document Understanding,Processing documents: OCR layout analysis and document Q&A with multimodal models.,document understanding ocr layout analysis document qa,concepts/document_understanding.py,python,"python learning-path/W08/concepts/document_understanding.py",transformers pytesseract pdf2image,W01,2,intermediate,75,"Extract text from images|Analyze document layout|Answer document questions",document pipeline,high
APPLICATION,exercise,W08,Multimodal AI,Image Captioning,Generate captions for images using vision-language models. Practical multimodal application.,image captioning vision multimodal blip,exercises/W8_image_caption.ipynb,notebook,"jupyter notebook learning-path/W08/exercises/W8_image_caption.ipynb",transformers torch pillow,None,1,beginner,90,"Generate image captions|Fine-tune captioning|Evaluate caption quality",captioning system,medium
APPLICATION,exercise,W08,Multimodal AI,PDF Q&A System,Question answering system for PDF documents using RAG. Document intelligence application.,pdf qa document question answering rag,exercises/W8_pdf_Q&A.ipynb,notebook,"jupyter notebook learning-path/W08/exercises/W8_pdf_Q&A.ipynb",pypdf transformers langchain,W02,2,intermediate,120,"Parse PDF documents|Build document RAG|Answer document questions",pdf qa system,high
APPLICATION,exercise,W08,Multimodal AI,Speech to Image Pipeline,Convert speech to images using Whisper and Stable Diffusion. Creative multimodal application.,speech to image whisper stable diffusion audio multimodal,exercises/W8_Speech_to_Image.ipynb,notebook,"jupyter notebook learning-path/W08/exercises/W8_Speech_to_Image.ipynb",transformers diffusers soundfile torch openai-whisper,W01,3,intermediate,150,"Transcribe speech|Generate images|Build multimodal pipeline",speech-to-image system,flagship
APPLICATION,concept,W09,Agentic RAG,Agentic RAG Architecture,Understanding agentic RAG: query routing self-correction and adaptive retrieval. Next-gen RAG applications.,agentic rag architecture query routing self-correction adaptive,concepts/agentic_rag_architecture.py,python,"python learning-path/W09/concepts/agentic_rag_architecture.py",llama-index,W03 W07,1,advanced,90,"Understand agentic RAG|Design routing logic|Implement self-correction",architecture guide,foundation
APPLICATION,concept,W09,Agentic RAG,Query Understanding,Advanced query processing: decomposition rewriting and intent classification for RAG.,query understanding decomposition rewriting intent classification,concepts/query_understanding.py,python,"python learning-path/W09/concepts/query_understanding.py",llama-index openai,W03,2,intermediate,75,"Decompose complex queries|Rewrite for retrieval|Classify query intent",query pipeline,high
APPLICATION,concept,W09,Agentic RAG,Retrieval Agents,Building retrieval agents: when to retrieve what to retrieve and how to combine results.,retrieval agents adaptive retrieval result combination,concepts/retrieval_agents.py,python,"python learning-path/W09/concepts/retrieval_agents.py",llama-index,W03 W07,2,advanced,90,"Build retrieval agents|Implement adaptive retrieval|Combine multiple sources",agent implementation,high
APPLICATION,exercise,W09,Agentic RAG,Agentic RAG with LlamaIndex,Build agentic RAG systems with LlamaIndex framework including query routing and tool calling.,agentic rag llamaindex query routing tool calling agents,exercises/W9_Agentic_RAG_LlamaIndex.ipynb,notebook,"jupyter notebook learning-path/W09/exercises/W9_Agentic_RAG_LlamaIndex.ipynb",llama-index llama-index-llms-openai openai,W03,1,advanced,180,"Build with LlamaIndex|Implement query routing|Add tool calling",agentic rag system,flagship
APPLICATION,exercise,W09,Agentic RAG,Self-Correcting RAG,Implement self-correcting RAG with hallucination detection and query refinement loops.,self-correcting rag hallucination detection query refinement,exercises/self_correcting_rag.py,python,"python learning-path/W09/exercises/self_correcting_rag.py",llama-index ragas,W03,2,advanced,150,"Detect retrieval failures|Implement correction loops|Refine queries",self-correcting system,high
APPLICATION,concept,W10,Multi-Agent Systems,Multi-Agent Architecture,Designing multi-agent systems: coordination communication and task delegation.,multi-agent systems coordination communication delegation,concepts/multi_agent_systems.py,python,"python learning-path/W10/concepts/multi_agent_systems.py",langchain langgraph,W07 W09,1,advanced,90,"Design multi-agent architectures|Implement coordination|Handle communication",architecture guide,foundation
APPLICATION,concept,W10,Multi-Agent Systems,Agent Orchestration Patterns,Orchestration patterns: supervisor hierarchical and collaborative agent coordination.,orchestration patterns supervisor hierarchical collaborative,concepts/orchestration_patterns.py,python,"python learning-path/W10/concepts/orchestration_patterns.py",langgraph,W07,2,advanced,90,"Implement supervisor pattern|Design hierarchies|Enable collaboration",pattern implementations,high
APPLICATION,concept,W10,Multi-Agent Systems,State Management for Agents,Managing state in agent systems: checkpointing rollback and persistent state.,state management checkpointing rollback persistent agents,concepts/state_management.py,python,"python learning-path/W10/concepts/state_management.py",langgraph redis,W07,2,advanced,75,"Implement checkpointing|Handle rollback|Persist agent state",state system,medium
APPLICATION,exercise,W10,Multi-Agent Systems,Advanced Agentic RAG,Advanced agentic RAG with multi-document agents and complex reasoning.,advanced agentic rag multi document agents reasoning,exercises/W10_Advanced_Agentic_RAG.ipynb,notebook,"jupyter notebook learning-path/W10/exercises/W10_Advanced_Agentic_RAG.ipynb",llama-index llama-index-llms-openai openai,W09,1,advanced,180,"Build multi-document agents|Implement complex reasoning|Handle advanced queries",advanced system,flagship
APPLICATION,exercise,W10,Multi-Agent Systems,Multi-Agent RAG System,Build multi-agent RAG with specialized retrieval analysis and synthesis agents.,multi-agent rag specialized agents retrieval analysis synthesis,exercises/multi_agent_rag.py,python,"python learning-path/W10/exercises/multi_agent_rag.py",langgraph llama-index,W07 W09,2,advanced,180,"Design specialized agents|Coordinate retrieval|Synthesize results",multi-agent system,flagship
APPLICATION,project,W10,Multi-Agent Systems,Research Assistant System,Multi-agent research assistant that searches analyzes and synthesizes information autonomously.,research assistant multi-agent autonomous search synthesis,projects/research_assistant/main.py,python,"cd learning-path/W10/projects/research_assistant && python main.py",langgraph llama-index openai,W07 W09,3,advanced,300,"Build autonomous researcher|Coordinate multiple agents|Generate research reports",research system,flagship
INFERENCE,concept,W02,Vector Infrastructure,Vector Database Fundamentals,Introduction to vector databases: indexing algorithms ANN search and FAISS internals. Core inference infrastructure.,vector database faiss indexing ann hnsw ivf,concepts/vector_db_fundamentals.py,python,"python learning-path/W02/concepts/vector_db_fundamentals.py",faiss-cpu numpy,W01,2,intermediate,75,"Understand ANN algorithms|Configure FAISS indices|Optimize search parameters",indexed database,foundation
INFERENCE,exercise,W02,Vector Infrastructure,Vector DB Comparison,Compare vector databases: FAISS vs ChromaDB vs Qdrant. Benchmark performance for inference optimization.,vector database comparison faiss chroma qdrant benchmark,exercises/vector_db_comparison.py,python,"python learning-path/W02/exercises/vector_db_comparison.py",faiss-cpu chromadb qdrant-client,W01,3,intermediate,90,"Benchmark vector databases|Compare query latencies|Evaluate feature sets",comparison report,high
INFERENCE,concept,W05,Inference Optimization,Quantization Fundamentals,Understanding quantization: INT8 INT4 GGUF AWQ and GPTQ formats. Critical for inference cost reduction.,quantization int8 int4 gguf awq gptq formats,concepts/quantization_fundamentals.py,python,"python learning-path/W05/concepts/quantization_fundamentals.py",transformers bitsandbytes,W01,1,intermediate,75,"Understand quantization types|Compare format tradeoffs|Select appropriate format",comparison guide,foundation
INFERENCE,concept,W05,Inference Optimization,Inference Optimization Techniques,Optimizing LLM inference: KV caching batching speculative decoding and continuous batching.,inference optimization kv cache batching speculative decoding,concepts/inference_optimization.py,python,"python learning-path/W05/concepts/inference_optimization.py",transformers,W01,2,advanced,90,"Understand KV caching|Implement batching|Apply optimization techniques",optimized inference,high
INFERENCE,concept,W05,Inference Optimization,Cost Optimization Strategies,Token budgeting caching strategies model routing and cost-aware architecture design.,cost optimization token budget caching routing architecture,concepts/cost_optimization.py,python,"python learning-path/W05/concepts/cost_optimization.py",tiktoken redis,W01,2,intermediate,60,"Calculate token costs|Implement caching|Design cost-aware systems",cost model,high
INFERENCE,concept,W05,Inference Optimization,Inference Throughput Analysis,Understanding throughput: tokens per second batching efficiency and latency vs throughput tradeoffs.,throughput tokens per second batching efficiency latency,concepts/throughput_analysis.py,python,"python learning-path/W05/concepts/throughput_analysis.py",transformers,W01,2,intermediate,60,"Measure throughput|Analyze batching efficiency|Optimize for workload",throughput report,high
INFERENCE,exercise,W05,Inference Optimization,Model Quantization,Quantize models using GGUF and AWQ. Measure latency and quality tradeoffs for production deployment.,quantization gguf awq optimization inference latency quality,exercises/model_quantization.py,python,"python learning-path/W05/exercises/model_quantization.py",transformers auto-gptq autoawq llama-cpp-python,W01,1,intermediate,120,"Quantize models|Benchmark performance|Analyze quality impact",quantized models,flagship
INFERENCE,exercise,W05,Inference Optimization,Inference Server Setup,Set up vLLM or TGI for production inference with benchmarking. Core infrastructure skill.,inference server vllm tgi production benchmarking deployment,exercises/inference_server.py,python,"python learning-path/W05/exercises/inference_server.py",vllm,W01,2,advanced,150,"Deploy inference server|Configure optimization|Benchmark throughput",running server,flagship
INFERENCE,exercise,W05,Inference Optimization,Latency Profiling,Profile and optimize LLM pipeline latency: tokenization inference and decoding.,latency profiling optimization tokenization inference decoding,exercises/latency_profiling.py,python,"python learning-path/W05/exercises/latency_profiling.py",transformers torch,W01,3,advanced,90,"Profile pipeline stages|Identify bottlenecks|Optimize latency",profiling report,high
INFERENCE,exercise,W05,Inference Optimization,Streaming with SSE,Implement Server-Sent Events for streaming LLM responses. Real-time token-by-token output to clients.,streaming sse server sent events real-time tokens,exercises/sse_streaming.py,python,"python learning-path/W05/exercises/sse_streaming.py",fastapi sse-starlette httpx,W01,2,intermediate,90,"Implement SSE endpoints|Stream LLM tokens|Handle client connections",streaming api,high
INFERENCE,exercise,W05,Inference Optimization,WebSocket Real-time AI,Build bidirectional real-time AI communication with WebSockets. Interactive agent conversations.,websocket real-time bidirectional interactive agents,exercises/websocket_ai.py,python,"python learning-path/W05/exercises/websocket_ai.py",fastapi websockets,W01,2,intermediate,120,"Implement WebSocket server|Handle bidirectional messages|Build interactive AI",websocket system,high
INFERENCE,exercise,W05,Inference Optimization,Token Cost Calculator,Build token cost calculator and optimizer for production LLM applications.,token cost calculator optimizer production budget,exercises/token_cost_calculator.py,python,"python learning-path/W05/exercises/token_cost_calculator.py",tiktoken,W01,2,intermediate,60,"Calculate token costs|Optimize prompts|Budget for production",cost tool,high
INFERENCE,project,W05,Inference Optimization,Inference Benchmark Dashboard,Interactive dashboard comparing inference providers: latency cost and quality metrics.,inference benchmark dashboard comparison metrics visualization,projects/inference_dashboard/main.py,python,"cd learning-path/W05/projects/inference_dashboard && python main.py",streamlit plotly tiktoken,W01,3,advanced,180,"Benchmark providers|Visualize metrics|Compare cost-performance",dashboard,flagship
INFERENCE,concept,W11,Observability,LLM Observability Fundamentals,Understanding LLM observability: tracing logging and metrics for AI systems. Production monitoring.,observability tracing logging metrics llm monitoring,concepts/observability_fundamentals.py,python,"python learning-path/W11/concepts/observability_fundamentals.py",opentelemetry-api langfuse,W01,1,intermediate,75,"Understand tracing|Implement logging|Design metrics",observability guide,foundation
INFERENCE,concept,W11,Observability,Evaluation in Production,Continuous evaluation: online metrics user feedback and drift detection.,production evaluation online metrics feedback drift detection,concepts/production_evaluation.py,python,"python learning-path/W11/concepts/production_evaluation.py",ragas evidently,W03,2,advanced,90,"Monitor production quality|Collect feedback|Detect drift",monitoring system,high
INFERENCE,concept,W11,Observability,Cost and Performance Monitoring,Monitoring costs and performance: token usage latency percentiles and optimization.,cost monitoring performance token usage latency optimization,concepts/cost_monitoring.py,python,"python learning-path/W11/concepts/cost_monitoring.py",opentelemetry-api tiktoken,W05,2,intermediate,60,"Track token costs|Monitor latency|Optimize performance",dashboards,high
INFERENCE,exercise,W11,Observability,LangSmith Integration,Set up LangSmith for tracing and debugging LLM applications.,langsmith tracing debugging llm applications,exercises/langsmith_setup.py,python,"python learning-path/W11/exercises/langsmith_setup.py",langsmith langchain,W01,1,intermediate,90,"Configure LangSmith|Trace applications|Debug issues",tracing setup,high
INFERENCE,exercise,W11,Observability,Langfuse Integration,Set up Langfuse for open-source LLM observability and analytics.,langfuse observability analytics open-source tracing,exercises/langfuse_setup.py,python,"python learning-path/W11/exercises/langfuse_setup.py",langfuse,W01,2,intermediate,90,"Deploy Langfuse|Integrate tracing|Analyze metrics",langfuse setup,high
INFERENCE,exercise,W11,Observability,Custom Metrics Dashboard,Build custom observability dashboard with Grafana and Prometheus for LLM metrics.,custom metrics dashboard grafana prometheus llm,exercises/custom_dashboard.py,python,"python learning-path/W11/exercises/custom_dashboard.py",prometheus-client,W01,3,advanced,120,"Design metrics|Build dashboard|Alert on anomalies",monitoring dashboard,flagship
INFERENCE,project,W11,Observability,Production Monitoring System,End-to-end monitoring system for LLM applications with alerting and cost tracking.,production monitoring alerting cost tracking llm,projects/monitoring_system/main.py,python,"cd learning-path/W11/projects/monitoring_system && python main.py",langfuse prometheus-client grafana,W05,3,advanced,240,"Build monitoring|Implement alerting|Track costs",monitoring system,flagship
TRAINING,concept,W04,Fine-tuning,Transfer Learning for LLMs,Understanding transfer learning: pre-training vs fine-tuning when to fine-tune vs use RAG.,transfer learning pre-training fine-tuning rag comparison,concepts/transfer_learning.py,python,"python learning-path/W04/concepts/transfer_learning.py",transformers,W01,1,intermediate,60,"Understand transfer learning|Compare fine-tuning vs RAG|Choose appropriate approach",decision framework,foundation
TRAINING,concept,W04,Fine-tuning,PEFT Techniques Overview,Parameter-efficient fine-tuning: LoRA QLoRA adapters and prefix tuning explained.,peft lora qlora adapters prefix tuning efficient,concepts/peft_overview.py,python,"python learning-path/W04/concepts/peft_overview.py",peft transformers,W01,1,intermediate,75,"Understand PEFT methods|Compare adapter approaches|Select optimal technique",comparison guide,foundation
TRAINING,concept,W04,Fine-tuning,Dataset Preparation,Preparing datasets for fine-tuning: formatting cleaning and creating instruction datasets.,dataset preparation formatting cleaning instruction tuning,concepts/dataset_prep.py,python,"python learning-path/W04/concepts/dataset_prep.py",datasets pandas,W01,2,beginner,60,"Format training data|Clean datasets|Create instruction sets",prepared dataset,medium
TRAINING,concept,W04,Fine-tuning,Training Dynamics,Understanding training: loss curves learning rates and convergence analysis.,training dynamics loss curves learning rate convergence,concepts/training_dynamics.py,python,"python learning-path/W04/concepts/training_dynamics.py",transformers wandb,W01,2,intermediate,75,"Analyze loss curves|Tune learning rates|Monitor convergence",analysis notebook,medium
TRAINING,exercise,W04,Fine-tuning,LoRA Fine-tuning,Fine-tune a model using LoRA on a custom dataset. Understand rank alpha and target modules.,lora fine-tuning rank alpha target modules custom dataset,exercises/lora_finetuning.py,python,"python learning-path/W04/exercises/lora_finetuning.py",peft transformers datasets accelerate,W01,1,intermediate,180,"Configure LoRA parameters|Fine-tune on custom data|Evaluate improvements",fine-tuned model,flagship
TRAINING,exercise,W04,Fine-tuning,QLoRA Fine-tuning,Memory-efficient fine-tuning with QLoRA using 4-bit quantization.,qlora fine-tuning quantization 4-bit memory efficient,exercises/qlora_finetuning.py,python,"python learning-path/W04/exercises/qlora_finetuning.py",peft transformers bitsandbytes,W01,2,intermediate,180,"Apply 4-bit quantization|Configure QLoRA|Train on limited hardware",quantized model,high
TRAINING,exercise,W04,Fine-tuning,Fine-tuning vs RAG Analysis,Empirical comparison: when fine-tuning beats RAG and vice versa with cost-benefit analysis.,fine-tuning rag comparison analysis cost benefit tradeoffs,exercises/ft_vs_rag_analysis.py,python,"python learning-path/W04/exercises/ft_vs_rag_analysis.py",transformers sentence-transformers ragas,W02 W03,3,advanced,120,"Compare approaches empirically|Analyze cost-benefit|Document tradeoffs",analysis report,high
TRAINING,concept,W04,Fine-tuning,Evaluation for Fine-tuned Models,Evaluating fine-tuned models: benchmark selection metrics and A/B testing.,evaluation fine-tuned models benchmarks metrics ab testing,concepts/ft_evaluation.py,python,"python learning-path/W04/concepts/ft_evaluation.py",lm-eval transformers,W01,2,intermediate,75,"Select benchmarks|Compute metrics|Design A/B tests",evaluation framework,high
TRAINING,project,W04,Fine-tuning,Domain-Specific Fine-tuning,Fine-tune a model for a specific domain with full evaluation pipeline.,domain specific fine-tuning evaluation pipeline,projects/domain_finetuning/main.py,python,"cd learning-path/W04/projects/domain_finetuning && python main.py",peft transformers datasets wandb,W01,3,advanced,360,"Fine-tune for domain|Evaluate thoroughly|Document improvements",fine-tuned model,flagship
TRAINING,concept,W04,Fine-tuning,Constitutional AI Principles,Understanding constitutional AI: self-critique RLHF and alignment techniques.,constitutional ai rlhf alignment self-critique principles,concepts/constitutional_ai.py,python,"python learning-path/W04/concepts/constitutional_ai.py",transformers,W01,3,advanced,75,"Understand RLHF|Learn constitutional AI|Apply alignment principles",concept notes,medium
TRAINING,concept,W04,Fine-tuning,Open vs Closed Models,Comparing open-source vs closed models: capabilities costs and strategic considerations.,open source closed models comparison capabilities costs,concepts/open_vs_closed.py,python,"python learning-path/W04/concepts/open_vs_closed.py",transformers openai,W01,2,intermediate,60,"Compare model types|Analyze costs|Make strategic decisions",comparison guide,medium
THOUGHT_LEADERSHIP,article,N/A,Blog,RAG vs Fine-tuning Decision Framework,Technical blog post on when to use RAG vs fine-tuning with real-world examples.,rag fine-tuning decision framework blog article,blog/rag_vs_finetuning.md,document,"N/A",None,W03 W04,1,advanced,180,"Articulate tradeoffs|Provide framework|Share examples",published article,flagship
THOUGHT_LEADERSHIP,article,N/A,Blog,Agentic Workflows in Production,Article on deploying agentic workflows: lessons learned and best practices.,agentic workflows production lessons best practices,blog/agentic_production.md,document,"N/A",None,W07 W09 W10,2,advanced,180,"Share production insights|Document patterns|Provide guidance",published article,flagship
THOUGHT_LEADERSHIP,article,N/A,Blog,The Three Layers of AI,Article explaining AI stack layers based on Andrew Ng's framework with portfolio perspective.,ai layers application inference training andrew ng,blog/three_layers_ai.md,document,"N/A",None,W01-W11,3,intermediate,120,"Explain AI stack|Connect to portfolio|Provide perspective",published article,high
THOUGHT_LEADERSHIP,article,N/A,Blog,LLM Inference Economics,Deep dive into inference costs throughput and optimization strategies for practitioners.,inference economics costs throughput optimization,blog/inference_economics.md,document,"N/A",None,W05 W11,4,advanced,180,"Analyze economics|Provide optimization guide|Share data",published article,flagship
THOUGHT_LEADERSHIP,article,N/A,Blog,Product Case Study - Job Fitment Agent,User research to AI solution case study. Demonstrates user empathy and product thinking.,case study user research product thinking job fitment,blog/job_fitment_case_study.md,document,"N/A",None,W03 W07,5,intermediate,120,"Document user needs|Show solution process|Measure outcomes",published article,high
THOUGHT_LEADERSHIP,article,N/A,Blog,Product Case Study - GreenTech RAG,End-to-end case study of building customer support RAG. From problem to production.,case study greentech rag customer support production,blog/greentech_case_study.md,document,"N/A",None,W03,6,intermediate,120,"Document business problem|Show technical solution|Present results",published article,high
THOUGHT_LEADERSHIP,article,N/A,Blog,Core LLM Concepts Explained,Educational article: Tokens Embeddings Attention Layers Tensors Parameters - the complete LLM pipeline.,core concepts tokens embeddings attention layers tensors parameters,blog/core_llm_concepts.md,document,"N/A",None,W00,5,intermediate,120,"Explain fundamentals|Use clear analogies|Provide code examples",published article,high
THOUGHT_LEADERSHIP,presentation,N/A,Talks,AI Portfolio Overview,Presentation deck showcasing AI portfolio using three-layer framework.,portfolio presentation ai layers showcase,presentations/portfolio_overview.pptx,document,"N/A",None,W00-W11,1,intermediate,120,"Create compelling narrative|Showcase projects|Demonstrate expertise",presentation deck,flagship